{
  "questions": [
    {
      "id": "ols-q001",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "On se place dans le modèle $Y=X\\beta+\\varepsilon$ (\\operatorname{rg}(X)=p)). Quelles hypothèses fait-on sur $\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2(XX')^{-1}$",
        "Sa moyenne empirique est nulle",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Son espérance est nulle",
        "Il est non-corrélé à $Y$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Hypothèses usuelles : $\\mathbb E[\\varepsilon]=0$ et $\\operatorname{Var}(\\varepsilon)=\\sigma^2 I_n$. En revanche $\\varepsilon$ n’est pas non-corrélé à $Y$ puisque $Y=X\\beta+\\varepsilon$.",
      "tags": [
        "OLS",
        "hypothèses",
        "Gauss–Markov"
      ]
    },
    {
      "id": "ols-q002",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Que vaut l’estimateur MCO (OLS) de $\\beta$ ?",
      "choices": [
        "$\\mathbb E\\big((XX')^{-1}XY\\big)$",
        "$(X'X)^{-1}X'Y$",
        "$(XX')^{-1}XY$",
        "$\\mathbb E\\big((X'X)^{-1}XY\\big)$"
      ],
      "answer": 1,
      "explanation": "Formule classique des MCO : $\\hat\\beta=(X'X)^{-1}X'Y$.",
      "tags": [
        "OLS",
        "estimateur",
        "formule"
      ]
    },
    {
      "id": "linalg-q003",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Que signifie $\\operatorname{rg}(X)=p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p\\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : colonnes linéairement indépendantes, donc $X'X$ inversible et nécessairement $p\\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ]
    },
    {
      "id": "test-q004",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur du test de Student pour le coefficient de $X^{(j)}$ vaut 0,023, que concluez-vous ?",
      "choices": [
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul.",
        "Le modèle est surajusté.",
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$."
      ],
      "answer": 3,
      "explanation": "Avec un seuil usuel $\\alpha=5\\%$, $p=0,023 < 0,05$ : on rejette $H_0$ (« coefficient nul »). Donc le coefficient est significatif.",
      "tags": [
        "test de Student",
        "p-value",
        "significativité"
      ]
    },
    {
      "id": "res-q005",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "test-q006",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Supposons que pour un test statistique donné, la région critique associée au risque de première espèce $\\alpha = 5\\%$ est $\\{|T| > 3.1\\}$, où $T$ désigne la statistique de test. On observe $T = -4$. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test."
      ],
      "answer": [
        4,
        3,
        5
      ],
      "explanation": "Comme $|T|=4 > 3.1$, on est dans la région critique pour $\\alpha=5\\%$ et $\\alpha=10\\%$ donc on rejette $H_0$. Mais $|T|$ n’est pas assez extrême pour rejeter à $\\alpha=1\\%$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "p-value"
      ]
    },
    {
      "id": "test-q007",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Pour le même test qu’à la question précédente, on note $F$ la fonction de répartition de $T$ sous $H_0$. On observe toujours $T=-4$. Que vaut alors la p-value ?",
      "choices": [
        "$1 - F(-4)$",
        "$F(-4)$",
        "$2F(-4)$",
        "$2(1 - F(-4))$"
      ],
      "answer": 2,
      "explanation": "La p-value bilatérale est $2\\times F(-4)$.",
      "tags": [
        "tests statistiques",
        "p-value",
        "loi de test"
      ]
    },
    {
      "id": "var-q008",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique effectuée auprès d’étudiants, on relève la mention obtenue au BAC. Cette variable peut être considérée comme :",
      "choices": [
        "qualitative ordinale",
        "quantitative continue",
        "qualitative nominale",
        "quantitative discrète"
      ],
      "answer": 0,
      "explanation": "Les mentions (« passable », « bien », « très bien ») ont un ordre naturel → qualitative ordinale.",
      "tags": [
        "nature des variables",
        "qualitative",
        "ordinale"
      ]
    },
    {
      "id": "test-q009",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique, on souhaite étudier le lien entre le pays d’origine des individus observés et leur souscription (ou non) à une assurance vie. Quel outil vous semble adapté ?",
      "choices": [
        "La mise en place d’un test du chi-deux",
        "La représentation d’un nuage de points",
        "Une représentation graphique à l’aide de boxplots",
        "Le calcul de la corrélation de Pearson"
      ],
      "answer": 0,
      "explanation": "Deux variables qualitatives → test du chi² d’indépendance.",
      "tags": [
        "test du chi²",
        "indépendance",
        "qualitative"
      ]
    },
    {
      "id": "ols-q010",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Dans l’écriture classique d’un modèle de régression linéaire concernant $n$ individus, $Y = X\\beta + \\varepsilon$, (plusieurs réponses possibles)",
      "choices": [
        "$Y$ et $X$ sont des vecteurs de taille $n$",
        "$Y$ est toujours considéré comme aléatoire",
        "$\\varepsilon$ est toujours considéré comme aléatoire",
        "$Y$ et $\\beta$ sont des vecteurs de taille $n$",
        "$X$ est une matrice"
      ],
      "answer": [
        1,
        2,
        4
      ],
      "explanation": "$Y$ est un vecteur aléatoire ($n\\times 1$), $\\varepsilon$ aussi. $X$ est une matrice ($n\\times p$). $\\beta$ est un vecteur $p\\times 1$, donc pas de taille $n$.",
      "tags": [
        "OLS",
        "modèle de régression",
        "dimensions"
      ]
    },
    {
      "id": "test-q011",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Supposons que lors de l’application d’un test statistique, on observe une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test."
      ],
      "answer": [
        2,
        4
      ],
      "explanation": "Comme $p = 0,037 < 0,05$, on rejette l’hypothèse nulle au seuil 5% et 10%. En revanche, $p > 0,01$ donc pas de rejet au seuil 1%.",
      "tags": [
        "tests statistiques",
        "p-value",
        "seuil de risque"
      ]
    },
    {
      "id": "test-q012",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Pour un test donné, la région critique associée à un risque de première espèce $\\alpha$ est $\\{|T| > a\\}$, où $T$ suit une loi continue. On a observé $T=-2$ et une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour $\\alpha=5\\%$, $a < 2$",
        "Pour $\\alpha=3,7\\%$, $a > 2$",
        "Pour $\\alpha=3,7\\%$, $a < 2$",
        "Pour $\\alpha=3,7\\%$, $a = 2$",
        "Pour $\\alpha=5\\%$, $a > 2$",
        "Pour $\\alpha=5\\%$, $a = 2$"
      ],
      "answer": [
        0,
        3
      ],
      "explanation": "La p-value est $0,037$, ce qui correspond à un seuil critique $\\alpha \\approx 3,7\\%$. Donc pour $\\alpha=5\\%$, on rejette l’hypothèse nulle et $a < 2$. Pour $\\alpha=3,7\\%$, $T$ est exactement sur la frontière, donc on a $a = 2$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "p-value"
      ]
    },
    {
      "id": "var-q013",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique effectuée auprès d’étudiants, on relève le code postal du lycée dont ils proviennent. Cette variable peut être considérée comme :",
      "choices": [
        "qualitative nominale",
        "quantitative continue",
        "qualitative ordinale",
        "quantitative discrète"
      ],
      "answer": 0,
      "explanation": "Le code postal est une simple étiquette servant à identifier une zone géographique, sans ordre naturel ni signification métrique → variable qualitative nominale.",
      "tags": [
        "nature des variables",
        "qualitative",
        "nominale"
      ]
    },
    {
      "id": "visu-q014",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique, on souhaite étudier le lien entre le pays d’origine des individus observés et leur revenu annuel. Quel outil vous semble adapté ?",
      "choices": [
        "La mise en place d’un test du chi-deux",
        "Le calcul de la corrélation de Pearson",
        "La représentation d’un nuage de points",
        "Une représentation graphique à l’aide de boxplots"
      ],
      "answer": 3,
      "explanation": "Le pays d’origine est une variable qualitative et le revenu une variable quantitative → on compare les distributions avec des boxplots.",
      "tags": [
        "visualisation",
        "qualitative vs quantitative",
        "boxplots"
      ]
    },
    {
      "id": "ols-q015",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "En pratique, lorsqu’on met en place un modèle de régression linéaire $Y = X\\beta + \\varepsilon$, quelles quantités sont connues (observées) ?",
      "choices": [
        "$Y$ et $X\\beta$",
        "$X\\beta$",
        "$X, Y$ et $\\varepsilon$",
        "$X$ et $Y$"
      ],
      "answer": 3,
      "explanation": "En pratique, on observe $X$ (variables explicatives) et $Y$ (variable dépendante). Les paramètres $\\beta$ et les erreurs $\\varepsilon$ sont inconnus.",
      "tags": [
        "OLS",
        "régression linéaire",
        "observables"
      ]
    },
    {
      "id": "var-q016",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Si $\\mathbb E(\\varepsilon)=0$ et $\\operatorname{Var}(\\varepsilon)=\\sigma^2 I_n$, que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2(X'X)^{-1}$",
        "$\\sigma^2X\\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2(XX')^{-1}$"
      ],
      "answer": 2,
      "explanation": "En effet, $\\operatorname{Var}(Y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\sigma^2 I_n$.",
      "tags": [
        "variance",
        "régression",
        "OLS"
      ]
    },
    {
      "id": "res-q017",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "var-q018",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Quelle statistique constitue un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\tfrac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum_{i=1}^n \\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n-p}\\sum_{i=1}^n \\varepsilon_i^2$"
      ],
      "answer": 0,
      "explanation": "L’estimateur sans biais de la variance est $\\hat\\sigma^2 = \\tfrac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2$.",
      "tags": [
        "variance",
        "estimateur sans biais",
        "OLS"
      ]
    },
    {
      "id": "test-q019",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Student associée au coefficient de régression de la variable $X^{(j)}$ vaut $-23$, que concluez-vous ?",
      "choices": [
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul.",
        "Le modèle est surajusté.",
        "Relation statistiquement significative entre $X^{(j)}$ et $Y$."
      ],
      "answer": 3,
      "explanation": "Avec une statistique $t = -23$ (valeur absolue extrêmement élevée), on rejette l’hypothèse nulle $H_0$. Il existe donc une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "test de Student",
        "significativité",
        "régression"
      ]
    },
    {
      "id": "ols-q020",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que peut-on dire de l’estimateur MCO de $\\beta$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2$",
        "C’est le meilleur estimateur parmi tous les estimateurs sans biais",
        "C’est un estimateur sans biais",
        "Son espérance est nulle",
        "Sa variance vaut $\\hat\\sigma^2/(n-p)$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "C’est le meilleur estimateur parmi tous les estimateurs linéaires et sans biais",
        "C’est le meilleur estimateur parmi tous les estimateurs",
        "C’est le meilleur estimateur parmi tous les estimateurs consistants"
      ],
      "answer": [
        2,
        5,
        6
      ],
      "explanation": "Théorème de Gauss–Markov : l’estimateur MCO est sans biais, sa variance est $\\sigma^2(X'X)^{-1}$ et il est le meilleur estimateur linéaire sans biais (BLUE).",
      "tags": [
        "OLS",
        "Gauss–Markov",
        "BLUE",
        "estimateur"
      ]
    },
    {
      "id": "test-q021",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Supposons que lors de l’application d’un test statistique, on observe une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test."
      ],
      "answer": [
        2,
        4
      ],
      "explanation": "Comme $p = 0,037 < 0,05$, on rejette $H_0$ au seuil de 5% et de 10%. En revanche, $p > 0,01$ donc pas de rejet au seuil de 1%.",
      "tags": [
        "tests statistiques",
        "p-value",
        "seuil de risque"
      ]
    },
    {
      "id": "ols-q022",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On se place dans le modèle $Y = X\\beta + \\varepsilon$ (rang($X$) = $p$). Quelles hypothèses fait-on sur $\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2(XX')^{-1}$",
        "Sa moyenne empirique est nulle",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Son espérance est nulle",
        "Il est non-corrélé à $Y$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Hypothèses usuelles : $\\mathbb E[\\varepsilon] = 0$ et $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I_n$. En revanche, $\\varepsilon$ n’est pas non-corrélé à $Y$ puisque $Y = X\\beta + \\varepsilon$.",
      "tags": [
        "OLS",
        "hypothèses",
        "Gauss–Markov"
      ]
    },
    {
      "id": "linalg-q023",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\operatorname{rg}(X) = p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p \\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : les colonnes sont linéairement indépendantes, donc $X'X$ est inversible et nécessairement $p \\le n$. La matrice $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ]
    },
    {
      "id": "ols-q024",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que vaut l’estimateur MCO de $\\beta$ ?",
      "choices": [
        "$\\mathbb E\\big((XX')^{-1}XY\\big)$",
        "$(X'X)^{-1}X'Y$",
        "$(XX')^{-1}XY$",
        "$\\mathbb E\\big((X'X)^{-1}XY\\big)$"
      ],
      "answer": 1,
      "explanation": "Formule classique des MCO : $\\hat\\beta = (X'X)^{-1}X'Y$.",
      "tags": [
        "OLS",
        "estimateur",
        "formule"
      ]
    },
    {
      "id": "res-q025",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelles relations sont vraies ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "test-q026",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Supposons que pour un test statistique donné, la région critique associée au risque de première espèce $\\alpha = 5\\%$ est $\\{|T| > 3.1\\}$, où $T$ désigne la statistique de test. On observe $T = -4$. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test."
      ],
      "answer": [
        4,
        3,
        5
      ],
      "explanation": "Comme $|T| = 4 > 3.1$, on est dans la région critique pour $\\alpha = 5\\%$ et $\\alpha = 10\\%$ donc on rejette $H_0$. Mais la valeur n’est pas assez extrême pour conclure au seuil $\\alpha = 1\\%$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "régression"
      ]
    },
    {
      "id": "ols-q027",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On se place dans un modèle de régression linéaire standard $Y = X\\beta + \\varepsilon$, où $\\beta \\in \\mathbb{R}^p$, $X$ est une matrice déterministe de taille $n \\times p$ de rang $p$, et $\\varepsilon$ est un vecteur aléatoire de taille $n$. Quelles hypothèses fait-on sur $\\varepsilon = (\\varepsilon_1,\\dots,\\varepsilon_n)$ ? (plusieurs réponses possibles)",
      "choices": [
        "$E[\\varepsilon_i \\varepsilon_j] = 0$ pour $i \\neq j$",
        "$E[\\varepsilon_i] = 0$",
        "$E[\\varepsilon_i Y_i] = 0$",
        "$Var(\\varepsilon_i) = \\sigma^2(XX')^{-1}_{ii}$",
        "$Var(\\varepsilon_i) = 0$",
        "$\\tfrac{1}{n} \\sum_i \\varepsilon_i = 0$"
      ],
      "answer": [
        0,
        1
      ],
      "explanation": "On suppose : espérance nulle ($E[\\varepsilon_i]=0$), non-corrélation des erreurs entre elles ($E[\\varepsilon_i \\varepsilon_j]=0$ pour $i\\neq j$), et variance homogène $Var(\\varepsilon_i) = \\sigma^2$. Les autres propositions sont fausses.",
      "tags": [
        "OLS",
        "hypothèses",
        "indépendance",
        "homoscédasticité"
      ]
    },
    {
      "id": "ols-q028",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que peut-on dire de l’estimateur MCO de $\\beta$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2$",
        "C’est le meilleur estimateur parmi tous les estimateurs sans biais",
        "Il s'agit d'un estimateur sans biais",
        "Son espérance est nulle",
        "Sa variance vaut $\\hat\\sigma^2/(n-p)$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "C’est le meilleur estimateur parmi tous les estimateurs linéaires et sans biais",
        "C’est le meilleur estimateur parmi tous les estimateurs",
        "C’est le meilleur estimateur parmi tous les estimateurs consistants"
      ],
      "answer": [
        2,
        5,
        6
      ],
      "explanation": "D’après le théorème de Gauss–Markov : l’estimateur MCO est sans biais, sa variance est $\\sigma^2(X'X)^{-1}$ et il est le meilleur estimateur linéaire sans biais (BLUE).",
      "tags": [
        "OLS",
        "Gauss–Markov",
        "BLUE",
        "estimateur"
      ]
    },
    {
      "id": "res-q029",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelles relations sont vraies ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I - P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "var-q030",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelle statistique donne un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\tfrac{1}{n-p}\\sum \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum \\varepsilon_i^2$",
        "$\\tfrac{1}{n-p}\\sum \\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum \\hat\\varepsilon_i^2$"
      ],
      "answer": 0,
      "explanation": "L’estimateur sans biais de la variance est $s^2 = \\tfrac{1}{n-p}\\sum \\hat\\varepsilon_i^2$.",
      "tags": [
        "variance",
        "estimateur sans biais",
        "résidus"
      ]
    },
    {
      "id": "var-q031",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2(X'X)^{-1}$",
        "$\\sigma^2 X\\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2(XX')^{-1}$"
      ],
      "answer": 2,
      "explanation": "Puisque $Y = X\\beta + \\varepsilon$ avec $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I_n$, on a $\\operatorname{Var}(Y) = \\sigma^2 I_n$.",
      "tags": [
        "variance",
        "régression",
        "moments"
      ]
    },
    {
      "id": "multi-q032",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Qu’est-ce que la multicolinéarité dans un modèle de régression multiple ?",
      "choices": [
        "L’erreur dans l’estimation des coefficients de régression",
        "L’interaction entre $Y$ et les variables explicatives",
        "L’influence excessive d’une seule observation sur l'estimation du modèle",
        "La forte corrélation entre deux ou plusieurs variables explicatives"
      ],
      "answer": 3,
      "explanation": "La multicolinéarité correspond à une forte corrélation linéaire entre deux ou plusieurs variables explicatives, ce qui rend $X'X$ proche de la singularité.",
      "tags": [
        "régression multiple",
        "multicolinéarité",
        "colinéarité"
      ]
    },
    {
      "id": "fisher-q033",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On note SCR la somme des carrés des résidus du modèle initial, et $SCR_c$ celle d’un sous-modèle contenant $p'$ variables ($p'<p$). Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n-p}{p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR - SCR_c}{SCR}$"
      ],
      "answer": 2,
      "explanation": "La statistique de Fisher utilisée est $F = \\dfrac{(SCR_c - SCR)/(p - p')}{SCR/(n - p)} = \\dfrac{n - p}{p - p'}\\dfrac{SCR_c - SCR}{SCR}$.",
      "tags": [
        "tests de Fisher",
        "comparaison de modèles",
        "régression multiple"
      ]
    },
    {
      "id": "r2-q034",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ? (plusieurs réponses possibles)",
      "choices": [
        "Le $R^2$ augmente",
        "Le $R^2_a$ diminue",
        "La SCR diminue",
        "Le $R^2$ diminue",
        "La SCR augmente",
        "Le $R^2_a$ augmente"
      ],
      "answer": [
        0,
        2
      ],
      "explanation": "L’ajout d’une variable explique au moins autant la variance, donc $R^2$ ne peut qu’augmenter et la SCR diminuer. En revanche, $R^2_a$ peut augmenter ou baisser selon la pertinence de la variable.",
      "tags": [
        "régression multiple",
        "$R^2$",
        "SCR"
      ]
    },
    {
      "id": "hetero-q035",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Dans le contexte d’une régression multiple, qu’est-ce que l’hétéroscédasticité ?",
      "choices": [
        "Le fait que la variance des $\\varepsilon_i$ n’est pas constante",
        "Le fait que les $\\varepsilon_i$ ne sont pas corrélés entre eux",
        "Le fait que la relation entre $Y$ et les explicatives n’est pas linéaire",
        "Le fait que les coefficients du modèle sont biaisés"
      ],
      "answer": 0,
      "explanation": "L’hétéroscédasticité correspond à une variance non constante des erreurs : $\\operatorname{Var}(\\varepsilon_i) = \\sigma_i^2$ dépend de $i$.",
      "tags": [
        "régression multiple",
        "hétéroscédasticité",
        "hypothèses"
      ]
    },
    {
      "id": "res-q036",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Supposons qu’une constante est incluse dans le modèle. Quelle(s) propriété(s) vérifie(nt) le vecteur des résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2 I_n$",
        "Il est non corrélé à $Y$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Sa moyenne est nulle",
        "Il est de taille $p$",
        "Il est non corrélé à $\\hat Y$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Les résidus vérifient $\\mathbb E[\\hat\\varepsilon] = 0$ (si constante incluse) et sont orthogonaux aux valeurs ajustées ($\\hat Y$).",
      "tags": [
        "résidus",
        "régression multiple",
        "propriétés"
      ]
    },
    {
      "id": "linalg-q037",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\operatorname{rg}(X) = p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p \\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : les colonnes de $X$ sont linéairement indépendantes, donc $X'X$ est inversible et nécessairement $p \\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ]
    },
    {
      "id": "r2-q038",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2$ ?",
      "choices": [
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar Y)^2}{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\hat Y_i)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\hat Y_i)^2}$"
      ],
      "answer": 2,
      "explanation": "Définition : $R^2 = \\dfrac{\\text{Somme des carrés expliquée}}{\\text{Somme totale des carrés}} = \\dfrac{\\sum (\\hat Y_i - \\bar Y)^2}{\\sum (Y_i - \\bar Y)^2}$.",
      "tags": [
        "$R^2$",
        "qualité d’ajustement",
        "régression"
      ]
    },
    {
      "id": "fisher-q039",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Fisher de significativité globale du modèle vaut 41, que concluez-vous ?",
      "choices": [
        "Toutes les variables explicatives sont significatives.",
        "Aucune variable explicative, autre que la constante, n’est significative dans le modèle.",
        "Toutes les variables explicatives, autre que la constante, sont significatives.",
        "Au moins une variable autre que la constante est significative."
      ],
      "answer": 3,
      "explanation": "Un test de Fisher global très significatif ($F=41$) permet de rejeter $H_0$ et de conclure qu’au moins une variable explicative influence $Y$.",
      "tags": [
        "tests de Fisher",
        "significativité globale",
        "régression multiple"
      ]
    },
    {
      "id": "homo-q040",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Dans le contexte d’une régression multiple, qu’est-ce que l’homoscedasticité ?",
      "choices": [
        "Le fait que les $\\varepsilon_i$ ne sont pas corrélés entre eux",
        "Le fait que la relation entre $Y$ et les variables explicatives est linéaire",
        "Le fait que l’estimateur MCO $\\hat\\beta$ est sans biais",
        "Le fait que la variance des $\\varepsilon_i$ est constante"
      ],
      "answer": 3,
      "explanation": "Homoscedasticité : $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$ pour tout $i$. Cela signifie que la variance est constante mais pas nécessairement que les erreurs soient indépendantes.",
      "tags": [
        "régression multiple",
        "homoscédasticité",
        "hypothèses"
      ]
    },
    {
      "id": "r2a-q041",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$1-\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$1-\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$"
      ],
      "answer": 3,
      "explanation": "Le $R^2_a$ corrige le $R^2$ en tenant compte du nombre de variables explicatives $p$. La bonne formule est $R^2_a = 1 - \\dfrac{n-1}{n-p}\\dfrac{SCR}{SCT}$.",
      "tags": [
        "$R^2_a$",
        "qualité d’ajustement",
        "régression multiple"
      ]
    },
    {
      "id": "qcm3-q01",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On considère le cas d’un modèle de régression simple : on observe le vecteur y et le vecteur x, et on estime la droite des moindres carrés associée. Que vaut l’estimation de la pente ?",
      "choices": [
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$",
        "$\\dfrac{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}{\\mathrm{cov}(x,y)}$",
        "$\\dfrac{\\mathrm{var}(x)}{\\mathrm{cov}(x,y)}$",
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}$"
      ],
      "answer": 0,
      "explanation": "Dans un modèle de régression simple $y = \\alpha + \\beta x + \\varepsilon$, l’estimateur des moindres carrés de la pente est $\\hat{\\beta} = \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$.",
      "tags": [
        "régression simple",
        "moindres carrés",
        "estimateur de pente"
      ]
    },
    {
      "id": "qcm3-q02",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur du test de Student associé au coefficient de régression de la variable $X^{(j)}$ vaut 0.003, que concluez-vous ?",
      "choices": [
        "Le modèle est surajusté.",
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul."
      ],
      "answer": 1,
      "explanation": "Une p-valeur de 0.003 est très inférieure au seuil classique de 5%. On rejette donc l’hypothèse nulle $H_0: \\beta_j = 0$ et on conclut qu’il existe une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "régression linéaire",
        "test de Student",
        "p-valeur",
        "significativité"
      ]
    },
    {
      "id": "qcm3-q03",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$"
      ],
      "answer": 2,
      "explanation": "Le $R^2_a$ corrige le $R^2$ en tenant compte du nombre de variables explicatives $p$. La bonne formule est $R^2_a = 1 - \\dfrac{n-1}{n-p}\\dfrac{SCR}{SCT}$, où $SCR = \\sum (Y_i - \\hat{Y}_i)^2$ et $SCT = \\sum (Y_i - \\bar{Y})^2$.",
      "tags": [
        "régression linéaire",
        "$R^2_a$",
        "qualité d’ajustement",
        "régression multiple"
      ]
    },
    {
      "id": "qcm3-q04",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Fisher de significativité globale du modèle vaut -41, que concluez-vous ?",
      "choices": [
        "Au moins une variable autre que la constante est significative.",
        "Toutes les variables explicatives sont significatives.",
        "Aucune variable explicative, autre que la constante, n’est significative dans le modèle.",
        "Toutes les variables explicatives, autre que la constante, sont significatives."
      ],
      "answer": 0,
      "explanation": "Le test de Fisher global permet de tester l’hypothèse nulle $H_0 : \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$. Si la statistique est significative, on rejette $H_0$ et on conclut qu’au moins une variable explicative (autre que la constante) a un effet significatif sur $Y$.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "significativité globale"
      ]
    },
    {
      "id": "qcm3-q05",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ?",
      "choices": [
        "La SCR diminue",
        "Le $R^2$ diminue",
        "Le $R^2_a$ diminue",
        "Le $R^2$ augmente",
        "Le $R^2_a$ augmente",
        "La SCR augmente"
      ],
      "answer": [
        0,
        3
      ],
      "explanation": "En ajoutant une variable explicative, la somme des carrés des résidus (SCR) ne peut qu’augmenter la qualité d’ajustement ou la laisser identique. Ainsi, la SCR diminue (ou reste identique) et le $R^2$ augmente (ou reste identique). En revanche, le $R^2_a$ peut augmenter ou diminuer selon la pertinence de la variable ajoutée.",
      "tags": [
        "régression linéaire",
        "SCR",
        "$R^2$",
        "$R^2_a$"
      ]
    },
    {
      "id": "qcm3-q06",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On considère le cas d’un modèle de régression simple : on observe le vecteur y et le vecteur x, et on estime la droite des moindres carrés associée. Que vaut l’estimation de l’ordonnée à l’origine ?",
      "choices": [
        "$\\bar{y} - \\dfrac{\\mathrm{cov}(y,x)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}\\,\\bar{x}$",
        "$\\dfrac{\\mathrm{cov}(y,x)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}$",
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$",
        "$\\bar{y} - \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}\\,\\bar{x}$"
      ],
      "answer": 3,
      "explanation": "Dans la régression simple $y = \\alpha + \\beta x + \\varepsilon$, on a $\\hat{\\beta} = \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$ et donc $\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\,\\bar{x}$. Cela donne bien $\\hat{\\alpha} = \\bar{y} - \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}\\,\\bar{x}$.",
      "tags": [
        "régression linéaire",
        "moindres carrés",
        "ordonnée à l’origine"
      ]
    },
    {
      "id": "qcm3-q07",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "La statistique du test Student associé au coefficient de régression de la variable $X^{(j)}$ vaut -23, que concluez-vous ?",
      "choices": [
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
        "Ce coefficient de régression est nul.",
        "Le modèle de régression est inadéquat.",
        "Le modèle est surajusté."
      ],
      "answer": 0,
      "explanation": "Une statistique de Student de grande valeur absolue (ici -23) implique une p-valeur extrêmement faible. On rejette donc $H_0 : \\beta_j = 0$ et on conclut qu’il existe une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "régression linéaire",
        "test de Student",
        "significativité"
      ]
    },
    {
      "id": "qcm3-q08",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2$ ?",
      "choices": [
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}$"
      ],
      "answer": 1,
      "explanation": "Le coefficient de détermination $R^2$ est défini par $R^2 = \\dfrac{SCE}{SCT}$, où $SCE = \\sum (\\hat{Y}_i - \\bar{Y})^2$ est la somme des carrés expliquée et $SCT = \\sum (Y_i - \\bar{Y})^2$ est la somme des carrés totale.",
      "tags": [
        "régression linéaire",
        "$R^2$",
        "qualité d’ajustement"
      ]
    },
    {
      "id": "qcm3-q09",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On note $SCR$ la somme des carrés des résidus du modèle initial, et $SCR_c$ celle d’un sous-modèle contenant $p'$ variables ($p' < p$). Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n-p}{p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$"
      ],
      "answer": 3,
      "explanation": "Pour comparer un modèle complet (p variables) et un sous-modèle restreint (p' variables), on utilise un test de Fisher. La statistique est $F = \\dfrac{(SCR_c - SCR)/(p - p')}{SCR/(n-p)}$. En simplifiant, le numérateur est proportionnel à $\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$, ce qui correspond à la réponse D.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "comparaison de modèles"
      ]
    },
    {
      "id": "qcm3-q10",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Supposons qu’une constante est incluse dans le modèle. Quelle(s) propriété(s) vérifie le vecteur des résidus $\\hat{\\varepsilon}$ ?",
      "choices": [
        "Sa variance vaut $\\sigma^2 (X'X)^{-1}$",
        "Sa moyenne est nulle",
        "Sa variance vaut $\\mathrm{Var}(\\hat{\\varepsilon}) = \\sigma^2 P_{[X]^\\perp}$ ($= \\sigma^2 \\left(I_n - X (X'X)^{-1} X'\\right)$",
        "Son espérance est nulle",
        "Il est de taille $p$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        1,
        2,
        3
      ],
      "explanation": "Le vecteur des résidus est $\\hat{\\varepsilon} = M Y$ avec $M = I - P_X$, la matrice de projection sur $[X]^\\perp$. On a $\\mathbb{E}[\\hat{\\varepsilon}] = 0$, $\\mathrm{Var}(\\hat{\\varepsilon}) = \\sigma^2 M = \\sigma^2 P_{[X]^\\perp}$, et sa somme vaut 0 donc sa moyenne est nulle.",
      "tags": [
        "régression linéaire",
        "résidus",
        "espérance",
        "variance"
      ]
    },
    {
      "id": "qcm5-q01",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2 (X'X)^{-1}$",
        "$\\sigma^2 X \\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2 (XX')^{-1}$"
      ],
      "answer": [
        2
      ],
      "explanation": "Dans le modèle $Y = X\\beta + \\varepsilon$ avec $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, on a $\\mathrm{Var}(Y) = \\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$.",
      "tags": [
        "régression linéaire",
        "variance",
        "modèle linéaire"
      ]
    },
    {
      "id": "qcm5-q02",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Que vaut $\\hat{\\beta}$, l’estimateur par moindres carrés ordinaires de $\\beta$ ?",
      "choices": [
        "$\\mathbb{E}((X'X)^{-1} X'Y)$",
        "$(X'X)^{-1} X'Y$",
        "$\\mathbb{E}((XX')^{-1} XY)$",
        "$(XX')^{-1} XY$"
      ],
      "answer": [
        1
      ],
      "explanation": "Par définition de l’estimateur des moindres carrés ordinaires (MCO), on a $\\hat{\\beta} = (X'X)^{-1} X'Y$, obtenu en minimisant la somme des carrés des résidus $\\|Y - X\\beta\\|^2$.",
      "tags": [
        "régression linéaire",
        "estimateur",
        "MCO"
      ]
    },
    {
      "id": "qcm5-q03",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat{\\varepsilon}$ ? (On note $P_{[X]} = X (X'X)^{-1} X'$ et $P_{[X]}^{\\perp} = I_n - P_{[X]}$.)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition, $\\hat{\\varepsilon} = Y - X\\hat{\\beta} = (I_n - P_{[X]})Y = P_{[X]}^{\\perp} Y$. En utilisant $Y = X\\beta + \\varepsilon$, on obtient aussi $\\hat{\\varepsilon} = P_{[X]}^{\\perp} \\varepsilon$. En revanche, $P_{[X]} Y$ donne les valeurs ajustées $\\hat{Y}$, et utiliser $Y - X\\beta$ correspond à l'erreur vraie, pas aux résidus.",
      "tags": [
        "régression linéaire",
        "résidus",
        "projection",
        "matrices de projection"
      ]
    },
    {
      "id": "qcm5-q04",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle statistique constitue un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\dfrac{1}{n} \\sum_{i=1}^{n} \\varepsilon_i^{2}$",
        "$\\dfrac{1}{n} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$",
        "$\\dfrac{1}{n-p} \\sum_{i=1}^{n} \\varepsilon_i^{2}$",
        "$\\dfrac{1}{n-p} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$"
      ],
      "answer": [
        3
      ],
      "explanation": "Avec $M = I_n - P_{[X]}$ (rang $n-p$), la somme des carr\\u00e9s des r\\u00e9sidus est $\\text{RSS} = \\hat{\\varepsilon}'\\hat{\\varepsilon} = Y' M Y$. On a $\\mathbb{E}[\\text{RSS}] = (n-p)\\sigma^2$, d'o\\u00f9 l'estimateur sans biais $\\hat{\\sigma}^2 = \\dfrac{\\text{RSS}}{n-p} = \\dfrac{1}{n-p} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$.",
      "tags": [
        "régression linéaire",
        "variance",
        "r\\u00e9sidus",
        "degr\\u00e9s de libert\\u00e9"
      ]
    },
    {
      "id": "qcm5-q05",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Parmi les quantités suivantes, lesquelles sont observables ou calculables à partir des observations ?",
      "choices": [
        "$\\hat{\\beta}$",
        "$\\hat{Y}$",
        "$Y$",
        "$\\hat{\\varepsilon}$",
        "$\\beta$",
        "$\\sigma^2$",
        "$\\varepsilon$"
      ],
      "answer": [
        0,
        1,
        2,
        3
      ],
      "explanation": "Les quantités observables ou calculables directement à partir des données $(X, Y)$ sont : $\\hat{\\beta}$ (estimateur MCO), $\\hat{Y} = X\\hat{\\beta}$ (valeurs ajustées), $Y$ (observations) et $\\hat{\\varepsilon} = Y - \\hat{Y}$ (résidus). En revanche, $\\beta$, $\\sigma^2$ et $\\varepsilon$ sont inobservables et ne peuvent être connus qu'au travers d'estimateurs.",
      "tags": [
        "régression linéaire",
        "observables",
        "paramètres",
        "résidus"
      ]
    },
    {
      "id": "qcm5-q06",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelles sont les variables non corrélées ?",
      "choices": [
        "$\\hat{Y}_1$ et $\\hat{\\varepsilon}_1$",
        "$\\hat{\\beta}_1$ et $\\hat{\\beta}_2$",
        "$Y_1$ et $\\hat{Y}_1$",
        "$\\varepsilon_1$ et $\\hat{\\varepsilon}_1$",
        "$\\varepsilon_1$ et $\\varepsilon_2$",
        "$Y_1$ et $Y_2$",
        "$Y_1$ et $\\hat{\\varepsilon}_1$",
        "$\\hat{\\varepsilon}_1$ et $\\hat{\\varepsilon}_2$"
      ],
      "answer": [
        0,
        4,
        5
      ],
      "explanation": "Les valeurs ajustées et les résidus ($\\hat{Y}$ et $\\hat{\\varepsilon}$) sont orthogonaux, donc non corrélés. Les erreurs vraies $\\varepsilon_i$ sont indépendantes, donc non corrélées. Enfin, dans le cadre classique des MCO avec erreurs indépendantes, les $Y_i$ sont eux aussi non corrélés. En revanche, les autres couples sont en général corrélés.",
      "tags": [
        "régression linéaire",
        "corrélation",
        "résidus",
        "valeurs ajustées",
        "observations"
      ]
    },
    {
      "id": "qcm5-q07",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\mathrm{rg}(X)=p$ ?",
      "choices": [
        "$p \\le n$",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La corrélation entre les variables explicatives est nulle",
        "La matrice $X$ est inversible",
        "La matrice $X'X$ est inversible"
      ],
      "answer": [
        0,
        1,
        4
      ],
      "explanation": "Si $\\mathrm{rg}(X)=p$, les colonnes de $X$ sont linéairement indépendantes, donc nécessairement $p \\le n$. Par ailleurs, $X'X$ est alors définie positive et inversible. En revanche, $X$ n’est pas forcément carrée (donc pas forcément inversible) et le rang plein n’implique pas l’absence de corrélation entre variables explicatives.",
      "tags": [
        "régression linéaire",
        "rang",
        "matrices",
        "identifiabilité"
      ]
    },
    {
      "id": "qcm5-q08",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Qu’est-ce que la multicolinéarité dans un modèle de régression multiple ?",
      "choices": [
        "L’influence excessive d’une seule observation sur l’estimation du modèle",
        "L’interaction entre $Y$ et les variables explicatives",
        "L’erreur dans l’estimation des coefficients de régression",
        "La forte corrélation entre deux ou plusieurs variables explicatives"
      ],
      "answer": [
        3
      ],
      "explanation": "La multicolinéarité désigne une forte corrélation (souvent linéaire) entre deux ou plusieurs variables explicatives. Elle rend les colonnes de $X$ presque dépendantes, ce qui entraîne des problèmes numériques et une instabilité dans l’estimation de $\\hat{\\beta}$.",
      "tags": [
        "régression linéaire",
        "multicolinéarité",
        "variables explicatives"
      ]
    },
    {
      "id": "qcm5-q09",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-p}{n-1} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{n-1}{n-p} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-p}{n-1} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-1}{n-p} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$"
      ],
      "answer": [
        3
      ],
      "explanation": "L’indice de détermination ajusté est défini par $R^2_a = 1 - \\dfrac{n-1}{n-p} \\cdot \\dfrac{\\text{SCR}}{\\text{SCT}}$, où $\\text{SCR} = \\sum (Y_i - \\hat{Y}_i)^2$ et $\\text{SCT} = \\sum (Y_i - \\bar{Y})^2$. Cette correction par $\\tfrac{n-1}{n-p}$ permet de compenser la tendance du $R^2$ à augmenter avec le nombre de variables explicatives.",
      "tags": [
        "régression linéaire",
        "R^2 ajusté",
        "variance expliquée",
        "qualité d’ajustement"
      ]
    },
    {
      "id": "qcm5-q10",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On note $R^2$ le $R^2$ obtenu après estimation du modèle initial, et $R^2_c$ celui d’un sous-modèle contenant $p'$ variables ($p' < p$) dont une constante. Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n - p}{p'} \\dfrac{R^2_c - R^2}{1 - R^2}$",
        "$\\dfrac{n - p}{p - p'} \\dfrac{R^2_c - R^2}{1 - R^2}$",
        "$\\dfrac{n - p}{p - p'} \\dfrac{R^2 - R^2_c}{1 - R^2}$",
        "$\\dfrac{n - p}{p'} \\dfrac{R^2 - R^2_c}{1 - R^2}$"
      ],
      "answer": [
        2
      ],
      "explanation": "La statistique de Fisher utilisée pour comparer un modèle complet à un sous-modèle imbriqué est $F = \\dfrac{(R^2 - R_c^2)/(p - p')}{(1 - R^2)/(n - p)}$, ce qui équivaut à $\\dfrac{n - p}{p - p'} \\cdot \\dfrac{R^2 - R_c^2}{1 - R^2}$. Cette statistique suit une loi de Fisher à $(p - p')$ et $(n - p)$ degrés de liberté sous l’hypothèse nulle.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "R^2",
        "comparaison de modèles"
      ]
    },
    {
      "id": "qcm5-q11",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle méthode permet de détecter une observation ayant une influence excessive sur un modèle de régression multiple ?",
      "choices": [
        "Le test de Durbin-Watson",
        "Le test de Breusch-Godfrey",
        "Le test de Shapiro-Wilk",
        "La distance de Cook"
      ],
      "answer": [
        3
      ],
      "explanation": "La distance de Cook mesure l’influence d’une observation sur l’estimation des coefficients de régression. Une valeur élevée indique qu’une seule observation exerce une influence excessive sur le modèle. Les autres tests cités concernent respectivement l’autocorrélation (Durbin-Watson, Breusch-Godfrey) et la normalité (Shapiro-Wilk).",
      "tags": [
        "régression linéaire",
        "influence",
        "diagnostic",
        "distance de Cook"
      ]
    },
    {
      "id": "qcm5-q12",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ?",
      "choices": [
        "Le $R^2$ augmente",
        "La SCR diminue",
        "Le $R^2_a$ diminue",
        "Le $R^2$ diminue",
        "La SCR augmente",
        "Le $R^2_a$ augmente"
      ],
      "answer": [
        0,
        1
      ],
      "explanation": "L’ajout d’une variable explicative au modèle entraîne toujours une diminution (ou au pire une constance) de la somme des carrés des résidus (SCR), ce qui implique une augmentation (ou constance) du $R^2$. En revanche, le $R^2_a$ peut augmenter ou diminuer selon la pertinence de la variable ajoutée.",
      "tags": [
        "régression linéaire",
        "R^2",
        "SCR",
        "ajout de variables"
      ]
    },
    {
      "id": "qcm5-q13",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On compare plusieurs modèles avec les critères AIC et BIC.",
      "choices": [
        "Le meilleur modèle au sens du BIC ne coïncide pas forcément avec le meilleur modèle au sens de l’AIC.",
        "Le meilleur modèle est automatiquement détecté : il s’agit de celui qui maximise l’AIC et qui maximise le BIC.",
        "Le BIC choisira un modèle de taille plus petite ou égale à celui choisi par l’AIC",
        "Il est possible que AIC et BIC soient tous les deux optimaux pour le plus gros modèle",
        "Si les modèles ont des tailles différentes, AIC choisira forcément le plus petit",
        "Si les modèles ont des tailles différentes, BIC choisira forcément le plus petit",
        "L’AIC choisira un modèle de taille plus petite ou égale à celui choisi par le BIC",
        "Le meilleur modèle est automatiquement détecté : il s’agit de celui qui minimise l’AIC et qui minimise le BIC."
      ],
      "answer": [
        0,
        2,
        3
      ],
      "explanation": "AIC et BIC sont deux critères d’information qui se minimisent. Le BIC pénalise davantage la complexité, ce qui conduit souvent à choisir un modèle plus petit ou égal à celui de l’AIC. Les deux critères peuvent mener à des choix différents, mais il arrive qu’ils coïncident. En revanche, aucun ne se maximise.",
      "tags": [
        "régression linéaire",
        "AIC",
        "BIC",
        "comparaison de modèles"
      ]
    },
    {
      "id": "qcm5-q14",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On a supposé $V(\\varepsilon) = \\sigma^2 I_n$. Si l’erreur de modélisation $\\varepsilon$ était hétéroscédastique, que deviendrait la matrice $V(\\varepsilon)$ ?",
      "choices": [
        "Les valeurs sur sa diagonale seraient différentes",
        "Elle ne serait pas inversible",
        "Elle ne serait plus diagonale",
        "Les valeurs sur la diagonale seraient nulles"
      ],
      "answer": [
        0
      ],
      "explanation": "En cas d’hétéroscédasticité, les erreurs ont des variances différentes mais restent supposées indépendantes. Ainsi $V(\\varepsilon) = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$ : la matrice reste diagonale mais ses valeurs sur la diagonale diffèrent. Elle demeure inversible tant qu’aucune variance n’est nulle.",
      "tags": [
        "régression linéaire",
        "hétéroscédasticité",
        "variance des erreurs",
        "matrice de variance"
      ]
    }
  ]
}
