{
  "questions": [
    {
      "id": "ols-q001",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "On se place dans le modèle $Y=X\\beta+\\varepsilon$ (\\operatorname{rg}(X)=p)). Quelles hypothèses fait-on sur $\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2(XX')^{-1}$",
        "Sa moyenne empirique est nulle",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Son espérance est nulle",
        "Il est non-corrélé à $Y$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Hypothèses usuelles : $\\mathbb E[\\varepsilon]=0$ et $\\operatorname{Var}(\\varepsilon)=\\sigma^2 I_n$. En revanche $\\varepsilon$ n’est pas non-corrélé à $Y$ puisque $Y=X\\beta+\\varepsilon$.",
      "tags": [
        "OLS",
        "hypothèses",
        "Gauss–Markov"
      ]
    },
    {
      "id": "ols-q002",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Que vaut l’estimateur MCO (OLS) de $\\beta$ ?",
      "choices": [
        "$\\mathbb E\\big((XX')^{-1}XY\\big)$",
        "$(X'X)^{-1}X'Y$",
        "$(XX')^{-1}XY$",
        "$\\mathbb E\\big((X'X)^{-1}XY\\big)$"
      ],
      "answer": 1,
      "explanation": "Formule classique des MCO : $\\hat\\beta=(X'X)^{-1}X'Y$.",
      "tags": [
        "OLS",
        "estimateur",
        "formule"
      ]
    },
    {
      "id": "linalg-q003",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Que signifie $\\operatorname{rg}(X)=p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p\\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : colonnes linéairement indépendantes, donc $X'X$ inversible et nécessairement $p\\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ]
    },
    {
      "id": "test-q004",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur du test de Student pour le coefficient de $X^{(j)}$ vaut 0,023, que concluez-vous ?",
      "choices": [
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul.",
        "Le modèle est surajusté.",
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$."
      ],
      "answer": 3,
      "explanation": "Avec un seuil usuel $\\alpha=5\\%$, $p=0,023 < 0,05$ : on rejette $H_0$ (« coefficient nul »). Donc le coefficient est significatif.",
      "tags": [
        "test de Student",
        "p-value",
        "significativité"
      ],
      "id_gen": [
        "qcm005_2025-10-10"
      ]
    },
    {
      "id": "res-q005",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "test-q006",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Supposons que pour un test statistique donné, la région critique associée au risque de première espèce $\\alpha = 5\\%$ est $\\{|T| > 3.1\\}$, où $T$ désigne la statistique de test. On observe $T = -4$. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test."
      ],
      "answer": [
        4,
        3,
        5
      ],
      "explanation": "Comme $|T|=4 > 3.1$, on est dans la région critique pour $\\alpha=5\\%$ et $\\alpha=10\\%$ donc on rejette $H_0$. Mais $|T|$ n’est pas assez extrême pour rejeter à $\\alpha=1\\%$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "p-value"
      ],
      "id_gen": [
        "qcm004_2025-10-10"
      ]
    },
    {
      "id": "test-q007",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Pour le même test qu’à la question précédente, on note $F$ la fonction de répartition de $T$ sous $H_0$. On observe toujours $T=-4$. Que vaut alors la p-value ?",
      "choices": [
        "$1 - F(-4)$",
        "$F(-4)$",
        "$2F(-4)$",
        "$2(1 - F(-4))$"
      ],
      "answer": 2,
      "explanation": "La p-value bilatérale est $2\\times F(-4)$.",
      "tags": [
        "tests statistiques",
        "p-value",
        "loi de test"
      ]
    },
    {
      "id": "var-q008",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique effectuée auprès d’étudiants, on relève la mention obtenue au BAC. Cette variable peut être considérée comme :",
      "choices": [
        "qualitative ordinale",
        "quantitative continue",
        "qualitative nominale",
        "quantitative discrète"
      ],
      "answer": 0,
      "explanation": "Les mentions (« passable », « bien », « très bien ») ont un ordre naturel → qualitative ordinale.",
      "tags": [
        "nature des variables",
        "qualitative",
        "ordinale"
      ],
      "id_gen": [
        "qcm005_2025-10-10"
      ]
    },
    {
      "id": "test-q009",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique, on souhaite étudier le lien entre le pays d’origine des individus observés et leur souscription (ou non) à une assurance vie. Quel outil vous semble adapté ?",
      "choices": [
        "La mise en place d’un test du chi-deux",
        "La représentation d’un nuage de points",
        "Une représentation graphique à l’aide de boxplots",
        "Le calcul de la corrélation de Pearson"
      ],
      "answer": 0,
      "explanation": "Deux variables qualitatives → test du chi² d’indépendance.",
      "tags": [
        "test du chi²",
        "indépendance",
        "qualitative"
      ]
    },
    {
      "id": "ols-q010",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Dans l’écriture classique d’un modèle de régression linéaire concernant $n$ individus, $Y = X\\beta + \\varepsilon$, (plusieurs réponses possibles)",
      "choices": [
        "$Y$ et $X$ sont des vecteurs de taille $n$",
        "$Y$ est toujours considéré comme aléatoire",
        "$\\varepsilon$ est toujours considéré comme aléatoire",
        "$Y$ et $\\beta$ sont des vecteurs de taille $n$",
        "$X$ est une matrice"
      ],
      "answer": [
        1,
        2,
        4
      ],
      "explanation": "$Y$ est un vecteur aléatoire ($n\\times 1$), $\\varepsilon$ aussi. $X$ est une matrice ($n\\times p$). $\\beta$ est un vecteur $p\\times 1$, donc pas de taille $n$.",
      "tags": [
        "OLS",
        "modèle de régression",
        "dimensions"
      ]
    },
    {
      "id": "test-q011",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Supposons que lors de l’application d’un test statistique, on observe une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test."
      ],
      "answer": [
        2,
        4
      ],
      "explanation": "Comme $p = 0,037 < 0,05$, on rejette l’hypothèse nulle au seuil 5% et 10%. En revanche, $p > 0,01$ donc pas de rejet au seuil 1%.",
      "tags": [
        "tests statistiques",
        "p-value",
        "seuil de risque"
      ],
      "id_gen": [
        "qcm004_2025-10-10",
        "qcm005_2025-10-10"
      ]
    },
    {
      "id": "test-q012",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Pour un test donné, la région critique associée à un risque de première espèce $\\alpha$ est $\\{|T| > a\\}$, où $T$ suit une loi continue. On a observé $T=-2$ et une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour $\\alpha=5\\%$, $a < 2$",
        "Pour $\\alpha=3,7\\%$, $a > 2$",
        "Pour $\\alpha=3,7\\%$, $a < 2$",
        "Pour $\\alpha=3,7\\%$, $a = 2$",
        "Pour $\\alpha=5\\%$, $a > 2$",
        "Pour $\\alpha=5\\%$, $a = 2$"
      ],
      "answer": [
        0,
        3
      ],
      "explanation": "La p-value est $0,037$, ce qui correspond à un seuil critique $\\alpha \\approx 3,7\\%$. Donc pour $\\alpha=5\\%$, on rejette l’hypothèse nulle et $a < 2$. Pour $\\alpha=3,7\\%$, $T$ est exactement sur la frontière, donc on a $a = 2$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "p-value"
      ]
    },
    {
      "id": "var-q013",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique effectuée auprès d’étudiants, on relève le code postal du lycée dont ils proviennent. Cette variable peut être considérée comme :",
      "choices": [
        "qualitative nominale",
        "quantitative continue",
        "qualitative ordinale",
        "quantitative discrète"
      ],
      "answer": 0,
      "explanation": "Le code postal est une simple étiquette servant à identifier une zone géographique, sans ordre naturel ni signification métrique → variable qualitative nominale.",
      "tags": [
        "nature des variables",
        "qualitative",
        "nominale"
      ]
    },
    {
      "id": "visu-q014",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique, on souhaite étudier le lien entre le pays d’origine des individus observés et leur revenu annuel. Quel outil vous semble adapté ?",
      "choices": [
        "La mise en place d’un test du chi-deux",
        "Le calcul de la corrélation de Pearson",
        "La représentation d’un nuage de points",
        "Une représentation graphique à l’aide de boxplots"
      ],
      "answer": 3,
      "explanation": "Le pays d’origine est une variable qualitative et le revenu une variable quantitative → on compare les distributions avec des boxplots.",
      "tags": [
        "visualisation",
        "qualitative vs quantitative",
        "boxplots"
      ],
      "id_gen": [
        "qcm003_2025-10-10"
      ]
    },
    {
      "id": "ols-q015",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "En pratique, lorsqu’on met en place un modèle de régression linéaire $Y = X\\beta + \\varepsilon$, quelles quantités sont connues (observées) ?",
      "choices": [
        "$Y$ et $X\\beta$",
        "$X\\beta$",
        "$X, Y$ et $\\varepsilon$",
        "$X$ et $Y$"
      ],
      "answer": 3,
      "explanation": "En pratique, on observe $X$ (variables explicatives) et $Y$ (variable dépendante). Les paramètres $\\beta$ et les erreurs $\\varepsilon$ sont inconnus.",
      "tags": [
        "OLS",
        "régression linéaire",
        "observables"
      ],
      "id_gen": [
        "qcm004_2025-10-10"
      ]
    },
    {
      "id": "var-q016",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Si $\\mathbb E(\\varepsilon)=0$ et $\\operatorname{Var}(\\varepsilon)=\\sigma^2 I_n$, que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2(X'X)^{-1}$",
        "$\\sigma^2X\\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2(XX')^{-1}$"
      ],
      "answer": 2,
      "explanation": "En effet, $\\operatorname{Var}(Y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\sigma^2 I_n$.",
      "tags": [
        "variance",
        "régression",
        "OLS"
      ],
      "id_gen": [
        "qcm001_2025-10-10",
        "qcm006_2025-10-10"
      ]
    },
    {
      "id": "res-q017",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "var-q018",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Quelle statistique constitue un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\tfrac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum_{i=1}^n \\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n-p}\\sum_{i=1}^n \\varepsilon_i^2$"
      ],
      "answer": 0,
      "explanation": "L’estimateur sans biais de la variance est $\\hat\\sigma^2 = \\tfrac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2$.",
      "tags": [
        "variance",
        "estimateur sans biais",
        "OLS"
      ],
      "id_gen": [
        "qcm004_2025-10-10",
        "qcm002_2025-10-24",
        "qcm003_2025-10-24"
      ]
    },
    {
      "id": "test-q019",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Student associée au coefficient de régression de la variable $X^{(j)}$ vaut $-23$, que concluez-vous ?",
      "choices": [
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul.",
        "Le modèle est surajusté.",
        "Relation statistiquement significative entre $X^{(j)}$ et $Y$."
      ],
      "answer": 3,
      "explanation": "Avec une statistique $t = -23$ (valeur absolue extrêmement élevée), on rejette l’hypothèse nulle $H_0$. Il existe donc une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "test de Student",
        "significativité",
        "régression"
      ]
    },
    {
      "id": "ols-q020",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que peut-on dire de l’estimateur MCO de $\\beta$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2$",
        "C’est le meilleur estimateur parmi tous les estimateurs sans biais",
        "C’est un estimateur sans biais",
        "Son espérance est nulle",
        "Sa variance vaut $\\hat\\sigma^2/(n-p)$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "C’est le meilleur estimateur parmi tous les estimateurs linéaires et sans biais",
        "C’est le meilleur estimateur parmi tous les estimateurs",
        "C’est le meilleur estimateur parmi tous les estimateurs consistants"
      ],
      "answer": [
        2,
        5,
        6
      ],
      "explanation": "Théorème de Gauss–Markov : l’estimateur MCO est sans biais, sa variance est $\\sigma^2(X'X)^{-1}$ et il est le meilleur estimateur linéaire sans biais (BLUE).",
      "tags": [
        "OLS",
        "Gauss–Markov",
        "BLUE",
        "estimateur"
      ],
      "id_gen": [
        "qcm002_2025-10-24"
      ]
    },
    {
      "id": "test-q021",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Supposons que lors de l’application d’un test statistique, on observe une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test."
      ],
      "answer": [
        2,
        4
      ],
      "explanation": "Comme $p = 0,037 < 0,05$, on rejette $H_0$ au seuil de 5% et de 10%. En revanche, $p > 0,01$ donc pas de rejet au seuil de 1%.",
      "tags": [
        "tests statistiques",
        "p-value",
        "seuil de risque"
      ],
      "id_gen": [
        "qcm002_2025-10-10"
      ]
    },
    {
      "id": "ols-q022",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On se place dans le modèle $Y = X\\beta + \\varepsilon$ (rang($X$) = $p$). Quelles hypothèses fait-on sur $\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2(XX')^{-1}$",
        "Sa moyenne empirique est nulle",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Son espérance est nulle",
        "Il est non-corrélé à $Y$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Hypothèses usuelles : $\\mathbb E[\\varepsilon] = 0$ et $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I_n$. En revanche, $\\varepsilon$ n’est pas non-corrélé à $Y$ puisque $Y = X\\beta + \\varepsilon$.",
      "tags": [
        "OLS",
        "hypothèses",
        "Gauss–Markov"
      ],
      "id_gen": [
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "linalg-q023",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\operatorname{rg}(X) = p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p \\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : les colonnes sont linéairement indépendantes, donc $X'X$ est inversible et nécessairement $p \\le n$. La matrice $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ]
    },
    {
      "id": "ols-q024",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que vaut l’estimateur MCO de $\\beta$ ?",
      "choices": [
        "$\\mathbb E\\big((XX')^{-1}XY\\big)$",
        "$(X'X)^{-1}X'Y$",
        "$(XX')^{-1}XY$",
        "$\\mathbb E\\big((X'X)^{-1}XY\\big)$"
      ],
      "answer": 1,
      "explanation": "Formule classique des MCO : $\\hat\\beta = (X'X)^{-1}X'Y$.",
      "tags": [
        "OLS",
        "estimateur",
        "formule"
      ],
      "id_gen": [
        "qcm001_2025-10-10"
      ]
    },
    {
      "id": "res-q025",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelles relations sont vraies ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "test-q026",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Supposons que pour un test statistique donné, la région critique associée au risque de première espèce $\\alpha = 5\\%$ est $\\{|T| > 3.1\\}$, où $T$ désigne la statistique de test. On observe $T = -4$. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test."
      ],
      "answer": [
        4,
        3,
        5
      ],
      "explanation": "Comme $|T| = 4 > 3.1$, on est dans la région critique pour $\\alpha = 5\\%$ et $\\alpha = 10\\%$ donc on rejette $H_0$. Mais la valeur n’est pas assez extrême pour conclure au seuil $\\alpha = 1\\%$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "régression"
      ],
      "id_gen": [
        "qcm002_2025-10-10",
        "qcm006_2025-10-10"
      ]
    },
    {
      "id": "ols-q027",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On se place dans un modèle de régression linéaire standard $Y = X\\beta + \\varepsilon$, où $\\beta \\in \\mathbb{R}^p$, $X$ est une matrice déterministe de taille $n \\times p$ de rang $p$, et $\\varepsilon$ est un vecteur aléatoire de taille $n$. Quelles hypothèses fait-on sur $\\varepsilon = (\\varepsilon_1,\\dots,\\varepsilon_n)$ ? (plusieurs réponses possibles)",
      "choices": [
        "$E[\\varepsilon_i \\varepsilon_j] = 0$ pour $i \\neq j$",
        "$E[\\varepsilon_i] = 0$",
        "$E[\\varepsilon_i Y_i] = 0$",
        "$Var(\\varepsilon_i) = \\sigma^2(XX')^{-1}_{ii}$",
        "$Var(\\varepsilon_i) = 0$",
        "$\\tfrac{1}{n} \\sum_i \\varepsilon_i = 0$"
      ],
      "answer": [
        0,
        1
      ],
      "explanation": "On suppose : espérance nulle ($E[\\varepsilon_i]=0$), non-corrélation des erreurs entre elles ($E[\\varepsilon_i \\varepsilon_j]=0$ pour $i\\neq j$), et variance homogène $Var(\\varepsilon_i) = \\sigma^2$. Les autres propositions sont fausses.",
      "tags": [
        "OLS",
        "hypothèses",
        "indépendance",
        "homoscédasticité"
      ],
      "id_gen": [
        "qcm006_2025-10-10"
      ]
    },
    {
      "id": "ols-q028",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que peut-on dire de l’estimateur MCO de $\\beta$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2$",
        "C’est le meilleur estimateur parmi tous les estimateurs sans biais",
        "Il s'agit d'un estimateur sans biais",
        "Son espérance est nulle",
        "Sa variance vaut $\\hat\\sigma^2/(n-p)$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "C’est le meilleur estimateur parmi tous les estimateurs linéaires et sans biais",
        "C’est le meilleur estimateur parmi tous les estimateurs",
        "C’est le meilleur estimateur parmi tous les estimateurs consistants"
      ],
      "answer": [
        2,
        5,
        6
      ],
      "explanation": "D’après le théorème de Gauss–Markov : l’estimateur MCO est sans biais, sa variance est $\\sigma^2(X'X)^{-1}$ et il est le meilleur estimateur linéaire sans biais (BLUE).",
      "tags": [
        "OLS",
        "Gauss–Markov",
        "BLUE",
        "estimateur"
      ]
    },
    {
      "id": "res-q029",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelles relations sont vraies ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I - P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "var-q030",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelle statistique donne un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\tfrac{1}{n-p}\\sum \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum \\varepsilon_i^2$",
        "$\\tfrac{1}{n-p}\\sum \\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum \\hat\\varepsilon_i^2$"
      ],
      "answer": 0,
      "explanation": "L’estimateur sans biais de la variance est $s^2 = \\tfrac{1}{n-p}\\sum \\hat\\varepsilon_i^2$.",
      "tags": [
        "variance",
        "estimateur sans biais",
        "résidus"
      ],
      "id_gen": [
        "qcm002_2025-10-10",
        "qcm003_2025-10-10"
      ]
    },
    {
      "id": "var-q031",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2(X'X)^{-1}$",
        "$\\sigma^2 X\\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2(XX')^{-1}$"
      ],
      "answer": 2,
      "explanation": "Puisque $Y = X\\beta + \\varepsilon$ avec $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I_n$, on a $\\operatorname{Var}(Y) = \\sigma^2 I_n$.",
      "tags": [
        "variance",
        "régression",
        "moments"
      ],
      "id_gen": [
        "qcm002_2025-10-10",
        "qcm002_2025-10-28"
      ]
    },
    {
      "id": "multi-q032",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Qu’est-ce que la multicolinéarité dans un modèle de régression multiple ?",
      "choices": [
        "L’erreur dans l’estimation des coefficients de régression",
        "L’interaction entre $Y$ et les variables explicatives",
        "L’influence excessive d’une seule observation sur l'estimation du modèle",
        "La forte corrélation entre deux ou plusieurs variables explicatives"
      ],
      "answer": 3,
      "explanation": "La multicolinéarité correspond à une forte corrélation linéaire entre deux ou plusieurs variables explicatives, ce qui rend $X'X$ proche de la singularité.",
      "tags": [
        "régression multiple",
        "multicolinéarité",
        "colinéarité"
      ],
      "id_gen": [
        "qcm003_2025-10-28"
      ]
    },
    {
      "id": "fisher-q033",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On note SCR la somme des carrés des résidus du modèle initial, et $SCR_c$ celle d’un sous-modèle contenant $p'$ variables ($p'<p$). Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n-p}{p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR - SCR_c}{SCR}$"
      ],
      "answer": 2,
      "explanation": "La statistique de Fisher utilisée est $F = \\dfrac{(SCR_c - SCR)/(p - p')}{SCR/(n - p)} = \\dfrac{n - p}{p - p'}\\dfrac{SCR_c - SCR}{SCR}$.",
      "tags": [
        "tests de Fisher",
        "comparaison de modèles",
        "régression multiple"
      ],
      "id_gen": [
        "qcm006_2025-10-10",
        "qcm003_2025-10-28"
      ]
    },
    {
      "id": "r2-q034",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ? (plusieurs réponses possibles)",
      "choices": [
        "Le $R^2$ augmente",
        "Le $R^2_a$ diminue",
        "La SCR diminue",
        "Le $R^2$ diminue",
        "La SCR augmente",
        "Le $R^2_a$ augmente"
      ],
      "answer": [
        0,
        2
      ],
      "explanation": "L’ajout d’une variable explique au moins autant la variance, donc $R^2$ ne peut qu’augmenter et la SCR diminuer. En revanche, $R^2_a$ peut augmenter ou baisser selon la pertinence de la variable.",
      "tags": [
        "régression multiple",
        "$R^2$",
        "SCR"
      ],
      "id_gen": [
        "qcm001_2025-10-28",
        "qcm003_2025-10-28"
      ]
    },
    {
      "id": "hetero-q035",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Dans le contexte d’une régression multiple, qu’est-ce que l’hétéroscédasticité ?",
      "choices": [
        "Le fait que la variance des $\\varepsilon_i$ n’est pas constante",
        "Le fait que les $\\varepsilon_i$ ne sont pas corrélés entre eux",
        "Le fait que la relation entre $Y$ et les explicatives n’est pas linéaire",
        "Le fait que les coefficients du modèle sont biaisés"
      ],
      "answer": 0,
      "explanation": "L’hétéroscédasticité correspond à une variance non constante des erreurs : $\\operatorname{Var}(\\varepsilon_i) = \\sigma_i^2$ dépend de $i$.",
      "tags": [
        "régression multiple",
        "hétéroscédasticité",
        "hypothèses"
      ],
      "id_gen": [
        "qcm001_2025-10-28"
      ]
    },
    {
      "id": "res-q036",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Supposons qu’une constante est incluse dans le modèle. Quelle(s) propriété(s) vérifie(nt) le vecteur des résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2 I_n$",
        "Il est non corrélé à $Y$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Sa moyenne est nulle",
        "Il est de taille $p$",
        "Il est non corrélé à $\\hat Y$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Les résidus vérifient $\\mathbb E[\\hat\\varepsilon] = 0$ (si constante incluse) et sont orthogonaux aux valeurs ajustées ($\\hat Y$).",
      "tags": [
        "résidus",
        "régression multiple",
        "propriétés"
      ],
      "id_gen": [
        "qcm002_2025-10-10",
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "linalg-q037",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\operatorname{rg}(X) = p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p \\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : les colonnes de $X$ sont linéairement indépendantes, donc $X'X$ est inversible et nécessairement $p \\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ],
      "id_gen": [
        "qcm007_2025-10-10",
        "qcm004_2025-10-28"
      ]
    },
    {
      "id": "r2-q038",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2$ ?",
      "choices": [
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar Y)^2}{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\hat Y_i)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\hat Y_i)^2}$"
      ],
      "answer": 2,
      "explanation": "Définition : $R^2 = \\dfrac{\\text{Somme des carrés expliquée}}{\\text{Somme totale des carrés}} = \\dfrac{\\sum (\\hat Y_i - \\bar Y)^2}{\\sum (Y_i - \\bar Y)^2}$.",
      "tags": [
        "$R^2$",
        "qualité d’ajustement",
        "régression"
      ],
      "id_gen": [
        "qcm001_2025-10-10",
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "fisher-q039",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Fisher de significativité globale du modèle vaut 41, que concluez-vous ?",
      "choices": [
        "Toutes les variables explicatives sont significatives.",
        "Aucune variable explicative, autre que la constante, n’est significative dans le modèle.",
        "Toutes les variables explicatives, autre que la constante, sont significatives.",
        "Au moins une variable autre que la constante est significative."
      ],
      "answer": 3,
      "explanation": "Un test de Fisher global très significatif ($F=41$) permet de rejeter $H_0$ et de conclure qu’au moins une variable explicative influence $Y$.",
      "tags": [
        "tests de Fisher",
        "significativité globale",
        "régression multiple"
      ],
      "id_gen": [
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "homo-q040",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Dans le contexte d’une régression multiple, qu’est-ce que l’homoscedasticité ?",
      "choices": [
        "Le fait que les $\\varepsilon_i$ ne sont pas corrélés entre eux",
        "Le fait que la relation entre $Y$ et les variables explicatives est linéaire",
        "Le fait que l’estimateur MCO $\\hat\\beta$ est sans biais",
        "Le fait que la variance des $\\varepsilon_i$ est constante"
      ],
      "answer": 3,
      "explanation": "Homoscedasticité : $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$ pour tout $i$. Cela signifie que la variance est constante mais pas nécessairement que les erreurs soient indépendantes.",
      "tags": [
        "régression multiple",
        "homoscédasticité",
        "hypothèses"
      ],
      "id_gen": [
        "qcm006_2025-10-10",
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "r2a-q041",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$1-\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$1-\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$"
      ],
      "answer": 3,
      "explanation": "Le $R^2_a$ corrige le $R^2$ en tenant compte du nombre de variables explicatives $p$. La bonne formule est $R^2_a = 1 - \\dfrac{n-1}{n-p}\\dfrac{SCR}{SCT}$.",
      "tags": [
        "$R^2_a$",
        "qualité d’ajustement",
        "régression multiple"
      ],
      "id_gen": [
        "qcm002_2025-10-28"
      ]
    },
    {
      "id": "qcm3-q01",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On considère le cas d’un modèle de régression simple : on observe le vecteur y et le vecteur x, et on estime la droite des moindres carrés associée. Que vaut l’estimation de la pente ?",
      "choices": [
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$",
        "$\\dfrac{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}{\\mathrm{cov}(x,y)}$",
        "$\\dfrac{\\mathrm{var}(x)}{\\mathrm{cov}(x,y)}$",
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}$"
      ],
      "answer": 0,
      "explanation": "Dans un modèle de régression simple $y = \\alpha + \\beta x + \\varepsilon$, l’estimateur des moindres carrés de la pente est $\\hat{\\beta} = \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$.",
      "tags": [
        "régression simple",
        "moindres carrés",
        "estimateur de pente"
      ],
      "id_gen": [
        "qcm001_2025-10-10",
        "qcm002_2025-10-28",
        "qcm003_2025-10-28"
      ]
    },
    {
      "id": "qcm3-q02",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur du test de Student associé au coefficient de régression de la variable $X^{(j)}$ vaut 0.003, que concluez-vous ?",
      "choices": [
        "Le modèle est surajusté.",
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul."
      ],
      "answer": 1,
      "explanation": "Une p-valeur de 0.003 est très inférieure au seuil classique de 5%. On rejette donc l’hypothèse nulle $H_0: \\beta_j = 0$ et on conclut qu’il existe une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "régression linéaire",
        "test de Student",
        "p-valeur",
        "significativité"
      ],
      "id_gen": [
        "qcm005_2025-10-10"
      ]
    },
    {
      "id": "qcm3-q03",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$"
      ],
      "answer": 2,
      "explanation": "Le $R^2_a$ corrige le $R^2$ en tenant compte du nombre de variables explicatives $p$. La bonne formule est $R^2_a = 1 - \\dfrac{n-1}{n-p}\\dfrac{SCR}{SCT}$, où $SCR = \\sum (Y_i - \\hat{Y}_i)^2$ et $SCT = \\sum (Y_i - \\bar{Y})^2$.",
      "tags": [
        "régression linéaire",
        "$R^2_a$",
        "qualité d’ajustement",
        "régression multiple"
      ],
      "id_gen": [
        "qcm001_2025-10-28"
      ]
    },
    {
      "id": "qcm3-q04",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Fisher de significativité globale du modèle vaut -41, que concluez-vous ?",
      "choices": [
        "Au moins une variable autre que la constante est significative.",
        "Toutes les variables explicatives sont significatives.",
        "Aucune variable explicative, autre que la constante, n’est significative dans le modèle.",
        "Toutes les variables explicatives, autre que la constante, sont significatives."
      ],
      "answer": 0,
      "explanation": "Le test de Fisher global permet de tester l’hypothèse nulle $H_0 : \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$. Si la statistique est significative, on rejette $H_0$ et on conclut qu’au moins une variable explicative (autre que la constante) a un effet significatif sur $Y$.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "significativité globale"
      ],
      "id_gen": [
        "qcm003_2025-10-24"
      ]
    },
    {
      "id": "qcm3-q05",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ?",
      "choices": [
        "La SCR diminue",
        "Le $R^2$ diminue",
        "Le $R^2_a$ diminue",
        "Le $R^2$ augmente",
        "Le $R^2_a$ augmente",
        "La SCR augmente"
      ],
      "answer": [
        0,
        3
      ],
      "explanation": "En ajoutant une variable explicative, la somme des carrés des résidus (SCR) ne peut qu’augmenter la qualité d’ajustement ou la laisser identique. Ainsi, la SCR diminue (ou reste identique) et le $R^2$ augmente (ou reste identique). En revanche, le $R^2_a$ peut augmenter ou diminuer selon la pertinence de la variable ajoutée.",
      "tags": [
        "régression linéaire",
        "SCR",
        "$R^2$",
        "$R^2_a$"
      ],
      "id_gen": [
        "qcm003_2025-10-24",
        "qcm001_2025-10-28"
      ]
    },
    {
      "id": "qcm3-q06",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On considère le cas d’un modèle de régression simple : on observe le vecteur y et le vecteur x, et on estime la droite des moindres carrés associée. Que vaut l’estimation de l’ordonnée à l’origine ?",
      "choices": [
        "$\\bar{y} - \\dfrac{\\mathrm{cov}(y,x)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}\\,\\bar{x}$",
        "$\\dfrac{\\mathrm{cov}(y,x)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}$",
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$",
        "$\\bar{y} - \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}\\,\\bar{x}$"
      ],
      "answer": 3,
      "explanation": "Dans la régression simple $y = \\alpha + \\beta x + \\varepsilon$, on a $\\hat{\\beta} = \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$ et donc $\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\,\\bar{x}$. Cela donne bien $\\hat{\\alpha} = \\bar{y} - \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}\\,\\bar{x}$.",
      "tags": [
        "régression linéaire",
        "moindres carrés",
        "ordonnée à l’origine"
      ],
      "id_gen": [
        "qcm002_2025-10-28"
      ]
    },
    {
      "id": "qcm3-q07",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "La statistique du test Student associé au coefficient de régression de la variable $X^{(j)}$ vaut -23, que concluez-vous ?",
      "choices": [
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
        "Ce coefficient de régression est nul.",
        "Le modèle de régression est inadéquat.",
        "Le modèle est surajusté."
      ],
      "answer": 0,
      "explanation": "Une statistique de Student de grande valeur absolue (ici -23) implique une p-valeur extrêmement faible. On rejette donc $H_0 : \\beta_j = 0$ et on conclut qu’il existe une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "régression linéaire",
        "test de Student",
        "significativité"
      ]
    },
    {
      "id": "qcm3-q08",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2$ ?",
      "choices": [
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}$"
      ],
      "answer": 1,
      "explanation": "Le coefficient de détermination $R^2$ est défini par $R^2 = \\dfrac{SCE}{SCT}$, où $SCE = \\sum (\\hat{Y}_i - \\bar{Y})^2$ est la somme des carrés expliquée et $SCT = \\sum (Y_i - \\bar{Y})^2$ est la somme des carrés totale.",
      "tags": [
        "régression linéaire",
        "$R^2$",
        "qualité d’ajustement"
      ]
    },
    {
      "id": "qcm3-q09",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On note $SCR$ la somme des carrés des résidus du modèle initial, et $SCR_c$ celle d’un sous-modèle contenant $p'$ variables ($p' < p$). Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n-p}{p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$"
      ],
      "answer": 3,
      "explanation": "Pour comparer un modèle complet (p variables) et un sous-modèle restreint (p' variables), on utilise un test de Fisher. La statistique est $F = \\dfrac{(SCR_c - SCR)/(p - p')}{SCR/(n-p)}$. En simplifiant, le numérateur est proportionnel à $\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$, ce qui correspond à la réponse D.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "comparaison de modèles"
      ],
      "id_gen": [
        "qcm001_2025-10-28",
        "qcm003_2025-10-28"
      ]
    },
    {
      "id": "qcm3-q10",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Supposons qu’une constante est incluse dans le modèle. Quelle(s) propriété(s) vérifie le vecteur des résidus $\\hat{\\varepsilon}$ ?",
      "choices": [
        "Sa variance vaut $\\sigma^2 (X'X)^{-1}$",
        "Sa moyenne est nulle",
        "Sa variance vaut $\\mathrm{Var}(\\hat{\\varepsilon}) = \\sigma^2 P_{[X]^\\perp}$ ($= \\sigma^2 \\left(I_n - X (X'X)^{-1} X'\\right)$",
        "Son espérance est nulle",
        "Il est de taille $p$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        1,
        2,
        3
      ],
      "explanation": "Le vecteur des résidus est $\\hat{\\varepsilon} = M Y$ avec $M = I - P_X$, la matrice de projection sur $[X]^\\perp$. On a $\\mathbb{E}[\\hat{\\varepsilon}] = 0$, $\\mathrm{Var}(\\hat{\\varepsilon}) = \\sigma^2 M = \\sigma^2 P_{[X]^\\perp}$, et sa somme vaut 0 donc sa moyenne est nulle.",
      "tags": [
        "régression linéaire",
        "résidus",
        "espérance",
        "variance"
      ],
      "id_gen": [
        "qcm002_2025-10-28"
      ]
    },
    {
      "id": "qcm5-q01",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2 (X'X)^{-1}$",
        "$\\sigma^2 X \\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2 (XX')^{-1}$"
      ],
      "answer": [
        2
      ],
      "explanation": "Dans le modèle $Y = X\\beta + \\varepsilon$ avec $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, on a $\\mathrm{Var}(Y) = \\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$.",
      "tags": [
        "régression linéaire",
        "variance",
        "modèle linéaire"
      ],
      "id_gen": [
        "qcm004_2025-10-10"
      ]
    },
    {
      "id": "qcm5-q02",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Que vaut $\\hat{\\beta}$, l’estimateur par moindres carrés ordinaires de $\\beta$ ?",
      "choices": [
        "$\\mathbb{E}((X'X)^{-1} X'Y)$",
        "$(X'X)^{-1} X'Y$",
        "$\\mathbb{E}((XX')^{-1} XY)$",
        "$(XX')^{-1} XY$"
      ],
      "answer": [
        1
      ],
      "explanation": "Par définition de l’estimateur des moindres carrés ordinaires (MCO), on a $\\hat{\\beta} = (X'X)^{-1} X'Y$, obtenu en minimisant la somme des carrés des résidus $\\|Y - X\\beta\\|^2$.",
      "tags": [
        "régression linéaire",
        "estimateur",
        "MCO"
      ],
      "id_gen": [
        "qcm003_2025-10-10",
        "qcm005_2025-10-10",
        "qcm002_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q03",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat{\\varepsilon}$ ? (On note $P_{[X]} = X (X'X)^{-1} X'$ et $P_{[X]}^{\\perp} = I_n - P_{[X]}$.)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition, $\\hat{\\varepsilon} = Y - X\\hat{\\beta} = (I_n - P_{[X]})Y = P_{[X]}^{\\perp} Y$. En utilisant $Y = X\\beta + \\varepsilon$, on obtient aussi $\\hat{\\varepsilon} = P_{[X]}^{\\perp} \\varepsilon$. En revanche, $P_{[X]} Y$ donne les valeurs ajustées $\\hat{Y}$, et utiliser $Y - X\\beta$ correspond à l'erreur vraie, pas aux résidus.",
      "tags": [
        "régression linéaire",
        "résidus",
        "projection",
        "matrices de projection"
      ],
      "id_gen": [
        "qcm003_2025-10-10",
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q04",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle statistique constitue un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\dfrac{1}{n} \\sum_{i=1}^{n} \\varepsilon_i^{2}$",
        "$\\dfrac{1}{n} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$",
        "$\\dfrac{1}{n-p} \\sum_{i=1}^{n} \\varepsilon_i^{2}$",
        "$\\dfrac{1}{n-p} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$"
      ],
      "answer": [
        3
      ],
      "explanation": "Avec $M = I_n - P_{[X]}$ (rang $n-p$), la somme des carr\\u00e9s des r\\u00e9sidus est $\\text{RSS} = \\hat{\\varepsilon}'\\hat{\\varepsilon} = Y' M Y$. On a $\\mathbb{E}[\\text{RSS}] = (n-p)\\sigma^2$, d'o\\u00f9 l'estimateur sans biais $\\hat{\\sigma}^2 = \\dfrac{\\text{RSS}}{n-p} = \\dfrac{1}{n-p} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$.",
      "tags": [
        "régression linéaire",
        "variance",
        "r\\u00e9sidus",
        "degr\\u00e9s de libert\\u00e9"
      ],
      "id_gen": [
        "qcm003_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q05",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Parmi les quantités suivantes, lesquelles sont observables ou calculables à partir des observations ?",
      "choices": [
        "$\\hat{\\beta}$",
        "$\\hat{Y}$",
        "$Y$",
        "$\\hat{\\varepsilon}$",
        "$\\beta$",
        "$\\sigma^2$",
        "$\\varepsilon$"
      ],
      "answer": [
        0,
        1,
        2,
        3
      ],
      "explanation": "Les quantités observables ou calculables directement à partir des données $(X, Y)$ sont : $\\hat{\\beta}$ (estimateur MCO), $\\hat{Y} = X\\hat{\\beta}$ (valeurs ajustées), $Y$ (observations) et $\\hat{\\varepsilon} = Y - \\hat{Y}$ (résidus). En revanche, $\\beta$, $\\sigma^2$ et $\\varepsilon$ sont inobservables et ne peuvent être connus qu'au travers d'estimateurs.",
      "tags": [
        "régression linéaire",
        "observables",
        "paramètres",
        "résidus"
      ],
      "id_gen": [
        "qcm002_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q06",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelles sont les variables non corrélées ?",
      "choices": [
        "$\\hat{Y}_1$ et $\\hat{\\varepsilon}_1$",
        "$\\hat{\\beta}_1$ et $\\hat{\\beta}_2$",
        "$Y_1$ et $\\hat{Y}_1$",
        "$\\varepsilon_1$ et $\\hat{\\varepsilon}_1$",
        "$\\varepsilon_1$ et $\\varepsilon_2$",
        "$Y_1$ et $Y_2$",
        "$Y_1$ et $\\hat{\\varepsilon}_1$",
        "$\\hat{\\varepsilon}_1$ et $\\hat{\\varepsilon}_2$"
      ],
      "answer": [
        0,
        4,
        5
      ],
      "explanation": "Les valeurs ajustées et les résidus ($\\hat{Y}$ et $\\hat{\\varepsilon}$) sont orthogonaux, donc non corrélés. Les erreurs vraies $\\varepsilon_i$ sont indépendantes, donc non corrélées. Enfin, dans le cadre classique des MCO avec erreurs indépendantes, les $Y_i$ sont eux aussi non corrélés. En revanche, les autres couples sont en général corrélés.",
      "tags": [
        "régression linéaire",
        "corrélation",
        "résidus",
        "valeurs ajustées",
        "observations"
      ]
    },
    {
      "id": "qcm5-q07",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\mathrm{rg}(X)=p$ ?",
      "choices": [
        "$p \\le n$",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La corrélation entre les variables explicatives est nulle",
        "La matrice $X$ est inversible",
        "La matrice $X'X$ est inversible"
      ],
      "answer": [
        0,
        1,
        4
      ],
      "explanation": "Si $\\mathrm{rg}(X)=p$, les colonnes de $X$ sont linéairement indépendantes, donc nécessairement $p \\le n$. Par ailleurs, $X'X$ est alors définie positive et inversible. En revanche, $X$ n’est pas forcément carrée (donc pas forcément inversible) et le rang plein n’implique pas l’absence de corrélation entre variables explicatives.",
      "tags": [
        "régression linéaire",
        "rang",
        "matrices",
        "identifiabilité"
      ]
    },
    {
      "id": "qcm5-q08",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Qu’est-ce que la multicolinéarité dans un modèle de régression multiple ?",
      "choices": [
        "L’influence excessive d’une seule observation sur l’estimation du modèle",
        "L’interaction entre $Y$ et les variables explicatives",
        "L’erreur dans l’estimation des coefficients de régression",
        "La forte corrélation entre deux ou plusieurs variables explicatives"
      ],
      "answer": [
        3
      ],
      "explanation": "La multicolinéarité désigne une forte corrélation (souvent linéaire) entre deux ou plusieurs variables explicatives. Elle rend les colonnes de $X$ presque dépendantes, ce qui entraîne des problèmes numériques et une instabilité dans l’estimation de $\\hat{\\beta}$.",
      "tags": [
        "régression linéaire",
        "multicolinéarité",
        "variables explicatives"
      ],
      "id_gen": [
        "qcm001_2025-10-10",
        "qcm003_2025-10-10"
      ]
    },
    {
      "id": "qcm5-q09",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-p}{n-1} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{n-1}{n-p} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-p}{n-1} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-1}{n-p} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$"
      ],
      "answer": [
        3
      ],
      "explanation": "L’indice de détermination ajusté est défini par $R^2_a = 1 - \\dfrac{n-1}{n-p} \\cdot \\dfrac{\\text{SCR}}{\\text{SCT}}$, où $\\text{SCR} = \\sum (Y_i - \\hat{Y}_i)^2$ et $\\text{SCT} = \\sum (Y_i - \\bar{Y})^2$. Cette correction par $\\tfrac{n-1}{n-p}$ permet de compenser la tendance du $R^2$ à augmenter avec le nombre de variables explicatives.",
      "tags": [
        "régression linéaire",
        "R^2 ajusté",
        "variance expliquée",
        "qualité d’ajustement"
      ]
    },
    {
      "id": "qcm5-q10",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On note $R^2$ le $R^2$ obtenu après estimation du modèle initial, et $R^2_c$ celui d’un sous-modèle contenant $p'$ variables ($p' < p$) dont une constante. Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n - p}{p'} \\dfrac{R^2_c - R^2}{1 - R^2}$",
        "$\\dfrac{n - p}{p - p'} \\dfrac{R^2_c - R^2}{1 - R^2}$",
        "$\\dfrac{n - p}{p - p'} \\dfrac{R^2 - R^2_c}{1 - R^2}$",
        "$\\dfrac{n - p}{p'} \\dfrac{R^2 - R^2_c}{1 - R^2}$"
      ],
      "answer": [
        2
      ],
      "explanation": "La statistique de Fisher utilisée pour comparer un modèle complet à un sous-modèle imbriqué est $F = \\dfrac{(R^2 - R_c^2)/(p - p')}{(1 - R^2)/(n - p)}$, ce qui équivaut à $\\dfrac{n - p}{p - p'} \\cdot \\dfrac{R^2 - R_c^2}{1 - R^2}$. Cette statistique suit une loi de Fisher à $(p - p')$ et $(n - p)$ degrés de liberté sous l’hypothèse nulle.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "R^2",
        "comparaison de modèles"
      ],
      "id_gen": [
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q11",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle méthode permet de détecter une observation ayant une influence excessive sur un modèle de régression multiple ?",
      "choices": [
        "Le test de Durbin-Watson",
        "Le test de Breusch-Godfrey",
        "Le test de Shapiro-Wilk",
        "La distance de Cook"
      ],
      "answer": [
        3
      ],
      "explanation": "La distance de Cook mesure l’influence d’une observation sur l’estimation des coefficients de régression. Une valeur élevée indique qu’une seule observation exerce une influence excessive sur le modèle. Les autres tests cités concernent respectivement l’autocorrélation (Durbin-Watson, Breusch-Godfrey) et la normalité (Shapiro-Wilk).",
      "tags": [
        "régression linéaire",
        "influence",
        "diagnostic",
        "distance de Cook"
      ]
    },
    {
      "id": "qcm5-q12",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ?",
      "choices": [
        "Le $R^2$ augmente",
        "La SCR diminue",
        "Le $R^2_a$ diminue",
        "Le $R^2$ diminue",
        "La SCR augmente",
        "Le $R^2_a$ augmente"
      ],
      "answer": [
        0,
        1
      ],
      "explanation": "L’ajout d’une variable explicative au modèle entraîne toujours une diminution (ou au pire une constance) de la somme des carrés des résidus (SCR), ce qui implique une augmentation (ou constance) du $R^2$. En revanche, le $R^2_a$ peut augmenter ou diminuer selon la pertinence de la variable ajoutée.",
      "tags": [
        "régression linéaire",
        "R^2",
        "SCR",
        "ajout de variables"
      ]
    },
    {
      "id": "qcm5-q13",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On compare plusieurs modèles avec les critères AIC et BIC.",
      "choices": [
        "Le meilleur modèle au sens du BIC ne coïncide pas forcément avec le meilleur modèle au sens de l’AIC.",
        "Le meilleur modèle est automatiquement détecté : il s’agit de celui qui maximise l’AIC et qui maximise le BIC.",
        "Le BIC choisira un modèle de taille plus petite ou égale à celui choisi par l’AIC",
        "Il est possible que AIC et BIC soient tous les deux optimaux pour le plus gros modèle",
        "Si les modèles ont des tailles différentes, AIC choisira forcément le plus petit",
        "Si les modèles ont des tailles différentes, BIC choisira forcément le plus petit",
        "L’AIC choisira un modèle de taille plus petite ou égale à celui choisi par le BIC",
        "Le meilleur modèle est automatiquement détecté : il s’agit de celui qui minimise l’AIC et qui minimise le BIC."
      ],
      "answer": [
        0,
        2,
        3
      ],
      "explanation": "AIC et BIC sont deux critères d’information qui se minimisent. Le BIC pénalise davantage la complexité, ce qui conduit souvent à choisir un modèle plus petit ou égal à celui de l’AIC. Les deux critères peuvent mener à des choix différents, mais il arrive qu’ils coïncident. En revanche, aucun ne se maximise.",
      "tags": [
        "régression linéaire",
        "AIC",
        "BIC",
        "comparaison de modèles"
      ]
    },
    {
      "id": "qcm5-q14",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On a supposé $V(\\varepsilon) = \\sigma^2 I_n$. Si l’erreur de modélisation $\\varepsilon$ était hétéroscédastique, que deviendrait la matrice $V(\\varepsilon)$ ?",
      "choices": [
        "Les valeurs sur sa diagonale seraient différentes",
        "Elle ne serait pas inversible",
        "Elle ne serait plus diagonale",
        "Les valeurs sur la diagonale seraient nulles"
      ],
      "answer": [
        0
      ],
      "explanation": "En cas d’hétéroscédasticité, les erreurs ont des variances différentes mais restent supposées indépendantes. Ainsi $V(\\varepsilon) = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$ : la matrice reste diagonale mais ses valeurs sur la diagonale diffèrent. Elle demeure inversible tant qu’aucune variance n’est nulle.",
      "tags": [
        "régression linéaire",
        "hétéroscédasticité",
        "variance des erreurs",
        "matrice de variance"
      ]
    },
    {
      "id": "qcm4-q06",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Dans le contexte d’une régression multiple, qu’est-ce que l’hétéroscédasticité ?",
      "choices": [
        "Le fait que les εⁱ ne sont pas corrélés entre eux",
        "Le fait que les coefficients du modèle sont biaisés",
        "Le fait que la variance des εⁱ n’est pas constante",
        "Le fait que la relation entre Y et les variables explicatives n’est pas linéaire"
      ],
      "answer": [
        2
      ],
      "explanation": "L’hétéroscédasticité signifie que la variance des erreurs εⁱ varie selon les observations, c’est-à-dire qu’elle n’est pas constante. Cela viole l’hypothèse d’homoscedasticité du modèle linéaire classique.",
      "tags": [
        "hétéroscédasticité",
        "variance des erreurs",
        "régression multiple"
      ]
    },
    {
      "id": "qcm4-q07",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelles affirmations à propos des valeurs de levier $h^{ii}$ sont vraies ?",
      "choices": [
        "$\\sum_{i=1}^n h^{ii} = p$ (nombre de paramètres)",
        "Le levier dépend de la variable réponse $Y$",
        "$h^{ii} = (P^{[X]})^{ii}$",
        "Les points à fort effet levier ont toujours de grands résidus",
        "$h^{ii} > 2p/n$ indique un point à fort effet levier"
      ],
      "answer": [
        0,
        2,
        4
      ],
      "explanation": "Les valeurs de levier $h^{ii}$ proviennent de la matrice de projection $P^{[X]} = X(X^{\\top}X)^{-1}X^{\\top}$. Leur somme vaut le nombre de paramètres $p$. Elles ne dépendent que de $X$, pas de $Y$. Une valeur élevée (souvent $> 2p/n$) signale un point influent.",
      "tags": [
        "valeurs de levier",
        "influence",
        "projection",
        "régression linéaire multiple"
      ]
    },
    {
      "id": "qcm4-q08",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelle est la corrélation théorique entre les résidus $\\hat{\\varepsilon}$ et les valeurs ajustées $\\hat{Y}$ ?",
      "choices": [
        "1",
        "Elle dépend des données",
        "0",
        "$\\sigma^2$"
      ],
      "answer": [
        2
      ],
      "explanation": "Dans le modèle de régression linéaire, les résidus $\\hat{\\varepsilon}$ sont orthogonaux aux valeurs ajustées $\\hat{Y}$. Cela implique une corrélation théorique nulle entre les deux ($\\operatorname{Corr}(\\hat{\\varepsilon}, \\hat{Y}) = 0$).",
      "tags": [
        "corrélation",
        "résidus",
        "valeurs ajustées",
        "orthogonalité"
      ],
      "id_gen": [
        "qcm004_2025-10-28"
      ]
    },
    {
      "id": "qcm4-q09",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Vous souhaitez tester si les variables $X^3$, $X^4$, $X^5$ peuvent être supprimées simultanément d’un modèle avec 8 régresseurs (y compris l’intercept). Sous $H_0$, quelle est la loi de la statistique de test de Fisher ?",
      "choices": [
        "$F_{3,\\,n-5}$",
        "$F_{5,\\,n-8}$",
        "$F_{3,\\,n-8}$",
        "$F_{5,\\,n-5}$"
      ],
      "answer": [
        2
      ],
      "explanation": "Sous l’hypothèse nulle $H_0 : \\beta^3 = \\beta^4 = \\beta^5 = 0$, on compare un modèle restreint (5 régressseurs) à un modèle complet (8 régressseurs). La statistique de Fisher suit alors une loi $F_{3,\\,n-8}$, car on teste 3 contraintes sur un modèle avec 8 paramètres estimés.",
      "tags": [
        "test de Fisher",
        "modèles emboîtés",
        "régression multiple",
        "hypothèse nulle"
      ],
      "id_gen": [
        "qcm002_2025-10-24"
      ]
    },
    {
      "id": "qcm4-q10",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Après une recherche exhaustive basée sur le $R^2$ ajusté (c’est-à-dire le $R^2_a$), que peut-on affirmer sur le modèle retenu ?",
      "choices": [
        "Il ne présente aucun problème d’autocorrélation",
        "Il a le plus grand $R^2_a$",
        "Il ne présente aucun problème d’hétéroscédasticité",
        "Il a le plus faible $R^2_a$",
        "Il a le meilleur score BIC",
        "Il peut souffrir de multicolinéarité"
      ],
      "answer": [
        1,
        5
      ],
      "explanation": "La sélection fondée sur le $R^2_a$ vise à maximiser ce critère, donc le modèle retenu possède le plus grand $R^2_a$. Cependant, cela n’exclut pas la présence de multicolinéarité entre variables explicatives.",
      "tags": [
        "R² ajusté",
        "sélection de modèle",
        "multicolinéarité",
        "BIC"
      ],
      "id_gen": [
        "qcm004_2025-10-28"
      ]
    },
    {
      "id": "qcm4-q11",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelle forme du nuage de points des résidus indique un problème d’hétéroscédasticité ?",
      "choices": [
        "Dispersion constante mais trois valeurs aberrantes",
        "Forme d’entonnoir : la variance augmente avec les valeurs ajustées",
        "Forme parabolique",
        "Nuage aléatoire avec une dispersion constante"
      ],
      "answer": [
        1
      ],
      "explanation": "Une forme d’entonnoir dans le nuage des résidus indique que la variance des erreurs augmente avec les valeurs ajustées, ce qui caractérise un problème d’hétéroscédasticité.",
      "tags": [
        "hétéroscédasticité",
        "résidus",
        "diagnostic graphique",
        "régression linéaire"
      ]
    },
    {
      "id": "qcm4-q12",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "La distance de Cook permet de détecter :",
      "choices": [
        "Les points à fort levier",
        "Les problèmes de surapprentissage",
        "L’autocorrélation des résidus",
        "L’hétéroscédasticité",
        "Les valeurs aberrantes"
      ],
      "answer": [
        0,
        4
      ],
      "explanation": "La distance de Cook met en évidence les observations ayant une forte influence sur le modèle, c’est-à-dire celles combinant un fort levier et un grand résidu. Elle permet donc d’identifier les points à fort levier et les valeurs aberrantes.",
      "tags": [
        "distance de Cook",
        "influence",
        "valeurs aberrantes",
        "levier"
      ],
      "id_gen": [
        "qcm004_2025-10-28"
      ]
    },
    {
      "id": "qcm4-q13",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Breusch–Godfrey donne une p-valeur de 0,8. Que cela suggère-t-il ?",
      "choices": [
        "Aucune hétéroscédasticité dans les données",
        "Une autocorrélation positive des résidus",
        "Aucune autocorrélation des résidus",
        "La présence d’un problème d’hétéroscédasticité"
      ],
      "answer": [
        2
      ],
      "explanation": "Une p-valeur élevée (ici 0,8) conduit à ne pas rejeter l’hypothèse nulle du test de Breusch–Godfrey, selon laquelle il n’existe pas d’autocorrélation des résidus. On conclut donc à l’absence d’autocorrélation.",
      "tags": [
        "test de Breusch–Godfrey",
        "autocorrélation",
        "résidus",
        "régression linéaire"
      ],
      "id_gen": [
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "qcm4-q14",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Dans un test de modèles emboîtés comparant des modèles à 10 et 7 paramètres, on obtient $F = 2{,}6$ avec une p-valeur de $0{,}22$. Que recommandez-vous ?",
      "choices": [
        "Il faut effectuer des tests de Student pour plus de puissance statistique",
        "Le modèle le plus simple à 7 paramètres est adéquat",
        "Il faut plus de données pour conclure",
        "Le modèle complet à 10 paramètres est nécessaire"
      ],
      "answer": [
        1
      ],
      "explanation": "Une p-valeur élevée (0,22) indique qu’on ne rejette pas l’hypothèse nulle selon laquelle le modèle restreint à 7 paramètres est suffisant. On retient donc le modèle le plus simple, jugé adéquat.",
      "tags": [
        "test de Fisher",
        "modèles emboîtés",
        "sélection de modèle",
        "régression multiple"
      ],
      "id_gen": [
        "qcm003_2025-10-24",
        "qcm004_2025-10-28"
      ]
    },
    {
      "id": "qcm4-q15",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Après une recherche exhaustive basée sur le BIC, que peut-on affirmer sur le modèle retenu ?",
      "choices": [
        "Il a le meilleur score $C_p$ de Mallows",
        "Il ne présente aucun problème de multicolinéarité",
        "Il ne présente aucun problème d’hétéroscédasticité",
        "Il a le plus faible BIC",
        "Il a le plus grand BIC",
        "Il peut souffrir de problèmes d’autocorrélation"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Le critère BIC (Bayesian Information Criterion) est minimisé pour sélectionner le meilleur modèle. Ainsi, le modèle retenu est celui avec le plus faible BIC, mais cela ne garantit pas l’absence d’autocorrélation des résidus.",
      "tags": [
        "BIC",
        "sélection de modèle",
        "autocorrélation",
        "critères d’information"
      ]
    },
    {
      "id": "qcm6-q01",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Dans un modèle de régression linéaire simple, l’estimation de la pente vaut $\\tfrac{1}{4}$, d’écart-type estimé $0{,}1$. En vous aidant des quantiles donnés dans la table ci-dessous, à quel niveau $\\alpha$ pouvez-vous affirmer que cette pente est significativement différente de 0 ?",
      "image": "/Images/regression_lineaire/qcm6-2024-q1.png",
      "choices": [
        "$\\alpha = 0{,}1$",
        "$\\alpha = 0{,}05$",
        "$\\alpha = 0{,}001$",
        "$\\alpha = 0{,}01$",
        "$\\alpha = 0{,}15$"
      ],
      "answer": [
        0,
        1,
        4
      ],
      "explanation": "La statistique de test vaut $t = \\tfrac{0{,}25}{0{,}1} = 2{,}5$. En comparant aux quantiles de Student (≈2{,}07 pour $\\alpha=0{,}1$, ≈1{,}71 pour $\\alpha=0{,}15$, ≈2{,}49 pour $\\alpha=0{,}05$), on rejette $H_0$ pour $\\alpha = 0{,}15$, $0{,}1$ et légèrement pour $0{,}05$, mais pas pour des niveaux plus faibles.",
      "tags": [
        "test de Student",
        "pente de régression",
        "niveau de signification",
        "régression simple"
      ]
    },
    {
      "id": "qcm6-q02",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "L’ANOVA permet de tester des différences significatives :",
      "choices": [
        "Entre deux ou plusieurs groupes",
        "Entre deux groupes seulement",
        "Entre plus de deux groupes uniquement",
        "Entre des variables qualitatives"
      ],
      "answer": [
        0
      ],
      "explanation": "L’ANOVA (analyse de la variance) compare les moyennes de plusieurs groupes pour déterminer s’il existe au moins une différence significative. Elle s’applique dès deux groupes, mais est surtout utile pour plus de deux.",
      "tags": [
        "ANOVA",
        "analyse de la variance",
        "comparaison de moyennes",
        "tests statistiques"
      ]
    },
    {
      "id": "qcm6-q03",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur obtenue dans une ANOVA vaut 0.013, cela signifie que :",
      "choices": [
        "Il y a une différence significative de variance entre au moins deux groupes",
        "Les moyennes des groupes sont toutes significativement différentes",
        "Toutes les moyennes des groupes sont identiques",
        "Les variances des groupes sont toutes significativement différentes",
        "Il y a une différence significative de moyenne entre au moins deux groupes"
      ],
      "answer": [
        4
      ],
      "explanation": "Une p-valeur de 0.013 est inférieure au seuil usuel de 5 %, donc on rejette l’hypothèse nulle d’égalité des moyennes. L’ANOVA conclut qu’il existe une différence significative de moyenne entre au moins deux groupes.",
      "tags": [
        "ANOVA",
        "p-valeur",
        "test de Fisher",
        "comparaison de moyennes"
      ],
      "id_gen": [
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "qcm6-q04",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "On considère un modèle de régression contenant deux variables explicatives, qui sont deux facteurs à respectivement 3 et 4 modalités. Si on n’introduit aucune interaction, combien de coefficients seront estimés ?",
      "choices": [
        "5",
        "6",
        "9",
        "8",
        "12",
        "7",
        "10"
      ],
      "answer": [
        1
      ],
      "explanation": "Un facteur à 3 modalités nécessite 2 coefficients (car une modalité sert de référence) et un facteur à 4 modalités en nécessite 3. Avec l’intercept, on obtient donc $1 + 2 + 3 = 6$ coefficients estimés.",
      "tags": [
        "variables qualitatives",
        "facteurs",
        "paramétrisation",
        "modèle linéaire"
      ]
    },
    {
      "id": "qcm6-q05",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Le test $F$ utilisé dans l’ANOVA est défini comme :",
      "choices": [
        "La moyenne (normalisée) des différences de variances entre chaque groupe",
        "Le rapport entre la variance intergroupes et la variance totale",
        "Le rapport entre la variance intergroupes et la variance intragroupes",
        "La moyenne (normalisée) des différences de moyennes entre chaque groupe"
      ],
      "answer": [
        2
      ],
      "explanation": "Le test $F$ de l’ANOVA repose sur le rapport entre la variance intergroupes (due au modèle) et la variance intragroupes (due à l’erreur). Une valeur élevée de ce rapport indique une différence significative entre les groupes.",
      "tags": [
        "ANOVA",
        "test F",
        "variance intergroupes",
        "variance intragroupes"
      ]
    },
    {
      "id": "qcm6-q06",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Dans un modèle de régression linéaire simple, l’estimation de l’ordonnée à l’origine vaut $-2$, d’écart-type estimé $1$. En vous aidant des quantiles donnés dans la table ci-dessous, à quel niveau $\\alpha$ pouvez-vous affirmer que cette ordonnée à l’origine est significativement différente de 0 ?",
      "image": "/Images/regression_lineaire/qcm6-2024-q6.png",
      "choices": [
        "$\\alpha = 0{,}15$",
        "$\\alpha = 0{,}05$",
        "$\\alpha = 0{,}001$",
        "$\\alpha = 0{,}01$",
        "$\\alpha = 0{,}1$"
      ],
      "answer": [
        0,
        4
      ],
      "explanation": "La statistique de test vaut $t = \\tfrac{-2}{1} = -2$. En valeur absolue, $|t| = 2$, ce qui dépasse les seuils critiques pour $\\alpha = 0{,}15$ et $\\alpha = 0{,}1$, mais pas pour $\\alpha$ plus petits. On rejette donc $H_0$ à ces niveaux uniquement.",
      "tags": [
        "test de Student",
        "ordonnée à l’origine",
        "niveau de signification",
        "régression simple"
      ]
    },
    {
      "id": "qcm6-q07",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Quel est l’objectif principal de l’ANOVA ?",
      "choices": [
        "Comparer les variances d’une variable quantitative dans plusieurs groupes",
        "Tester la significativité de l’égalité de plusieurs variances",
        "Déterminer si la corrélation entre deux variables qualitatives est significative",
        "Comparer les moyennes d’une variable quantitative dans plusieurs groupes"
      ],
      "answer": [
        3
      ],
      "explanation": "L’ANOVA (analyse de la variance) vise à comparer les moyennes d’une variable quantitative entre plusieurs groupes afin de déterminer si elles diffèrent significativement.",
      "tags": [
        "ANOVA",
        "comparaison de moyennes",
        "analyse statistique",
        "variance intergroupes"
      ]
    },
    {
      "id": "qcm6-q08",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Parmi ces options, quelle est la principale condition à vérifier avant d’appliquer une ANOVA ?",
      "choices": [
        "Vérifier que chaque groupe a même espérance",
        "Vérifier la normalité des résidus",
        "Vérifier que chaque groupe a même variance",
        "Détecter la présence de valeurs aberrantes dans les données",
        "Vérifier que chaque groupe a même taille"
      ],
      "answer": [
        2
      ],
      "explanation": "L’ANOVA suppose l’homogénéité des variances (homoscédasticité) entre les groupes. Cette condition garantit la validité du test de Fisher utilisé pour comparer les moyennes.",
      "tags": [
        "ANOVA",
        "conditions d’application",
        "homoscédasticité",
        "variance"
      ]
    },
    {
      "id": "qcm6-q09",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "On considère un modèle de régression contenant deux variables explicatives, qui sont un facteur à trois modalités et une variable quantitative. Si on introduit une interaction entre les deux variables, combien de coefficients seront estimés ?",
      "choices": [
        "3",
        "7",
        "4",
        "6",
        "9",
        "8",
        "5"
      ],
      "answer": [
        3
      ],
      "explanation": "Un facteur à trois modalités nécessite 2 coefficients, une variable quantitative en ajoute 1, et l’interaction entre les deux introduit 2 coefficients supplémentaires (car elle dépend des modalités du facteur). Avec l’intercept, on obtient $1 + 2 + 1 + 2 = 6$ coefficients estimés.",
      "tags": [
        "interaction",
        "modèle linéaire",
        "facteurs",
        "variables explicatives"
      ]
    },
    {
      "id": "qcm6-q10",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Lorsqu’une ANOVA indique une différence significative, que devrait-on faire pour identifier précisément quels groupes diffèrent ?",
      "choices": [
        "Effectuer un test de Fisher",
        "Effectuer un test de Tukey",
        "Effectuer des tests de Student 2 à 2",
        "Effectuer un test de Durbin–Watson"
      ],
      "answer": [
        1
      ],
      "explanation": "Après une ANOVA significative, le test de Tukey permet de comparer toutes les paires de moyennes tout en contrôlant le risque global d’erreur de première espèce.",
      "tags": [
        "ANOVA",
        "test de Tukey",
        "comparaison multiple",
        "analyse post-hoc"
      ]
    },
    {
      "id": "qcmChat-q1",
      "qcm": "QCM-Chatgpt1",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Breusch–Godfrey donne une p-valeur très faible. Que cela suggère-t-il ?",
      "choices": [
        "Aucune hétéroscédasticité dans les données",
        "Une autocorrélation positive des résidus",
        "Aucune autocorrélation des résidus",
        "La présence d’un problème d’hétéroscédasticité"
      ],
      "answer": [
        1
      ],
      "explanation": "Une p-valeur très faible conduit à rejeter l’hypothèse nulle du test de Breusch–Godfrey, selon laquelle il n’existe pas d’autocorrélation des résidus. On conclut donc à la présence d’une autocorrélation positive des résidus.",
      "tags": [
        "test de Breusch–Godfrey",
        "autocorrélation",
        "résidus",
        "régression linéaire"
      ]
    },
    {
      "id": "qcmChat-q2",
      "qcm": "QCM-Chatgpt1",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Breusch–Pagan donne une p-valeur de 0,8. Que cela suggère-t-il ?",
      "choices": [
        "Aucune hétéroscédasticité dans les données",
        "Présence d’autocorrélation des résidus",
        "Présence d’hétéroscédasticité dans les données",
        "Les résidus suivent une loi normale"
      ],
      "answer": [
        0
      ],
      "explanation": "Une p-valeur élevée (ici 0,8) conduit à ne pas rejeter l’hypothèse nulle du test de Breusch–Pagan, selon laquelle les résidus sont homoscédastiques. On conclut donc à l’absence d’hétéroscédasticité dans les données.",
      "tags": [
        "test de Breusch–Pagan",
        "hétéroscédasticité",
        "homoscédasticité",
        "résidus",
        "régression linéaire"
      ]
    },
    {
      "id": "qcmChat-q3",
      "qcm": "QCM-Chatgpt1",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Breusch–Pagan donne une p-valeur très faible. Que cela suggère-t-il ?",
      "choices": [
        "Aucune hétéroscédasticité dans les données",
        "Présence d’autocorrélation des résidus",
        "Présence d’hétéroscédasticité dans les données",
        "Les résidus suivent une loi normale"
      ],
      "answer": [
        2
      ],
      "explanation": "Une p-valeur très faible conduit à rejeter l’hypothèse nulle du test de Breusch–Pagan, selon laquelle les résidus sont homoscédastiques. On conclut donc à la présence d’hétéroscédasticité dans les données.",
      "tags": [
        "test de Breusch–Pagan",
        "hétéroscédasticité",
        "homoscédasticité",
        "résidus",
        "régression linéaire"
      ]
    },
    {
      "id": "qcmChat-q6",
      "qcm": "QCM-Chatgpt1",
      "theme": "Régression linéaire",
      "question": "Le test de Durbin–Watson appliqué au modèle `reg2` donne DW = 1.98, p-value = 0.82. Que cela suggère-t-il ?",
      "choices": [
        "Présence d’une autocorrélation positive des résidus",
        "Présence d’une autocorrélation négative des résidus",
        "Aucune autocorrélation des résidus",
        "Présence d’hétéroscédasticité"
      ],
      "answer": [
        2
      ],
      "explanation": "La statistique DW est très proche de 2 et la p-valeur est élevée (0,82). Cela conduit à ne pas rejeter l’hypothèse nulle d’absence d’autocorrélation. On conclut donc à l’absence d’autocorrélation des résidus.",
      "tags": [
        "test de Durbin–Watson",
        "autocorrélation",
        "résidus",
        "régression linéaire"
      ]
    },
    {
      "id": "qcmChat-q7",
      "qcm": "QCM-Chatgpt1",
      "theme": "Régression linéaire",
      "question": "Le test de Durbin–Watson appliqué au modèle `reg2` donne DW = 1.54, p-value = 0.033. Que cela suggère-t-il ?",
      "choices": [
        "Présence d’une autocorrélation positive des résidus",
        "Présence d’une autocorrélation négative des résidus",
        "Aucune autocorrélation des résidus",
        "Présence d’hétéroscédasticité"
      ],
      "answer": [
        0
      ],
      "explanation": "La statistique DW est inférieure à 2 et la p-valeur est faible (0,033), ce qui conduit à rejeter l’hypothèse nulle d’absence d’autocorrélation. On conclut donc à la présence d’une autocorrélation positive des résidus.",
      "tags": [
        "test de Durbin–Watson",
        "autocorrélation",
        "résidus",
        "régression linéaire"
      ]
    },
    {
      "id": "qcmChat-q8",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Quelle est la valeur possible du coefficient de corrélation linéaire de Pearson \\(\\hat{\\rho}\\) ?",
      "choices": [
        "Elle peut dépasser 1 si les variables sont fortement liées",
        "Elle est toujours comprise entre -1 et 1",
        "Elle est toujours positive",
        "Elle dépend du nombre d’observations"
      ],
      "answer": 1,
      "explanation": "Le coefficient de corrélation de Pearson est borné entre -1 et 1, quels que soient les échantillons.",
      "tags": [
        "corrélation",
        "bornes",
        "statistiques descriptives"
      ]
    },
    {
      "id": "qcmChat-q9",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Si \\(\\hat{\\rho} = 0\\), on peut conclure que : (plusieurs réponses possibles)",
      "choices": [
        "Il n’y a pas de lien linéaire entre X et Y",
        "Il n’y a aucun lien entre X et Y",
        "Il peut exister un lien non linéaire",
        "Les variables sont indépendantes"
      ],
      "answer": [
        0,
        2
      ],
      "explanation": "Corrélation nulle ⇒ absence de lien linéaire. Un lien non linéaire (ou une dépendance non linéaire) peut subsister.",
      "tags": [
        "corrélation",
        "interprétation",
        "non-linéarité"
      ]
    },
    {
      "id": "qcmChat-q10",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Le test de corrélation sous hypothèse \\(H_0 : \\rho = 0\\) utilise :",
      "choices": [
        "Une loi normale",
        "Une loi du Khi-deux",
        "Une loi de Student à \\(n-2\\) degrés de liberté",
        "Une loi de Fisher"
      ],
      "answer": 2,
      "explanation": "Sous \\(H_0\\), la statistique basée sur \\(\\hat{\\rho}\\) suit une loi de Student à \\(n-2\\) ddl.",
      "tags": [
        "tests",
        "corrélation",
        "Student"
      ]
    },
    {
      "id": "qcmChat-q11",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Le but principal d’une ANOVA à un facteur est de :",
      "choices": [
        "Comparer des variances entre deux échantillons",
        "Tester l’égalité des moyennes entre plusieurs groupes",
        "Vérifier la normalité des résidus",
        "Maximiser la variance inter-classe"
      ],
      "answer": 1,
      "explanation": "ANOVA à un facteur : test d’égalité de moyennes entre plusieurs groupes via une décomposition de variance.",
      "tags": [
        "anova",
        "moyennes",
        "test F"
      ]
    },
    {
      "id": "qcmChat-q12",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Dans une ANOVA à un facteur équilibrée : (plusieurs réponses possibles)",
      "choices": [
        "La variance inter-groupes mesure la variabilité entre les groupes",
        "La variance intra-groupes mesure la variabilité à l’intérieur des groupes",
        "La statistique F suit une loi \\(F(I-1, n-I)\\) sous \\(H_0\\)",
        "On rejette \\(H_0\\) si \\(F < F_{critique}\\)"
      ],
      "answer": [
        0,
        1,
        2
      ],
      "explanation": "On rejette \\(H_0\\) si \\(F\\) est supérieur (pas inférieur) à la valeur critique.",
      "tags": [
        "anova",
        "variance",
        "loi de Fisher"
      ]
    },
    {
      "id": "qcmChat-q13",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "En plan équilibré à deux facteurs (A et B), le terme \\(S^2_{AB}\\) mesure :",
      "choices": [
        "L’effet global de A et B combinés (sans interaction)",
        "L’interaction entre A et B",
        "La somme des variances individuelles de A et B",
        "La variance intra-groupes"
      ],
      "answer": 1,
      "explanation": "Le terme d’interaction quantifie la modification de l’effet d’un facteur selon le niveau de l’autre.",
      "tags": [
        "anova à deux facteurs",
        "interaction"
      ]
    },
    {
      "id": "qcmChat-q14",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Le critère de Mallows \\(C_p\\) sert principalement à :",
      "choices": [
        "Maximiser la variance expliquée par le modèle",
        "Équilibrer biais et variance pour la prédiction",
        "Tester la normalité des résidus",
        "Estimer la corrélation entre variables explicatives"
      ],
      "answer": 1,
      "explanation": "Le \\(C_p\\) fournit un compromis biais/variance en vue de la performance prédictive.",
      "tags": [
        "sélection de modèles",
        "Cp",
        "prédiction"
      ]
    },
    {
      "id": "qcmChat-q15",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Le critère AIC est fondé sur :",
      "choices": [
        "La distance euclidienne",
        "La divergence de Kullback–Leibler",
        "La médiane absolue des résidus",
        "Le test du Khi-deux"
      ],
      "answer": 1,
      "explanation": "AIC ≈ estimation (pénalisée) de la divergence KL entre modèle et vérité ; on **minimise** l’AIC.",
      "tags": [
        "AIC",
        "information",
        "sélection de modèles"
      ]
    },
    {
      "id": "qcmChat-q16",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Le test de Durbin–Watson permet de détecter :",
      "choices": [
        "L’hétéroscédasticité",
        "Une autocorrélation d’ordre 1 des erreurs",
        "Une endogénéité des régresseurs",
        "Des valeurs aberrantes"
      ],
      "answer": 1,
      "explanation": "DW cible l’autocorrélation sérielle d’ordre 1 dans les résidus.",
      "tags": [
        "autocorrélation",
        "Durbin–Watson",
        "résidus"
      ]
    },
    {
      "id": "qcmChat-q17",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Le test de Breusch–Godfrey généralise Durbin–Watson car il :",
      "choices": [
        "Teste des autocorrélations d’ordre supérieur",
        "Ne requiert pas d’homoscédasticité",
        "Utilise une loi de Student",
        "Ne nécessite pas de régression auxiliaire"
      ],
      "answer": 0,
      "explanation": "BG permet de tester AR(p) pour p>1 (via une régression auxiliaire des résidus).",
      "tags": [
        "autocorrélation",
        "Breusch–Godfrey",
        "régressions auxiliaires"
      ]
    },
    {
      "id": "qcmChat-q18",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Dans le modèle linéaire classique, on suppose :",
      "choices": [
        "\\(Y = X\\beta + \\varepsilon\\) avec \\(\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\)",
        "Que \\(X\\) suit une loi uniforme",
        "Que \\(\\beta\\) est aléatoire",
        "Que \\(\\varepsilon\\) dépend de \\(X\\)"
      ],
      "answer": 0,
      "explanation": "Hypothèses Gaussiennes usuelles : erreurs centrées, homoscédastiques et indépendantes du plan d’expérience.",
      "tags": [
        "modèle linéaire",
        "hypothèses",
        "Gauss–Markov"
      ]
    },
    {
      "id": "qcmChat-q19",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Une p-valeur faible dans une ANOVA signifie que : (plusieurs réponses possibles)",
      "choices": [
        "Au moins deux moyennes de groupes diffèrent",
        "On rejette \\(H_0\\) d’égalité des moyennes",
        "Les variances de tous les groupes sont égales",
        "Le modèle est forcément mal spécifié"
      ],
      "answer": [
        0,
        1
      ],
      "explanation": "ANOVA rejette l’égalité des moyennes si la p-valeur est faible. Cela ne dit rien sur l’égalité des variances a posteriori.",
      "tags": [
        "anova",
        "p-valeur",
        "interprétation"
      ]
    },
    {
      "id": "qcmChat-q20",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Après une ANOVA significative, le test post-hoc de Tukey sert à :",
      "choices": [
        "Comparer les variances des groupes",
        "Identifier quelles paires de moyennes diffèrent",
        "Tester l’autocorrélation des résidus",
        "Mesurer la corrélation entre deux variables"
      ],
      "answer": 1,
      "explanation": "Tukey contrôle l’erreur de type I familiale pour des comparaisons multiples de moyennes 2-à-2.",
      "tags": [
        "anova",
        "post-hoc",
        "Tukey"
      ]
    },
    {
      "id": "qcmChat-q21",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Dans une régression multiple avec constante, la statistique de Fisher globale s’écrit en fonction de \\(R^2\\) comme :",
      "choices": [
        "\\(F = \\dfrac{(1-R^2)/p}{R^2/(n-p)}\\)",
        "\\(F = \\dfrac{R^2/p}{(1-R^2)/(n-p)}\\)",
        "\\(F = \\dfrac{R^2/(n-p)}{(1-R^2)/p}\\)",
        "\\(F = \\dfrac{(1-R^2)/(n-p)}{R^2/p}\\)"
      ],
      "answer": 1,
      "explanation": "Formule standard : \\(F = \\frac{(R^2/p)}{((1-R^2)/(n-p))}\\) pour tester la significativité globale (hors intercept).",
      "tags": [
        "Fisher",
        "R2",
        "significativité globale"
      ]
    },
    {
      "id": "qcmChat-q22",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Si les erreurs sont autocorrélées (mais centrées), alors : (plusieurs réponses possibles)",
      "choices": [
        "L’estimateur MCO reste sans biais",
        "Les écarts-types usuels des MCO deviennent incorrects",
        "Les tests de Student classiques peuvent être invalides",
        "L’ordonnée à l’origine est forcément biaisée"
      ],
      "answer": [
        0,
        1,
        2
      ],
      "explanation": "Autocorrélation : \\(\\hat\\beta\\) MCO reste sans biais mais n’est plus BLUE ; la variance est mal estimée ⇒ tests usuels invalides.",
      "tags": [
        "autocorrélation",
        "variance des MCO",
        "inférences"
      ]
    },
    {
      "id": "qcmChat-q23",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "En présence d’hétéroscédasticité : (plusieurs réponses possibles)",
      "choices": [
        "MCO reste sans biais",
        "MCO est inefficace (variance non minimale) parmi les linéaires sans biais",
        "On peut corriger via des écarts-types robustes (type White)",
        "Le \\(R^2\\) devient invalide"
      ],
      "answer": [
        0,
        1,
        2
      ],
      "explanation": "Hétéroscédasticité : MCO sans biais mais pas BLUE ; on corrige l’inférence avec des SE robustes ou par GLS/FGLS.",
      "tags": [
        "hétéroscédasticité",
        "robuste",
        "BLUE"
      ]
    },
    {
      "id": "qcmChat-q24",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Pourquoi utiliser un test de Tukey plutôt que des tests de Student 2-à-2 non ajustés après ANOVA ?",
      "choices": [
        "Pour maximiser \\(R^2\\)",
        "Pour contrôler l’erreur de type I familiale",
        "Pour éviter l’hétéroscédasticité",
        "Parce que Tukey est non paramétrique"
      ],
      "answer": 1,
      "explanation": "Les comparaisons multiples nécessitent un ajustement pour contrôler le risque global d’erreur de première espèce.",
      "tags": [
        "comparaisons multiples",
        "Tukey",
        "contrôle de l’erreur"
      ]
    },
    {
      "id": "qcmChat-q25",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Dans un test de modèles emboîtés (complet vs restreint), les degrés de liberté de la statistique F sont : (plusieurs réponses possibles)",
      "choices": [
        "Au numérateur : le nombre de contraintes testées",
        "Au dénominateur : les ddl résiduels du modèle complet",
        "Au numérateur : le nombre total d’observations",
        "Au dénominateur : \\(n-1\\)"
      ],
      "answer": [
        0,
        1
      ],
      "explanation": "ddl \\(= (p-p',\\; n-p)\\) lorsque le complet a \\(p\\) paramètres estimés et le restreint \\(p'\\).",
      "tags": [
        "modèles emboîtés",
        "ddl",
        "Fisher"
      ]
    },
    {
      "id": "qcmChat-q26",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "Un VIF (Variance Inflation Factor) élevé pour une variable explicative indique :",
      "choices": [
        "Une forte multicolinéarité",
        "Une hétéroscédasticité sévère",
        "Des résidus non normaux",
        "Un bon pouvoir prédictif"
      ],
      "answer": 0,
      "explanation": "VIF mesure l’inflation de la variance due à la colinéarité. Des VIF > 10 (règle pratique) suggèrent un problème sérieux.",
      "tags": [
        "multicolinéarité",
        "VIF",
        "stabilité des coefficients"
      ]
    },
    {
      "id": "qcmChat-q27",
      "qcm": "QCM-Chatgpt",
      "theme": "Régression linéaire",
      "question": "On modélise avec deux facteurs qualitatifs : A (3 modalités) et B (4 modalités), en codage par référence et en incluant leur interaction. Combien de coefficients sont estimés au total (constante incluse) ?",
      "choices": [
        "8",
        "9",
        "12",
        "11",
        "10"
      ],
      "answer": 2,
      "explanation": "Sans interactions : 1 (constante) + (3-1) + (4-1) = 1+2+3=6. Avec interaction : (3-1)(4-1)=6 de plus ⇒ total 12.",
      "tags": [
        "facteurs",
        "paramétrisation",
        "interactions"
      ]
    },
    {
      "id": "qcm5-q15",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Dans un modèle de régression linéaire standard, utilisant les notations du cours, quelles quantités sont observables ou calculables à partir des observations ?",
      "choices": [
        "$\\varepsilon$",
        "$\\widehat{Y}$",
        "$\\widehat{\\beta}$",
        "$Y$",
        "$\\beta$",
        "$\\widehat{\\varepsilon}$"
      ],
      "answer": [
        1,
        2,
        3,
        5
      ],
      "explanation": "Dans un modèle linéaire standard, seules les quantités dépendant directement des données sont observables ou calculables. $Y$ est observé, et les estimateurs $\\widehat{Y}$, $\\widehat{\\beta}$ et les résidus $\\widehat{\\varepsilon}$ sont calculables. En revanche, l'erreur vraie $\\varepsilon$ et les paramètres réels $\\beta$ ne sont pas observables.",
      "tags": [
        "régression linéaire",
        "quantités observables",
        "estimateurs",
        "résidus"
      ]
    },
    {
      "id": "qcm5-q16",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Avec les notations standards d’un modèle de régression linéaire, quelles égalités sont exactes ?",
      "choices": [
        "$\\widehat{Y} = P_{[X]}Y$",
        "$\\widehat{\\varepsilon} = Y - \\widehat{Y}$",
        "$\\widehat{\\varepsilon} = P_{[X]^\\perp}Y$",
        "$\\widehat{\\varepsilon} = P_{[X]^\\perp}\\varepsilon$",
        "$\\widehat{\\varepsilon} = P_{[X]}\\varepsilon$",
        "$\\widehat{Y} = P_{[X]^\\perp}Y$"
      ],
      "answer": [
        0,
        1,
        2,
        3
      ],
      "explanation": "Les égalités exactes dans le modèle sont : $\\widehat{Y} = P_{[X]}Y$, $\\widehat{\\varepsilon} = Y - \\widehat{Y}$, $\\widehat{\\varepsilon} = P_{[X]^\\perp}Y$ et, dans le modèle théorique, $\\widehat{\\varepsilon} = P_{[X]^\\perp}\\varepsilon$. Les égalités impliquant $P_{[X]}\\varepsilon$ ou $P_{[X]^\\perp}Y$ pour $\\widehat{Y}$ sont incorrectes.",
      "tags": [
        "régression linéaire",
        "projecteurs",
        "résidus",
        "modèle linéaire"
      ]
    },
    {
      "id": "qcm5-q17",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Sous les hypothèses et les notations standards d’un modèle de régression linéaire, quelles sont les variables non corrélées ?",
      "choices": [
        "$\\widehat{Y}^1 \\text{ et } \\widehat{\\varepsilon}^1$",
        "$Y^1 \\text{ et } Y^2$",
        "$\\widehat{\\varepsilon}^1 \\text{ et } \\widehat{\\varepsilon}^2$",
        "$\\varepsilon^1 \\text{ et } \\widehat{\\varepsilon}^1$",
        "$\\varepsilon^1 \\text{ et } \\varepsilon^2$",
        "$Y^1 \\text{ et } \\widehat{\\varepsilon}^1$",
        "$Y^1 \\text{ et } \\widehat{Y}^1$",
        "$\\widehat{\\beta}^1 \\text{ et } \\widehat{\\beta}^2$"
      ],
      "answer": [
        0,
        1,
        4
      ],
      "explanation": "Dans le modèle de régression linéaire standard avec erreurs non corrélées, on a : (i) $\\widehat{Y}$ et les résidus $\\widehat{\\varepsilon}$ sont non corrélés car ce sont des projections orthogonales de $Y$, donc en particulier $\\widehat{Y}^1$ et $\\widehat{\\varepsilon}^1$ ; (ii) les observations $Y^1$ et $Y^2$ sont non corrélées lorsque les erreurs le sont et que $X$ est non aléatoire ; (iii) les erreurs vraies $\\varepsilon^1$ et $\\varepsilon^2$ sont supposées non corrélées. Les autres couples impliquent des quantités qui, en général, sont corrélées.",
      "tags": [
        "régression linéaire",
        "corrélation",
        "erreurs",
        "résidus",
        "variables aléatoires"
      ]
    },
    {
      "id": "qcm5-q18",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Que se passe-t-il si l’hypothèse \\(\\operatorname{rg}(X) = p\\) n’est plus vérifiée ?",
      "choices": [
        "La matrice $X^T X$ devient inversible",
        "La matrice $X^T X$ n’est plus inversible",
        "La corrélation entre deux régresseurs vaut 1 ou $-1$",
        "L’estimateur $\\widehat{\\beta}$ n’est plus bien défini",
        "Les résidus $\\widehat{\\varepsilon}$ ne sont plus bien définis",
        "Les valeurs ajustées $\\widehat{Y}$ ne sont plus bien définies"
      ],
      "answer": [
        1,
        3
      ],
      "explanation": "Si l’hypothèse de rang complet $\\operatorname{rg}(X) = p$ n’est plus vérifiée, la matrice $X^T X$ n’est plus inversible et l’estimateur des moindres carrés $\\widehat{\\beta} = (X^T X)^{-1} X^T Y$ n’est plus bien défini. En revanche, les valeurs ajustées et les résidus peuvent encore être définis via la projection de $Y$ sur l’espace engendré par les colonnes de $X$.",
      "tags": [
        "régression linéaire",
        "rang de X",
        "multicolinéarité",
        "estimateur des moindres carrés"
      ]
    },
    {
      "id": "qcm5-q19",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On suppose que le bruit \\(\\varepsilon\\) est un vecteur Gaussien \\(\\mathcal{N}(0, \\sigma^{2} I_n)\\). Soit \\((\\widehat{\\beta}^{MLE}, \\widehat{\\sigma}^{2,MLE})\\) et \\((\\widehat{\\beta}^{OLS}, \\widehat{\\sigma}^{2,OLS})\\) les estimateurs de \\(\\beta\\) et \\(\\sigma^{2}\\) respectivement par maximum de vraisemblance et par moindres carrés ordinaires. Quelles affirmations sont exactes ?",
      "choices": [
        "$\\widehat{\\beta}^{MLE}$ est non biaisé",
        "$\\widehat{\\sigma}^{2,MLE}$ est non biaisé",
        "$\\widehat{\\beta}^{OLS}$ et $\\widehat{\\sigma}^{2,OLS}$ sont indépendants",
        "$\\widehat{\\beta}^{MLE} = \\widehat{\\beta}^{OLS}$",
        "$\\widehat{\\sigma}^{2,MLE} = \\widehat{\\sigma}^{2,OLS}$"
      ],
      "answer": [
        0,
        2,
        3
      ],
      "explanation": "Sous l’hypothèse de bruit gaussien, l’estimateur de \\(\\beta\\) par maximum de vraisemblance coïncide avec l’estimateur des moindres carrés ordinaires : on a donc $\\widehat{\\beta}^{MLE} = \\widehat{\\beta}^{OLS}$, et cet estimateur est non biaisé. Dans le modèle gaussien, $\\widehat{\\beta}^{OLS}$ et $\\widehat{\\sigma}^{2,OLS}$ sont indépendants. En revanche, $\\widehat{\\sigma}^{2,MLE}$ est un estimateur biaisé de \\(\\sigma^{2}\\) et il diffère de $\\widehat{\\sigma}^{2,OLS}$, qui est l’estimateur sans biais usuel.",
      "tags": [
        "régression linéaire",
        "maximum de vraisemblance",
        "moindres carrés ordinaires",
        "biais",
        "indépendance"
      ]
    },
    {
      "id": "qcm5-q20",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On estime $\\widehat{\\beta}$ sur $n$ individus, et on considère un nouvel individu $o$ pour lequel on ne connaît que les régresseurs $X^o$. Lorsque $n$ tend vers $+\\infty$, pour un taux de couverture de 95%,",
      "choices": [
        "la largeur de l’intervalle de confiance de $\\beta^1$ ne tend pas vers 0",
        "la largeur de l’intervalle de prédiction de $Y^o$ ne tend pas vers 0",
        "la largeur de l’intervalle de prédiction de $Y^o$ tend vers 0",
        "la largeur de l’intervalle de confiance de $\\beta^1$ tend vers 0"
      ],
      "answer": [
        1,
        3
      ],
      "explanation": "Quand $n \\to +\\infty$, la variance de l’estimateur de $\\beta^1$ décroît comme $1/n$, donc la largeur de l’intervalle de confiance pour $\\beta^1$ tend vers 0. En revanche, l’intervalle de prédiction pour $Y^o$ incorpore la variance irréductible du bruit $\\varepsilon$, qui ne diminue pas avec $n$ : sa largeur ne tend donc pas vers 0.",
      "tags": [
        "régression linéaire",
        "intervalle de confiance",
        "intervalle de prédiction",
        "asymptotique"
      ]
    },
    {
      "id": "qcm5-q21",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Dans un modèle de régression linéaire simple, l’estimation de la pente auprès de 25 individus vaut 1/4, d’écart-type estimé 0.1. En vous aidant des quantiles donnés dans la table, à quel niveau α pouvez-vous conclure que cette pente est significativement différente de 0 ?",
      "image": "/Images/regression_lineaire/qcm5-2025-q21.png",
      "choices": [
        "$\\alpha = 0.15$",
        "$\\alpha = 0.05$",
        "$\\alpha = 0.001$",
        "$\\alpha = 0.01$",
        "$\\alpha = 0.1$"
      ],
      "answer": [
        0,
        1,
        4
      ],
      "explanation": "Le statistique de test vaut $t = \\frac{0.25}{0.1} = 2.5$. Pour $n = 25$ individus, il y a $24$ degrés de liberté. D’après la table, le quantile pour $p = 0.99$ (donc test bilatéral au niveau $\\alpha = 0.02$) vaut $2.492$. Comme $2.5 > 2.492$, on rejette l’hypothèse nulle pour tous les niveaux d’α supérieurs ou égaux à 0.02. Les niveaux proposés qui vérifient cela sont $\\alpha = 0.15$, $\\alpha = 0.10$ et $\\alpha = 0.05$.",
      "tags": [
        "régression linéaire",
        "test de Student",
        "pente",
        "significativité"
      ]
    },
    {
      "id": "qcm5-q22",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Dans un test de modèles emboîtés comparant des modèles à 10 et 7 paramètres, on obtient $F = 2.6$ avec une p-valeur de 0.22. Que recommandez-vous ?",
      "choices": [
        "Il faut plus de données pour conclure",
        "Le modèle le plus simple à 7 paramètres est adéquat",
        "Il faut effectuer des tests de Student pour plus de puissance statistique",
        "Le modèle complet à 10 paramètres est nécessaire"
      ],
      "answer": [
        1
      ],
      "explanation": "Avec une p-valeur de 0.22, on ne rejette pas l’hypothèse nulle selon laquelle les paramètres supplémentaires du modèle à 10 paramètres n’améliorent pas significativement l’ajustement. Le modèle à 7 paramètres est donc jugé adéquat.",
      "tags": [
        "régression linéaire",
        "test F",
        "modèles emboîtés",
        "sélection de modèle"
      ]
    },
    {
      "id": "qcm5-q23",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "L’hypothèse usuelle sur l’erreur de modélisation est $\\mathrm{V}(\\varepsilon) = \\sigma^{2} I_n$. En cas d’hétéroscédasticité, que deviendrait la matrice $\\mathrm{V}(\\varepsilon)$ ?",
      "choices": [
        "Les valeurs sur sa diagonale seraient différentes",
        "Les valeurs sur la diagonale seraient nulles",
        "Elle ne serait pas inversible",
        "Elle ne serait plus diagonale"
      ],
      "answer": [
        0
      ],
      "explanation": "En présence d’hétéroscédasticité, la variance de l’erreur n’est plus constante d’un individu à l’autre : la matrice $\\mathrm{V}(\\varepsilon)$ reste diagonale mais ses éléments diagonaux deviennent différents.",
      "tags": [
        "régression linéaire",
        "hétéroscédasticité",
        "variance de l’erreur",
        "matrice de variance"
      ]
    },
    {
      "id": "qcm5-q24",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On compare plusieurs modèles avec les critères AIC et BIC.",
      "choices": [
        "Si les modèles ont des tailles différentes, l’AIC choisira forcément le plus petit",
        "Le meilleur modèle sera détecté : il s’agit de celui qui minimise à la fois l’AIC et le BIC",
        "L’AIC choisira un modèle de taille plus petite ou égale à celui choisi par le BIC",
        "Si les modèles ont des tailles différentes, le BIC choisira forcément le plus petit",
        "Il est possible que l’AIC et le BIC soient tous les deux optimaux pour le plus gros modèle",
        "Le meilleur modèle sera détecté : il s’agit de celui qui maximise à la fois l’AIC et le BIC",
        "Le meilleur modèle au sens du BIC ne coïncide pas forcément avec celui au sens de l’AIC",
        "Le BIC choisira un modèle de taille plus petite ou égale à celui choisi par l’AIC"
      ],
      "answer": [
        4,
        6,
        7
      ],
      "explanation": "L’AIC et le BIC peuvent tous deux sélectionner le modèle le plus complexe si la pénalisation n’est pas suffisante pour compenser le gain en vraisemblance. Le meilleur modèle selon le BIC ne coïncide pas forcément avec celui selon l’AIC, car la pénalisation de la complexité diffère. En général, le BIC pénalise davantage les modèles complexes et choisira un modèle de taille plus petite ou égale à celui choisi par l’AIC.",
      "tags": [
        "régression linéaire",
        "sélection de modèle",
        "AIC",
        "BIC"
      ]
    },
    {
      "id": "qcm5-q25",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quel est l’objectif principal d’une ANOVA à un facteur ?",
      "choices": [
        "Comparer les variances d’une variable quantitative dans plusieurs groupes",
        "Tester la significativité de l’égalité de plusieurs variances",
        "Comparer les moyennes d’une variable quantitative dans plusieurs groupes",
        "Déterminer si la corrélation entre deux variables qualitatives est significative"
      ],
      "answer": [
        2
      ],
      "explanation": "L’objectif principal d’une ANOVA à un facteur est de comparer les moyennes d’une variable quantitative entre plusieurs groupes, afin de déterminer si elles diffèrent significativement.",
      "tags": [
        "ANOVA",
        "régression linéaire",
        "comparaison de moyennes",
        "statistiques inférentielles"
      ]
    },
    {
      "id": "qcm5-q26",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Parmi ces options, quelle est la principale condition à vérifier avant d’appliquer une ANOVA ?",
      "choices": [
        "Détecter la présence de valeurs aberrantes dans les données",
        "Vérifier la normalité des résidus",
        "Vérifier que chaque groupe a même espérance",
        "Vérifier que chaque groupe a même variance",
        "Vérifier que chaque groupe a même taille"
      ],
      "answer": [
        3
      ],
      "explanation": "La principale hypothèse de l’ANOVA à vérifier avant son application est l’homogénéité des variances entre groupes (homoscédasticité).",
      "tags": [
        "ANOVA",
        "homoscédasticité",
        "conditions d’application",
        "statistiques"
      ]
    },
    {
      "id": "qcm5-q27",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur d’un test ANOVA vaut 0.013, cela signifie que :",
      "choices": [
        "Les moyennes des groupes sont toutes significativement différentes",
        "Les variances des groupes sont toutes significativement différentes",
        "Toutes les moyennes des groupes sont identiques",
        "Il y a une différence significative de moyenne entre au moins 2 groupes",
        "Il y a une différence significative de variance entre au moins 2 groupes"
      ],
      "answer": [
        3
      ],
      "explanation": "Une p-valeur de 0.013 dans un test ANOVA signifie que l’hypothèse nulle d’égalité des moyennes est rejetée au seuil usuel de 5%. On conclut donc qu’au moins deux groupes ont des moyennes significativement différentes.",
      "tags": [
        "ANOVA",
        "test F",
        "comparaison de moyennes",
        "statistiques inférentielles"
      ]
    },
    {
      "id": "qcm5-q28",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Lorsqu’une ANOVA indique une différence significative, que devrait-on faire pour identifier précisément les groupes qui diffèrent ?",
      "choices": [
        "Effectuer des tests de Student 2 à 2",
        "Effectuer un test de Fisher",
        "Effectuer un test de Tukey",
        "Effectuer un test de Durbin Watson"
      ],
      "answer": [
        2
      ],
      "explanation": "Après un résultat significatif d’ANOVA, le test post-hoc le plus couramment utilisé pour identifier précisément quels groupes diffèrent est le test de Tukey (Tukey HSD).",
      "tags": [
        "ANOVA",
        "test post-hoc",
        "Tukey",
        "comparaisons multiples"
      ]
    },
    {
      "id": "qcm5-q29",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On considère un modèle de régression contenant 2 variables explicatives, qui sont deux facteurs à respectivement 3 et 4 modalités. Si on n’introduit aucune interaction, combien de coefficients seront estimés ?",
      "choices": [
        "10",
        "8",
        "12",
        "5",
        "7",
        "9",
        "6"
      ],
      "answer": [
        6
      ],
      "explanation": "Un facteur à 3 modalités nécessite 2 indicatrices, un facteur à 4 modalités en nécessite 3. Avec l’ordonnée à l’origine, cela donne $1 + 2 + 3 = 6$ coefficients estimés au total.",
      "tags": [
        "régression linéaire",
        "variables catégorielles",
        "facteurs",
        "paramétrisation"
      ]
    },
    {
      "id": "qcm5-q30",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On considère un modèle de régression contenant 2 variables explicatives, qui sont 1 facteur à 3 modalités et une variable quantitative. Si on introduit une interaction entre les 2 variables, combien de coefficients seront estimés ?",
      "choices": [
        "3",
        "8",
        "7",
        "4",
        "6",
        "9",
        "5"
      ],
      "answer": [
        4
      ],
      "explanation": "Un facteur à 3 modalités nécessite 2 indicatrices. La variable quantitative ajoute 1 coefficient supplémentaire. L’interaction entre les deux ajoute 2 coefficients additionnels (une interaction par indicatrice). Avec l’intercept, cela donne un total de $1 + 2 + 1 + 2 = 6$ coefficients.",
      "tags": [
        "régression linéaire",
        "interaction",
        "variables catégorielles",
        "paramétrisation"
      ]
    }
  ]
}
