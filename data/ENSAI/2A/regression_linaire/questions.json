{
  "questions": [
    {
      "id": "ols-q001",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "On se place dans le modèle $Y=X\\beta+\\varepsilon$ (\\operatorname{rg}(X)=p)). Quelles hypothèses fait-on sur $\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2(XX')^{-1}$",
        "Sa moyenne empirique est nulle",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Son espérance est nulle",
        "Il est non-corrélé à $Y$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Hypothèses usuelles : $\\mathbb E[\\varepsilon]=0$ et $\\operatorname{Var}(\\varepsilon)=\\sigma^2 I_n$. En revanche $\\varepsilon$ n’est pas non-corrélé à $Y$ puisque $Y=X\\beta+\\varepsilon$.",
      "tags": [
        "OLS",
        "hypothèses",
        "Gauss–Markov"
      ]
    },
    {
      "id": "ols-q002",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Que vaut l’estimateur MCO (OLS) de $\\beta$ ?",
      "choices": [
        "$\\mathbb E\\big((XX')^{-1}XY\\big)$",
        "$(X'X)^{-1}X'Y$",
        "$(XX')^{-1}XY$",
        "$\\mathbb E\\big((X'X)^{-1}XY\\big)$"
      ],
      "answer": 1,
      "explanation": "Formule classique des MCO : $\\hat\\beta=(X'X)^{-1}X'Y$.",
      "tags": [
        "OLS",
        "estimateur",
        "formule"
      ]
    },
    {
      "id": "linalg-q003",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Que signifie $\\operatorname{rg}(X)=p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p\\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : colonnes linéairement indépendantes, donc $X'X$ inversible et nécessairement $p\\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ]
    },
    {
      "id": "test-q004",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur du test de Student pour le coefficient de $X^{(j)}$ vaut 0,023, que concluez-vous ?",
      "choices": [
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul.",
        "Le modèle est surajusté.",
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$."
      ],
      "answer": 3,
      "explanation": "Avec un seuil usuel $\\alpha=5\\%$, $p=0,023 < 0,05$ : on rejette $H_0$ (« coefficient nul »). Donc le coefficient est significatif.",
      "tags": [
        "test de Student",
        "p-value",
        "significativité"
      ],
      "id_gen": [
        "qcm005_2025-10-10"
      ]
    },
    {
      "id": "res-q005",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "test-q006",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Supposons que pour un test statistique donné, la région critique associée au risque de première espèce $\\alpha = 5\\%$ est $\\{|T| > 3.1\\}$, où $T$ désigne la statistique de test. On observe $T = -4$. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test."
      ],
      "answer": [
        4,
        3,
        5
      ],
      "explanation": "Comme $|T|=4 > 3.1$, on est dans la région critique pour $\\alpha=5\\%$ et $\\alpha=10\\%$ donc on rejette $H_0$. Mais $|T|$ n’est pas assez extrême pour rejeter à $\\alpha=1\\%$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "p-value"
      ],
      "id_gen": [
        "qcm004_2025-10-10"
      ]
    },
    {
      "id": "test-q007",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Pour le même test qu’à la question précédente, on note $F$ la fonction de répartition de $T$ sous $H_0$. On observe toujours $T=-4$. Que vaut alors la p-value ?",
      "choices": [
        "$1 - F(-4)$",
        "$F(-4)$",
        "$2F(-4)$",
        "$2(1 - F(-4))$"
      ],
      "answer": 2,
      "explanation": "La p-value bilatérale est $2\\times F(-4)$.",
      "tags": [
        "tests statistiques",
        "p-value",
        "loi de test"
      ]
    },
    {
      "id": "var-q008",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique effectuée auprès d’étudiants, on relève la mention obtenue au BAC. Cette variable peut être considérée comme :",
      "choices": [
        "qualitative ordinale",
        "quantitative continue",
        "qualitative nominale",
        "quantitative discrète"
      ],
      "answer": 0,
      "explanation": "Les mentions (« passable », « bien », « très bien ») ont un ordre naturel → qualitative ordinale.",
      "tags": [
        "nature des variables",
        "qualitative",
        "ordinale"
      ],
      "id_gen": [
        "qcm005_2025-10-10"
      ]
    },
    {
      "id": "test-q009",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique, on souhaite étudier le lien entre le pays d’origine des individus observés et leur souscription (ou non) à une assurance vie. Quel outil vous semble adapté ?",
      "choices": [
        "La mise en place d’un test du chi-deux",
        "La représentation d’un nuage de points",
        "Une représentation graphique à l’aide de boxplots",
        "Le calcul de la corrélation de Pearson"
      ],
      "answer": 0,
      "explanation": "Deux variables qualitatives → test du chi² d’indépendance.",
      "tags": [
        "test du chi²",
        "indépendance",
        "qualitative"
      ]
    },
    {
      "id": "ols-q010",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Dans l’écriture classique d’un modèle de régression linéaire concernant $n$ individus, $Y = X\\beta + \\varepsilon$, (plusieurs réponses possibles)",
      "choices": [
        "$Y$ et $X$ sont des vecteurs de taille $n$",
        "$Y$ est toujours considéré comme aléatoire",
        "$\\varepsilon$ est toujours considéré comme aléatoire",
        "$Y$ et $\\beta$ sont des vecteurs de taille $n$",
        "$X$ est une matrice"
      ],
      "answer": [
        1,
        2,
        4
      ],
      "explanation": "$Y$ est un vecteur aléatoire ($n\\times 1$), $\\varepsilon$ aussi. $X$ est une matrice ($n\\times p$). $\\beta$ est un vecteur $p\\times 1$, donc pas de taille $n$.",
      "tags": [
        "OLS",
        "modèle de régression",
        "dimensions"
      ]
    },
    {
      "id": "test-q011",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Supposons que lors de l’application d’un test statistique, on observe une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test."
      ],
      "answer": [
        2,
        4
      ],
      "explanation": "Comme $p = 0,037 < 0,05$, on rejette l’hypothèse nulle au seuil 5% et 10%. En revanche, $p > 0,01$ donc pas de rejet au seuil 1%.",
      "tags": [
        "tests statistiques",
        "p-value",
        "seuil de risque"
      ],
      "id_gen": [
        "qcm004_2025-10-10",
        "qcm005_2025-10-10"
      ]
    },
    {
      "id": "test-q012",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "Pour un test donné, la région critique associée à un risque de première espèce $\\alpha$ est $\\{|T| > a\\}$, où $T$ suit une loi continue. On a observé $T=-2$ et une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour $\\alpha=5\\%$, $a < 2$",
        "Pour $\\alpha=3,7\\%$, $a > 2$",
        "Pour $\\alpha=3,7\\%$, $a < 2$",
        "Pour $\\alpha=3,7\\%$, $a = 2$",
        "Pour $\\alpha=5\\%$, $a > 2$",
        "Pour $\\alpha=5\\%$, $a = 2$"
      ],
      "answer": [
        0,
        3
      ],
      "explanation": "La p-value est $0,037$, ce qui correspond à un seuil critique $\\alpha \\approx 3,7\\%$. Donc pour $\\alpha=5\\%$, on rejette l’hypothèse nulle et $a < 2$. Pour $\\alpha=3,7\\%$, $T$ est exactement sur la frontière, donc on a $a = 2$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "p-value"
      ]
    },
    {
      "id": "var-q013",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique effectuée auprès d’étudiants, on relève le code postal du lycée dont ils proviennent. Cette variable peut être considérée comme :",
      "choices": [
        "qualitative nominale",
        "quantitative continue",
        "qualitative ordinale",
        "quantitative discrète"
      ],
      "answer": 0,
      "explanation": "Le code postal est une simple étiquette servant à identifier une zone géographique, sans ordre naturel ni signification métrique → variable qualitative nominale.",
      "tags": [
        "nature des variables",
        "qualitative",
        "nominale"
      ]
    },
    {
      "id": "visu-q014",
      "qcm": "QCM1",
      "theme": "Statistiques descriptives",
      "question": "Lors d’une étude statistique, on souhaite étudier le lien entre le pays d’origine des individus observés et leur revenu annuel. Quel outil vous semble adapté ?",
      "choices": [
        "La mise en place d’un test du chi-deux",
        "Le calcul de la corrélation de Pearson",
        "La représentation d’un nuage de points",
        "Une représentation graphique à l’aide de boxplots"
      ],
      "answer": 3,
      "explanation": "Le pays d’origine est une variable qualitative et le revenu une variable quantitative → on compare les distributions avec des boxplots.",
      "tags": [
        "visualisation",
        "qualitative vs quantitative",
        "boxplots"
      ],
      "id_gen": [
        "qcm003_2025-10-10"
      ]
    },
    {
      "id": "ols-q015",
      "qcm": "QCM1",
      "theme": "Régression linéaire",
      "question": "En pratique, lorsqu’on met en place un modèle de régression linéaire $Y = X\\beta + \\varepsilon$, quelles quantités sont connues (observées) ?",
      "choices": [
        "$Y$ et $X\\beta$",
        "$X\\beta$",
        "$X, Y$ et $\\varepsilon$",
        "$X$ et $Y$"
      ],
      "answer": 3,
      "explanation": "En pratique, on observe $X$ (variables explicatives) et $Y$ (variable dépendante). Les paramètres $\\beta$ et les erreurs $\\varepsilon$ sont inconnus.",
      "tags": [
        "OLS",
        "régression linéaire",
        "observables"
      ],
      "id_gen": [
        "qcm004_2025-10-10"
      ]
    },
    {
      "id": "var-q016",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Si $\\mathbb E(\\varepsilon)=0$ et $\\operatorname{Var}(\\varepsilon)=\\sigma^2 I_n$, que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2(X'X)^{-1}$",
        "$\\sigma^2X\\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2(XX')^{-1}$"
      ],
      "answer": 2,
      "explanation": "En effet, $\\operatorname{Var}(Y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\sigma^2 I_n$.",
      "tags": [
        "variance",
        "régression",
        "OLS"
      ],
      "id_gen": [
        "qcm001_2025-10-10",
        "qcm006_2025-10-10"
      ]
    },
    {
      "id": "res-q017",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "var-q018",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Quelle statistique constitue un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\tfrac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum_{i=1}^n \\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum_{i=1}^n \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n-p}\\sum_{i=1}^n \\varepsilon_i^2$"
      ],
      "answer": 0,
      "explanation": "L’estimateur sans biais de la variance est $\\hat\\sigma^2 = \\tfrac{1}{n-p}\\sum_{i=1}^n \\hat\\varepsilon_i^2$.",
      "tags": [
        "variance",
        "estimateur sans biais",
        "OLS"
      ],
      "id_gen": [
        "qcm004_2025-10-10",
        "qcm002_2025-10-24",
        "qcm003_2025-10-24"
      ]
    },
    {
      "id": "test-q019",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Student associée au coefficient de régression de la variable $X^{(j)}$ vaut $-23$, que concluez-vous ?",
      "choices": [
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul.",
        "Le modèle est surajusté.",
        "Relation statistiquement significative entre $X^{(j)}$ et $Y$."
      ],
      "answer": 3,
      "explanation": "Avec une statistique $t = -23$ (valeur absolue extrêmement élevée), on rejette l’hypothèse nulle $H_0$. Il existe donc une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "test de Student",
        "significativité",
        "régression"
      ]
    },
    {
      "id": "ols-q020",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que peut-on dire de l’estimateur MCO de $\\beta$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2$",
        "C’est le meilleur estimateur parmi tous les estimateurs sans biais",
        "C’est un estimateur sans biais",
        "Son espérance est nulle",
        "Sa variance vaut $\\hat\\sigma^2/(n-p)$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "C’est le meilleur estimateur parmi tous les estimateurs linéaires et sans biais",
        "C’est le meilleur estimateur parmi tous les estimateurs",
        "C’est le meilleur estimateur parmi tous les estimateurs consistants"
      ],
      "answer": [
        2,
        5,
        6
      ],
      "explanation": "Théorème de Gauss–Markov : l’estimateur MCO est sans biais, sa variance est $\\sigma^2(X'X)^{-1}$ et il est le meilleur estimateur linéaire sans biais (BLUE).",
      "tags": [
        "OLS",
        "Gauss–Markov",
        "BLUE",
        "estimateur"
      ],
      "id_gen": [
        "qcm002_2025-10-24"
      ]
    },
    {
      "id": "test-q021",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Supposons que lors de l’application d’un test statistique, on observe une p-value de 0,037. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test."
      ],
      "answer": [
        2,
        4
      ],
      "explanation": "Comme $p = 0,037 < 0,05$, on rejette $H_0$ au seuil de 5% et de 10%. En revanche, $p > 0,01$ donc pas de rejet au seuil de 1%.",
      "tags": [
        "tests statistiques",
        "p-value",
        "seuil de risque"
      ],
      "id_gen": [
        "qcm002_2025-10-10"
      ]
    },
    {
      "id": "ols-q022",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On se place dans le modèle $Y = X\\beta + \\varepsilon$ (rang($X$) = $p$). Quelles hypothèses fait-on sur $\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2(XX')^{-1}$",
        "Sa moyenne empirique est nulle",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Son espérance est nulle",
        "Il est non-corrélé à $Y$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Hypothèses usuelles : $\\mathbb E[\\varepsilon] = 0$ et $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I_n$. En revanche, $\\varepsilon$ n’est pas non-corrélé à $Y$ puisque $Y = X\\beta + \\varepsilon$.",
      "tags": [
        "OLS",
        "hypothèses",
        "Gauss–Markov"
      ],
      "id_gen": [
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "linalg-q023",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\operatorname{rg}(X) = p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p \\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : les colonnes sont linéairement indépendantes, donc $X'X$ est inversible et nécessairement $p \\le n$. La matrice $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ]
    },
    {
      "id": "ols-q024",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que vaut l’estimateur MCO de $\\beta$ ?",
      "choices": [
        "$\\mathbb E\\big((XX')^{-1}XY\\big)$",
        "$(X'X)^{-1}X'Y$",
        "$(XX')^{-1}XY$",
        "$\\mathbb E\\big((X'X)^{-1}XY\\big)$"
      ],
      "answer": 1,
      "explanation": "Formule classique des MCO : $\\hat\\beta = (X'X)^{-1}X'Y$.",
      "tags": [
        "OLS",
        "estimateur",
        "formule"
      ],
      "id_gen": [
        "qcm001_2025-10-10"
      ]
    },
    {
      "id": "res-q025",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelles relations sont vraies ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I-P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "test-q026",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Supposons que pour un test statistique donné, la région critique associée au risque de première espèce $\\alpha = 5\\%$ est $\\{|T| > 3.1\\}$, où $T$ désigne la statistique de test. On observe $T = -4$. Que peut-on conclure ? (plusieurs réponses possibles)",
      "choices": [
        "Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 5%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.",
        "Pour un risque de première espèce de 1%, on ne peut pas conclure au test.",
        "Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test."
      ],
      "answer": [
        4,
        3,
        5
      ],
      "explanation": "Comme $|T| = 4 > 3.1$, on est dans la région critique pour $\\alpha = 5\\%$ et $\\alpha = 10\\%$ donc on rejette $H_0$. Mais la valeur n’est pas assez extrême pour conclure au seuil $\\alpha = 1\\%$.",
      "tags": [
        "tests statistiques",
        "valeur critique",
        "régression"
      ],
      "id_gen": [
        "qcm002_2025-10-10",
        "qcm006_2025-10-10"
      ]
    },
    {
      "id": "ols-q027",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On se place dans un modèle de régression linéaire standard $Y = X\\beta + \\varepsilon$, où $\\beta \\in \\mathbb{R}^p$, $X$ est une matrice déterministe de taille $n \\times p$ de rang $p$, et $\\varepsilon$ est un vecteur aléatoire de taille $n$. Quelles hypothèses fait-on sur $\\varepsilon = (\\varepsilon_1,\\dots,\\varepsilon_n)$ ? (plusieurs réponses possibles)",
      "choices": [
        "$E[\\varepsilon_i \\varepsilon_j] = 0$ pour $i \\neq j$",
        "$E[\\varepsilon_i] = 0$",
        "$E[\\varepsilon_i Y_i] = 0$",
        "$Var(\\varepsilon_i) = \\sigma^2(XX')^{-1}_{ii}$",
        "$Var(\\varepsilon_i) = 0$",
        "$\\tfrac{1}{n} \\sum_i \\varepsilon_i = 0$"
      ],
      "answer": [
        0,
        1
      ],
      "explanation": "On suppose : espérance nulle ($E[\\varepsilon_i]=0$), non-corrélation des erreurs entre elles ($E[\\varepsilon_i \\varepsilon_j]=0$ pour $i\\neq j$), et variance homogène $Var(\\varepsilon_i) = \\sigma^2$. Les autres propositions sont fausses.",
      "tags": [
        "OLS",
        "hypothèses",
        "indépendance",
        "homoscédasticité"
      ],
      "id_gen": [
        "qcm006_2025-10-10"
      ]
    },
    {
      "id": "ols-q028",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "Que peut-on dire de l’estimateur MCO de $\\beta$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2$",
        "C’est le meilleur estimateur parmi tous les estimateurs sans biais",
        "Il s'agit d'un estimateur sans biais",
        "Son espérance est nulle",
        "Sa variance vaut $\\hat\\sigma^2/(n-p)$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "C’est le meilleur estimateur parmi tous les estimateurs linéaires et sans biais",
        "C’est le meilleur estimateur parmi tous les estimateurs",
        "C’est le meilleur estimateur parmi tous les estimateurs consistants"
      ],
      "answer": [
        2,
        5,
        6
      ],
      "explanation": "D’après le théorème de Gauss–Markov : l’estimateur MCO est sans biais, sa variance est $\\sigma^2(X'X)^{-1}$ et il est le meilleur estimateur linéaire sans biais (BLUE).",
      "tags": [
        "OLS",
        "Gauss–Markov",
        "BLUE",
        "estimateur"
      ]
    },
    {
      "id": "res-q029",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelles relations sont vraies ? (plusieurs réponses possibles)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition $\\hat\\varepsilon = Y - X\\hat\\beta = (I - P[X])Y = P[X]^\\perp Y$. Comme $Y = X\\beta + \\varepsilon$, on a aussi $P[X]^\\perp Y = P[X]^\\perp \\varepsilon$.",
      "tags": [
        "résidus",
        "projections",
        "OLS"
      ]
    },
    {
      "id": "var-q030",
      "qcm": "QCM2",
      "theme": "Régression linéaire",
      "question": "On note $\\hat\\varepsilon$ le vecteur des résidus de la régression. Quelle statistique donne un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\tfrac{1}{n-p}\\sum \\hat\\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum \\varepsilon_i^2$",
        "$\\tfrac{1}{n-p}\\sum \\varepsilon_i^2$",
        "$\\tfrac{1}{n}\\sum \\hat\\varepsilon_i^2$"
      ],
      "answer": 0,
      "explanation": "L’estimateur sans biais de la variance est $s^2 = \\tfrac{1}{n-p}\\sum \\hat\\varepsilon_i^2$.",
      "tags": [
        "variance",
        "estimateur sans biais",
        "résidus"
      ],
      "id_gen": [
        "qcm002_2025-10-10",
        "qcm003_2025-10-10"
      ]
    },
    {
      "id": "var-q031",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2(X'X)^{-1}$",
        "$\\sigma^2 X\\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2(XX')^{-1}$"
      ],
      "answer": 2,
      "explanation": "Puisque $Y = X\\beta + \\varepsilon$ avec $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I_n$, on a $\\operatorname{Var}(Y) = \\sigma^2 I_n$.",
      "tags": [
        "variance",
        "régression",
        "moments"
      ],
      "id_gen": [
        "qcm002_2025-10-10"
      ]
    },
    {
      "id": "multi-q032",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Qu’est-ce que la multicolinéarité dans un modèle de régression multiple ?",
      "choices": [
        "L’erreur dans l’estimation des coefficients de régression",
        "L’interaction entre $Y$ et les variables explicatives",
        "L’influence excessive d’une seule observation sur l'estimation du modèle",
        "La forte corrélation entre deux ou plusieurs variables explicatives"
      ],
      "answer": 3,
      "explanation": "La multicolinéarité correspond à une forte corrélation linéaire entre deux ou plusieurs variables explicatives, ce qui rend $X'X$ proche de la singularité.",
      "tags": [
        "régression multiple",
        "multicolinéarité",
        "colinéarité"
      ]
    },
    {
      "id": "fisher-q033",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On note SCR la somme des carrés des résidus du modèle initial, et $SCR_c$ celle d’un sous-modèle contenant $p'$ variables ($p'<p$). Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n-p}{p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR - SCR_c}{SCR}$"
      ],
      "answer": 2,
      "explanation": "La statistique de Fisher utilisée est $F = \\dfrac{(SCR_c - SCR)/(p - p')}{SCR/(n - p)} = \\dfrac{n - p}{p - p'}\\dfrac{SCR_c - SCR}{SCR}$.",
      "tags": [
        "tests de Fisher",
        "comparaison de modèles",
        "régression multiple"
      ],
      "id_gen": [
        "qcm006_2025-10-10"
      ]
    },
    {
      "id": "r2-q034",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ? (plusieurs réponses possibles)",
      "choices": [
        "Le $R^2$ augmente",
        "Le $R^2_a$ diminue",
        "La SCR diminue",
        "Le $R^2$ diminue",
        "La SCR augmente",
        "Le $R^2_a$ augmente"
      ],
      "answer": [
        0,
        2
      ],
      "explanation": "L’ajout d’une variable explique au moins autant la variance, donc $R^2$ ne peut qu’augmenter et la SCR diminuer. En revanche, $R^2_a$ peut augmenter ou baisser selon la pertinence de la variable.",
      "tags": [
        "régression multiple",
        "$R^2$",
        "SCR"
      ]
    },
    {
      "id": "hetero-q035",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Dans le contexte d’une régression multiple, qu’est-ce que l’hétéroscédasticité ?",
      "choices": [
        "Le fait que la variance des $\\varepsilon_i$ n’est pas constante",
        "Le fait que les $\\varepsilon_i$ ne sont pas corrélés entre eux",
        "Le fait que la relation entre $Y$ et les explicatives n’est pas linéaire",
        "Le fait que les coefficients du modèle sont biaisés"
      ],
      "answer": 0,
      "explanation": "L’hétéroscédasticité correspond à une variance non constante des erreurs : $\\operatorname{Var}(\\varepsilon_i) = \\sigma_i^2$ dépend de $i$.",
      "tags": [
        "régression multiple",
        "hétéroscédasticité",
        "hypothèses"
      ]
    },
    {
      "id": "res-q036",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Supposons qu’une constante est incluse dans le modèle. Quelle(s) propriété(s) vérifie(nt) le vecteur des résidus $\\hat\\varepsilon$ ? (plusieurs réponses possibles)",
      "choices": [
        "Sa variance vaut $\\sigma^2 I_n$",
        "Il est non corrélé à $Y$",
        "Sa variance vaut $\\sigma^2(X'X)^{-1}$",
        "Sa moyenne est nulle",
        "Il est de taille $p$",
        "Il est non corrélé à $\\hat Y$"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Les résidus vérifient $\\mathbb E[\\hat\\varepsilon] = 0$ (si constante incluse) et sont orthogonaux aux valeurs ajustées ($\\hat Y$).",
      "tags": [
        "résidus",
        "régression multiple",
        "propriétés"
      ],
      "id_gen": [
        "qcm002_2025-10-10",
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "linalg-q037",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\operatorname{rg}(X) = p$ ? (plusieurs réponses possibles)",
      "choices": [
        "$p \\le n$",
        "La matrice $X'X$ est inversible",
        "La corrélation entre les variables explicatives est nulle",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La matrice $X$ est inversible"
      ],
      "answer": [
        0,
        1,
        3
      ],
      "explanation": "Rang colonne plein : les colonnes de $X$ sont linéairement indépendantes, donc $X'X$ est inversible et nécessairement $p \\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.",
      "tags": [
        "plein rang",
        "algèbre linéaire",
        "régression"
      ],
      "id_gen": [
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "r2-q038",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2$ ?",
      "choices": [
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar Y)^2}{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\hat Y_i)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (\\hat Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\bar Y)^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar Y)^2}{\\sum_{i=1}^n (Y_i - \\hat Y_i)^2}$"
      ],
      "answer": 2,
      "explanation": "Définition : $R^2 = \\dfrac{\\text{Somme des carrés expliquée}}{\\text{Somme totale des carrés}} = \\dfrac{\\sum (\\hat Y_i - \\bar Y)^2}{\\sum (Y_i - \\bar Y)^2}$.",
      "tags": [
        "$R^2$",
        "qualité d’ajustement",
        "régression"
      ],
      "id_gen": [
        "qcm001_2025-10-10",
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "fisher-q039",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Fisher de significativité globale du modèle vaut 41, que concluez-vous ?",
      "choices": [
        "Toutes les variables explicatives sont significatives.",
        "Aucune variable explicative, autre que la constante, n’est significative dans le modèle.",
        "Toutes les variables explicatives, autre que la constante, sont significatives.",
        "Au moins une variable autre que la constante est significative."
      ],
      "answer": 3,
      "explanation": "Un test de Fisher global très significatif ($F=41$) permet de rejeter $H_0$ et de conclure qu’au moins une variable explicative influence $Y$.",
      "tags": [
        "tests de Fisher",
        "significativité globale",
        "régression multiple"
      ],
      "id_gen": [
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "homo-q040",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Dans le contexte d’une régression multiple, qu’est-ce que l’homoscedasticité ?",
      "choices": [
        "Le fait que les $\\varepsilon_i$ ne sont pas corrélés entre eux",
        "Le fait que la relation entre $Y$ et les variables explicatives est linéaire",
        "Le fait que l’estimateur MCO $\\hat\\beta$ est sans biais",
        "Le fait que la variance des $\\varepsilon_i$ est constante"
      ],
      "answer": 3,
      "explanation": "Homoscedasticité : $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2$ pour tout $i$. Cela signifie que la variance est constante mais pas nécessairement que les erreurs soient indépendantes.",
      "tags": [
        "régression multiple",
        "homoscédasticité",
        "hypothèses"
      ],
      "id_gen": [
        "qcm006_2025-10-10",
        "qcm007_2025-10-10"
      ]
    },
    {
      "id": "r2a-q041",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$1-\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$",
        "$1-\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{\\sum_{i=1}^n (Y_i-\\bar Y)^2}$"
      ],
      "answer": 3,
      "explanation": "Le $R^2_a$ corrige le $R^2$ en tenant compte du nombre de variables explicatives $p$. La bonne formule est $R^2_a = 1 - \\dfrac{n-1}{n-p}\\dfrac{SCR}{SCT}$.",
      "tags": [
        "$R^2_a$",
        "qualité d’ajustement",
        "régression multiple"
      ]
    },
    {
      "id": "qcm3-q01",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On considère le cas d’un modèle de régression simple : on observe le vecteur y et le vecteur x, et on estime la droite des moindres carrés associée. Que vaut l’estimation de la pente ?",
      "choices": [
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$",
        "$\\dfrac{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}{\\mathrm{cov}(x,y)}$",
        "$\\dfrac{\\mathrm{var}(x)}{\\mathrm{cov}(x,y)}$",
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}$"
      ],
      "answer": 0,
      "explanation": "Dans un modèle de régression simple $y = \\alpha + \\beta x + \\varepsilon$, l’estimateur des moindres carrés de la pente est $\\hat{\\beta} = \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$.",
      "tags": [
        "régression simple",
        "moindres carrés",
        "estimateur de pente"
      ],
      "id_gen": [
        "qcm001_2025-10-10"
      ]
    },
    {
      "id": "qcm3-q02",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur du test de Student associé au coefficient de régression de la variable $X^{(j)}$ vaut 0.003, que concluez-vous ?",
      "choices": [
        "Le modèle est surajusté.",
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
        "Le modèle de régression est inadéquat.",
        "Ce coefficient de régression est nul."
      ],
      "answer": 1,
      "explanation": "Une p-valeur de 0.003 est très inférieure au seuil classique de 5%. On rejette donc l’hypothèse nulle $H_0: \\beta_j = 0$ et on conclut qu’il existe une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "régression linéaire",
        "test de Student",
        "p-valeur",
        "significativité"
      ],
      "id_gen": [
        "qcm005_2025-10-10"
      ]
    },
    {
      "id": "qcm3-q03",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-1}{n-p}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{n-p}{n-1}\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$"
      ],
      "answer": 2,
      "explanation": "Le $R^2_a$ corrige le $R^2$ en tenant compte du nombre de variables explicatives $p$. La bonne formule est $R^2_a = 1 - \\dfrac{n-1}{n-p}\\dfrac{SCR}{SCT}$, où $SCR = \\sum (Y_i - \\hat{Y}_i)^2$ et $SCT = \\sum (Y_i - \\bar{Y})^2$.",
      "tags": [
        "régression linéaire",
        "$R^2_a$",
        "qualité d’ajustement",
        "régression multiple"
      ]
    },
    {
      "id": "qcm3-q04",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Fisher de significativité globale du modèle vaut -41, que concluez-vous ?",
      "choices": [
        "Au moins une variable autre que la constante est significative.",
        "Toutes les variables explicatives sont significatives.",
        "Aucune variable explicative, autre que la constante, n’est significative dans le modèle.",
        "Toutes les variables explicatives, autre que la constante, sont significatives."
      ],
      "answer": 0,
      "explanation": "Le test de Fisher global permet de tester l’hypothèse nulle $H_0 : \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$. Si la statistique est significative, on rejette $H_0$ et on conclut qu’au moins une variable explicative (autre que la constante) a un effet significatif sur $Y$.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "significativité globale"
      ],
      "id_gen": [
        "qcm003_2025-10-24"
      ]
    },
    {
      "id": "qcm3-q05",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ?",
      "choices": [
        "La SCR diminue",
        "Le $R^2$ diminue",
        "Le $R^2_a$ diminue",
        "Le $R^2$ augmente",
        "Le $R^2_a$ augmente",
        "La SCR augmente"
      ],
      "answer": [
        0,
        3
      ],
      "explanation": "En ajoutant une variable explicative, la somme des carrés des résidus (SCR) ne peut qu’augmenter la qualité d’ajustement ou la laisser identique. Ainsi, la SCR diminue (ou reste identique) et le $R^2$ augmente (ou reste identique). En revanche, le $R^2_a$ peut augmenter ou diminuer selon la pertinence de la variable ajoutée.",
      "tags": [
        "régression linéaire",
        "SCR",
        "$R^2$",
        "$R^2_a$"
      ],
      "id_gen": [
        "qcm003_2025-10-24"
      ]
    },
    {
      "id": "qcm3-q06",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On considère le cas d’un modèle de régression simple : on observe le vecteur y et le vecteur x, et on estime la droite des moindres carrés associée. Que vaut l’estimation de l’ordonnée à l’origine ?",
      "choices": [
        "$\\bar{y} - \\dfrac{\\mathrm{cov}(y,x)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}\\,\\bar{x}$",
        "$\\dfrac{\\mathrm{cov}(y,x)}{\\sqrt{\\mathrm{var}(x)\\mathrm{var}(y)}}$",
        "$\\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$",
        "$\\bar{y} - \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}\\,\\bar{x}$"
      ],
      "answer": 3,
      "explanation": "Dans la régression simple $y = \\alpha + \\beta x + \\varepsilon$, on a $\\hat{\\beta} = \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}$ et donc $\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\,\\bar{x}$. Cela donne bien $\\hat{\\alpha} = \\bar{y} - \\dfrac{\\mathrm{cov}(x,y)}{\\mathrm{var}(x)}\\,\\bar{x}$.",
      "tags": [
        "régression linéaire",
        "moindres carrés",
        "ordonnée à l’origine"
      ]
    },
    {
      "id": "qcm3-q07",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "La statistique du test Student associé au coefficient de régression de la variable $X^{(j)}$ vaut -23, que concluez-vous ?",
      "choices": [
        "Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
        "Ce coefficient de régression est nul.",
        "Le modèle de régression est inadéquat.",
        "Le modèle est surajusté."
      ],
      "answer": 0,
      "explanation": "Une statistique de Student de grande valeur absolue (ici -23) implique une p-valeur extrêmement faible. On rejette donc $H_0 : \\beta_j = 0$ et on conclut qu’il existe une relation statistiquement significative entre $X^{(j)}$ et $Y$.",
      "tags": [
        "régression linéaire",
        "test de Student",
        "significativité"
      ]
    },
    {
      "id": "qcm3-q08",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2$ ?",
      "choices": [
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}$"
      ],
      "answer": 1,
      "explanation": "Le coefficient de détermination $R^2$ est défini par $R^2 = \\dfrac{SCE}{SCT}$, où $SCE = \\sum (\\hat{Y}_i - \\bar{Y})^2$ est la somme des carrés expliquée et $SCT = \\sum (Y_i - \\bar{Y})^2$ est la somme des carrés totale.",
      "tags": [
        "régression linéaire",
        "$R^2$",
        "qualité d’ajustement"
      ]
    },
    {
      "id": "qcm3-q09",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "On note $SCR$ la somme des carrés des résidus du modèle initial, et $SCR_c$ celle d’un sous-modèle contenant $p'$ variables ($p' < p$). Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n-p}{p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR - SCR_c}{SCR}$",
        "$\\dfrac{n-p}{p'}\\dfrac{SCR_c - SCR}{SCR}$",
        "$\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$"
      ],
      "answer": 3,
      "explanation": "Pour comparer un modèle complet (p variables) et un sous-modèle restreint (p' variables), on utilise un test de Fisher. La statistique est $F = \\dfrac{(SCR_c - SCR)/(p - p')}{SCR/(n-p)}$. En simplifiant, le numérateur est proportionnel à $\\dfrac{n-p}{p-p'}\\dfrac{SCR_c - SCR}{SCR}$, ce qui correspond à la réponse D.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "comparaison de modèles"
      ]
    },
    {
      "id": "qcm3-q10",
      "qcm": "QCM3",
      "theme": "Régression linéaire",
      "question": "Supposons qu’une constante est incluse dans le modèle. Quelle(s) propriété(s) vérifie le vecteur des résidus $\\hat{\\varepsilon}$ ?",
      "choices": [
        "Sa variance vaut $\\sigma^2 (X'X)^{-1}$",
        "Sa moyenne est nulle",
        "Sa variance vaut $\\mathrm{Var}(\\hat{\\varepsilon}) = \\sigma^2 P_{[X]^\\perp}$ ($= \\sigma^2 \\left(I_n - X (X'X)^{-1} X'\\right)$",
        "Son espérance est nulle",
        "Il est de taille $p$",
        "Sa variance vaut $\\sigma^2 I_n$"
      ],
      "answer": [
        1,
        2,
        3
      ],
      "explanation": "Le vecteur des résidus est $\\hat{\\varepsilon} = M Y$ avec $M = I - P_X$, la matrice de projection sur $[X]^\\perp$. On a $\\mathbb{E}[\\hat{\\varepsilon}] = 0$, $\\mathrm{Var}(\\hat{\\varepsilon}) = \\sigma^2 M = \\sigma^2 P_{[X]^\\perp}$, et sa somme vaut 0 donc sa moyenne est nulle.",
      "tags": [
        "régression linéaire",
        "résidus",
        "espérance",
        "variance"
      ]
    },
    {
      "id": "qcm5-q01",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Que vaut la variance du vecteur $Y$ ?",
      "choices": [
        "$\\sigma^2 (X'X)^{-1}$",
        "$\\sigma^2 X \\beta$",
        "$\\sigma^2 I_n$",
        "$\\sigma^2 (XX')^{-1}$"
      ],
      "answer": [
        2
      ],
      "explanation": "Dans le modèle $Y = X\\beta + \\varepsilon$ avec $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, on a $\\mathrm{Var}(Y) = \\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$.",
      "tags": [
        "régression linéaire",
        "variance",
        "modèle linéaire"
      ],
      "id_gen": [
        "qcm004_2025-10-10"
      ]
    },
    {
      "id": "qcm5-q02",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Que vaut $\\hat{\\beta}$, l’estimateur par moindres carrés ordinaires de $\\beta$ ?",
      "choices": [
        "$\\mathbb{E}((X'X)^{-1} X'Y)$",
        "$(X'X)^{-1} X'Y$",
        "$\\mathbb{E}((XX')^{-1} XY)$",
        "$(XX')^{-1} XY$"
      ],
      "answer": [
        1
      ],
      "explanation": "Par définition de l’estimateur des moindres carrés ordinaires (MCO), on a $\\hat{\\beta} = (X'X)^{-1} X'Y$, obtenu en minimisant la somme des carrés des résidus $\\|Y - X\\beta\\|^2$.",
      "tags": [
        "régression linéaire",
        "estimateur",
        "MCO"
      ],
      "id_gen": [
        "qcm003_2025-10-10",
        "qcm005_2025-10-10",
        "qcm002_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q03",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Comment sont définis les résidus $\\hat{\\varepsilon}$ ? (On note $P_{[X]} = X (X'X)^{-1} X'$ et $P_{[X]}^{\\perp} = I_n - P_{[X]}$.)",
      "choices": [
        "$\\hat\\varepsilon = P[X]Y$",
        "$\\hat\\varepsilon = \\hat Y - X\\beta$",
        "$\\hat\\varepsilon = Y - X\\hat\\beta$",
        "$\\hat\\varepsilon = P[X]\\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp \\varepsilon$",
        "$\\hat\\varepsilon = P[X]^\\perp Y$",
        "$\\hat\\varepsilon = Y - X\\beta$"
      ],
      "answer": [
        2,
        4,
        5
      ],
      "explanation": "Par définition, $\\hat{\\varepsilon} = Y - X\\hat{\\beta} = (I_n - P_{[X]})Y = P_{[X]}^{\\perp} Y$. En utilisant $Y = X\\beta + \\varepsilon$, on obtient aussi $\\hat{\\varepsilon} = P_{[X]}^{\\perp} \\varepsilon$. En revanche, $P_{[X]} Y$ donne les valeurs ajustées $\\hat{Y}$, et utiliser $Y - X\\beta$ correspond à l'erreur vraie, pas aux résidus.",
      "tags": [
        "régression linéaire",
        "résidus",
        "projection",
        "matrices de projection"
      ],
      "id_gen": [
        "qcm003_2025-10-10",
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q04",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle statistique constitue un estimateur sans biais de $\\sigma^2$ ?",
      "choices": [
        "$\\dfrac{1}{n} \\sum_{i=1}^{n} \\varepsilon_i^{2}$",
        "$\\dfrac{1}{n} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$",
        "$\\dfrac{1}{n-p} \\sum_{i=1}^{n} \\varepsilon_i^{2}$",
        "$\\dfrac{1}{n-p} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$"
      ],
      "answer": [
        3
      ],
      "explanation": "Avec $M = I_n - P_{[X]}$ (rang $n-p$), la somme des carr\\u00e9s des r\\u00e9sidus est $\\text{RSS} = \\hat{\\varepsilon}'\\hat{\\varepsilon} = Y' M Y$. On a $\\mathbb{E}[\\text{RSS}] = (n-p)\\sigma^2$, d'o\\u00f9 l'estimateur sans biais $\\hat{\\sigma}^2 = \\dfrac{\\text{RSS}}{n-p} = \\dfrac{1}{n-p} \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^{2}$.",
      "tags": [
        "régression linéaire",
        "variance",
        "r\\u00e9sidus",
        "degr\\u00e9s de libert\\u00e9"
      ],
      "id_gen": [
        "qcm003_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q05",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Parmi les quantités suivantes, lesquelles sont observables ou calculables à partir des observations ?",
      "choices": [
        "$\\hat{\\beta}$",
        "$\\hat{Y}$",
        "$Y$",
        "$\\hat{\\varepsilon}$",
        "$\\beta$",
        "$\\sigma^2$",
        "$\\varepsilon$"
      ],
      "answer": [
        0,
        1,
        2,
        3
      ],
      "explanation": "Les quantités observables ou calculables directement à partir des données $(X, Y)$ sont : $\\hat{\\beta}$ (estimateur MCO), $\\hat{Y} = X\\hat{\\beta}$ (valeurs ajustées), $Y$ (observations) et $\\hat{\\varepsilon} = Y - \\hat{Y}$ (résidus). En revanche, $\\beta$, $\\sigma^2$ et $\\varepsilon$ sont inobservables et ne peuvent être connus qu'au travers d'estimateurs.",
      "tags": [
        "régression linéaire",
        "observables",
        "paramètres",
        "résidus"
      ],
      "id_gen": [
        "qcm002_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q06",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelles sont les variables non corrélées ?",
      "choices": [
        "$\\hat{Y}_1$ et $\\hat{\\varepsilon}_1$",
        "$\\hat{\\beta}_1$ et $\\hat{\\beta}_2$",
        "$Y_1$ et $\\hat{Y}_1$",
        "$\\varepsilon_1$ et $\\hat{\\varepsilon}_1$",
        "$\\varepsilon_1$ et $\\varepsilon_2$",
        "$Y_1$ et $Y_2$",
        "$Y_1$ et $\\hat{\\varepsilon}_1$",
        "$\\hat{\\varepsilon}_1$ et $\\hat{\\varepsilon}_2$"
      ],
      "answer": [
        0,
        4,
        5
      ],
      "explanation": "Les valeurs ajustées et les résidus ($\\hat{Y}$ et $\\hat{\\varepsilon}$) sont orthogonaux, donc non corrélés. Les erreurs vraies $\\varepsilon_i$ sont indépendantes, donc non corrélées. Enfin, dans le cadre classique des MCO avec erreurs indépendantes, les $Y_i$ sont eux aussi non corrélés. En revanche, les autres couples sont en général corrélés.",
      "tags": [
        "régression linéaire",
        "corrélation",
        "résidus",
        "valeurs ajustées",
        "observations"
      ]
    },
    {
      "id": "qcm5-q07",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Qu’implique l’hypothèse $\\mathrm{rg}(X)=p$ ?",
      "choices": [
        "$p \\le n$",
        "Les colonnes de $X$ sont linéairement indépendantes",
        "La corrélation entre les variables explicatives est nulle",
        "La matrice $X$ est inversible",
        "La matrice $X'X$ est inversible"
      ],
      "answer": [
        0,
        1,
        4
      ],
      "explanation": "Si $\\mathrm{rg}(X)=p$, les colonnes de $X$ sont linéairement indépendantes, donc nécessairement $p \\le n$. Par ailleurs, $X'X$ est alors définie positive et inversible. En revanche, $X$ n’est pas forcément carrée (donc pas forcément inversible) et le rang plein n’implique pas l’absence de corrélation entre variables explicatives.",
      "tags": [
        "régression linéaire",
        "rang",
        "matrices",
        "identifiabilité"
      ]
    },
    {
      "id": "qcm5-q08",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Qu’est-ce que la multicolinéarité dans un modèle de régression multiple ?",
      "choices": [
        "L’influence excessive d’une seule observation sur l’estimation du modèle",
        "L’interaction entre $Y$ et les variables explicatives",
        "L’erreur dans l’estimation des coefficients de régression",
        "La forte corrélation entre deux ou plusieurs variables explicatives"
      ],
      "answer": [
        3
      ],
      "explanation": "La multicolinéarité désigne une forte corrélation (souvent linéaire) entre deux ou plusieurs variables explicatives. Elle rend les colonnes de $X$ presque dépendantes, ce qui entraîne des problèmes numériques et une instabilité dans l’estimation de $\\hat{\\beta}$.",
      "tags": [
        "régression linéaire",
        "multicolinéarité",
        "variables explicatives"
      ],
      "id_gen": [
        "qcm001_2025-10-10",
        "qcm003_2025-10-10"
      ]
    },
    {
      "id": "qcm5-q09",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle est la formule du $R^2_a$ lorsqu’une constante est incluse dans le modèle ?",
      "choices": [
        "$\\dfrac{n-p}{n-1} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$\\dfrac{n-1}{n-p} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-p}{n-1} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$",
        "$1 - \\dfrac{n-1}{n-p} \\dfrac{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}$"
      ],
      "answer": [
        3
      ],
      "explanation": "L’indice de détermination ajusté est défini par $R^2_a = 1 - \\dfrac{n-1}{n-p} \\cdot \\dfrac{\\text{SCR}}{\\text{SCT}}$, où $\\text{SCR} = \\sum (Y_i - \\hat{Y}_i)^2$ et $\\text{SCT} = \\sum (Y_i - \\bar{Y})^2$. Cette correction par $\\tfrac{n-1}{n-p}$ permet de compenser la tendance du $R^2$ à augmenter avec le nombre de variables explicatives.",
      "tags": [
        "régression linéaire",
        "R^2 ajusté",
        "variance expliquée",
        "qualité d’ajustement"
      ]
    },
    {
      "id": "qcm5-q10",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On note $R^2$ le $R^2$ obtenu après estimation du modèle initial, et $R^2_c$ celui d’un sous-modèle contenant $p'$ variables ($p' < p$) dont une constante. Avec quelle statistique peut-on comparer les deux modèles ?",
      "choices": [
        "$\\dfrac{n - p}{p'} \\dfrac{R^2_c - R^2}{1 - R^2}$",
        "$\\dfrac{n - p}{p - p'} \\dfrac{R^2_c - R^2}{1 - R^2}$",
        "$\\dfrac{n - p}{p - p'} \\dfrac{R^2 - R^2_c}{1 - R^2}$",
        "$\\dfrac{n - p}{p'} \\dfrac{R^2 - R^2_c}{1 - R^2}$"
      ],
      "answer": [
        2
      ],
      "explanation": "La statistique de Fisher utilisée pour comparer un modèle complet à un sous-modèle imbriqué est $F = \\dfrac{(R^2 - R_c^2)/(p - p')}{(1 - R^2)/(n - p)}$, ce qui équivaut à $\\dfrac{n - p}{p - p'} \\cdot \\dfrac{R^2 - R_c^2}{1 - R^2}$. Cette statistique suit une loi de Fisher à $(p - p')$ et $(n - p)$ degrés de liberté sous l’hypothèse nulle.",
      "tags": [
        "régression linéaire",
        "test de Fisher",
        "R^2",
        "comparaison de modèles"
      ],
      "id_gen": [
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "qcm5-q11",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "Quelle méthode permet de détecter une observation ayant une influence excessive sur un modèle de régression multiple ?",
      "choices": [
        "Le test de Durbin-Watson",
        "Le test de Breusch-Godfrey",
        "Le test de Shapiro-Wilk",
        "La distance de Cook"
      ],
      "answer": [
        3
      ],
      "explanation": "La distance de Cook mesure l’influence d’une observation sur l’estimation des coefficients de régression. Une valeur élevée indique qu’une seule observation exerce une influence excessive sur le modèle. Les autres tests cités concernent respectivement l’autocorrélation (Durbin-Watson, Breusch-Godfrey) et la normalité (Shapiro-Wilk).",
      "tags": [
        "régression linéaire",
        "influence",
        "diagnostic",
        "distance de Cook"
      ]
    },
    {
      "id": "qcm5-q12",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ?",
      "choices": [
        "Le $R^2$ augmente",
        "La SCR diminue",
        "Le $R^2_a$ diminue",
        "Le $R^2$ diminue",
        "La SCR augmente",
        "Le $R^2_a$ augmente"
      ],
      "answer": [
        0,
        1
      ],
      "explanation": "L’ajout d’une variable explicative au modèle entraîne toujours une diminution (ou au pire une constance) de la somme des carrés des résidus (SCR), ce qui implique une augmentation (ou constance) du $R^2$. En revanche, le $R^2_a$ peut augmenter ou diminuer selon la pertinence de la variable ajoutée.",
      "tags": [
        "régression linéaire",
        "R^2",
        "SCR",
        "ajout de variables"
      ]
    },
    {
      "id": "qcm5-q13",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On compare plusieurs modèles avec les critères AIC et BIC.",
      "choices": [
        "Le meilleur modèle au sens du BIC ne coïncide pas forcément avec le meilleur modèle au sens de l’AIC.",
        "Le meilleur modèle est automatiquement détecté : il s’agit de celui qui maximise l’AIC et qui maximise le BIC.",
        "Le BIC choisira un modèle de taille plus petite ou égale à celui choisi par l’AIC",
        "Il est possible que AIC et BIC soient tous les deux optimaux pour le plus gros modèle",
        "Si les modèles ont des tailles différentes, AIC choisira forcément le plus petit",
        "Si les modèles ont des tailles différentes, BIC choisira forcément le plus petit",
        "L’AIC choisira un modèle de taille plus petite ou égale à celui choisi par le BIC",
        "Le meilleur modèle est automatiquement détecté : il s’agit de celui qui minimise l’AIC et qui minimise le BIC."
      ],
      "answer": [
        0,
        2,
        3
      ],
      "explanation": "AIC et BIC sont deux critères d’information qui se minimisent. Le BIC pénalise davantage la complexité, ce qui conduit souvent à choisir un modèle plus petit ou égal à celui de l’AIC. Les deux critères peuvent mener à des choix différents, mais il arrive qu’ils coïncident. En revanche, aucun ne se maximise.",
      "tags": [
        "régression linéaire",
        "AIC",
        "BIC",
        "comparaison de modèles"
      ]
    },
    {
      "id": "qcm5-q14",
      "qcm": "QCM5",
      "theme": "Régression linéaire",
      "question": "On a supposé $V(\\varepsilon) = \\sigma^2 I_n$. Si l’erreur de modélisation $\\varepsilon$ était hétéroscédastique, que deviendrait la matrice $V(\\varepsilon)$ ?",
      "choices": [
        "Les valeurs sur sa diagonale seraient différentes",
        "Elle ne serait pas inversible",
        "Elle ne serait plus diagonale",
        "Les valeurs sur la diagonale seraient nulles"
      ],
      "answer": [
        0
      ],
      "explanation": "En cas d’hétéroscédasticité, les erreurs ont des variances différentes mais restent supposées indépendantes. Ainsi $V(\\varepsilon) = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$ : la matrice reste diagonale mais ses valeurs sur la diagonale diffèrent. Elle demeure inversible tant qu’aucune variance n’est nulle.",
      "tags": [
        "régression linéaire",
        "hétéroscédasticité",
        "variance des erreurs",
        "matrice de variance"
      ]
    },
    {
      "id": "qcm4-q06",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Dans le contexte d’une régression multiple, qu’est-ce que l’hétéroscédasticité ?",
      "choices": [
        "Le fait que les εⁱ ne sont pas corrélés entre eux",
        "Le fait que les coefficients du modèle sont biaisés",
        "Le fait que la variance des εⁱ n’est pas constante",
        "Le fait que la relation entre Y et les variables explicatives n’est pas linéaire"
      ],
      "answer": [
        2
      ],
      "explanation": "L’hétéroscédasticité signifie que la variance des erreurs εⁱ varie selon les observations, c’est-à-dire qu’elle n’est pas constante. Cela viole l’hypothèse d’homoscedasticité du modèle linéaire classique.",
      "tags": [
        "hétéroscédasticité",
        "variance des erreurs",
        "régression multiple"
      ]
    },
    {
      "id": "qcm4-q07",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelles affirmations à propos des valeurs de levier $h^{ii}$ sont vraies ?",
      "choices": [
        "$\\sum_{i=1}^n h^{ii} = p$ (nombre de paramètres)",
        "Le levier dépend de la variable réponse $Y$",
        "$h^{ii} = (P^{[X]})^{ii}$",
        "Les points à fort effet levier ont toujours de grands résidus",
        "$h^{ii} > 2p/n$ indique un point à fort effet levier"
      ],
      "answer": [
        0,
        2,
        4
      ],
      "explanation": "Les valeurs de levier $h^{ii}$ proviennent de la matrice de projection $P^{[X]} = X(X^{\\top}X)^{-1}X^{\\top}$. Leur somme vaut le nombre de paramètres $p$. Elles ne dépendent que de $X$, pas de $Y$. Une valeur élevée (souvent $> 2p/n$) signale un point influent.",
      "tags": [
        "valeurs de levier",
        "influence",
        "projection",
        "régression linéaire multiple"
      ]
    },
    {
      "id": "qcm4-q08",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelle est la corrélation théorique entre les résidus $\\hat{\\varepsilon}$ et les valeurs ajustées $\\hat{Y}$ ?",
      "choices": [
        "1",
        "Elle dépend des données",
        "0",
        "$\\sigma^2$"
      ],
      "answer": [
        2
      ],
      "explanation": "Dans le modèle de régression linéaire, les résidus $\\hat{\\varepsilon}$ sont orthogonaux aux valeurs ajustées $\\hat{Y}$. Cela implique une corrélation théorique nulle entre les deux ($\\operatorname{Corr}(\\hat{\\varepsilon}, \\hat{Y}) = 0$).",
      "tags": [
        "corrélation",
        "résidus",
        "valeurs ajustées",
        "orthogonalité"
      ]
    },
    {
      "id": "qcm4-q09",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Vous souhaitez tester si les variables $X^3$, $X^4$, $X^5$ peuvent être supprimées simultanément d’un modèle avec 8 régresseurs (y compris l’intercept). Sous $H_0$, quelle est la loi de la statistique de test de Fisher ?",
      "choices": [
        "$F_{3,\\,n-5}$",
        "$F_{5,\\,n-8}$",
        "$F_{3,\\,n-8}$",
        "$F_{5,\\,n-5}$"
      ],
      "answer": [
        2
      ],
      "explanation": "Sous l’hypothèse nulle $H_0 : \\beta^3 = \\beta^4 = \\beta^5 = 0$, on compare un modèle restreint (5 régressseurs) à un modèle complet (8 régressseurs). La statistique de Fisher suit alors une loi $F_{3,\\,n-8}$, car on teste 3 contraintes sur un modèle avec 8 paramètres estimés.",
      "tags": [
        "test de Fisher",
        "modèles emboîtés",
        "régression multiple",
        "hypothèse nulle"
      ],
      "id_gen": [
        "qcm002_2025-10-24"
      ]
    },
    {
      "id": "qcm4-q10",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Après une recherche exhaustive basée sur le $R^2$ ajusté (c’est-à-dire le $R^2_a$), que peut-on affirmer sur le modèle retenu ?",
      "choices": [
        "Il ne présente aucun problème d’autocorrélation",
        "Il a le plus grand $R^2_a$",
        "Il ne présente aucun problème d’hétéroscédasticité",
        "Il a le plus faible $R^2_a$",
        "Il a le meilleur score BIC",
        "Il peut souffrir de multicolinéarité"
      ],
      "answer": [
        1,
        5
      ],
      "explanation": "La sélection fondée sur le $R^2_a$ vise à maximiser ce critère, donc le modèle retenu possède le plus grand $R^2_a$. Cependant, cela n’exclut pas la présence de multicolinéarité entre variables explicatives.",
      "tags": [
        "R² ajusté",
        "sélection de modèle",
        "multicolinéarité",
        "BIC"
      ]
    },
    {
      "id": "qcm4-q11",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Quelle forme du nuage de points des résidus indique un problème d’hétéroscédasticité ?",
      "choices": [
        "Dispersion constante mais trois valeurs aberrantes",
        "Forme d’entonnoir : la variance augmente avec les valeurs ajustées",
        "Forme parabolique",
        "Nuage aléatoire avec une dispersion constante"
      ],
      "answer": [
        1
      ],
      "explanation": "Une forme d’entonnoir dans le nuage des résidus indique que la variance des erreurs augmente avec les valeurs ajustées, ce qui caractérise un problème d’hétéroscédasticité.",
      "tags": [
        "hétéroscédasticité",
        "résidus",
        "diagnostic graphique",
        "régression linéaire"
      ]
    },
    {
      "id": "qcm4-q12",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "La distance de Cook permet de détecter :",
      "choices": [
        "Les points à fort levier",
        "Les problèmes de surapprentissage",
        "L’autocorrélation des résidus",
        "L’hétéroscédasticité",
        "Les valeurs aberrantes"
      ],
      "answer": [
        0,
        4
      ],
      "explanation": "La distance de Cook met en évidence les observations ayant une forte influence sur le modèle, c’est-à-dire celles combinant un fort levier et un grand résidu. Elle permet donc d’identifier les points à fort levier et les valeurs aberrantes.",
      "tags": [
        "distance de Cook",
        "influence",
        "valeurs aberrantes",
        "levier"
      ]
    },
    {
      "id": "qcm4-q13",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "La statistique du test de Breusch–Godfrey donne une p-valeur de 0,8. Que cela suggère-t-il ?",
      "choices": [
        "Aucune hétéroscédasticité dans les données",
        "Une autocorrélation positive des résidus",
        "Aucune autocorrélation des résidus",
        "La présence d’un problème d’hétéroscédasticité"
      ],
      "answer": [
        2
      ],
      "explanation": "Une p-valeur élevée (ici 0,8) conduit à ne pas rejeter l’hypothèse nulle du test de Breusch–Godfrey, selon laquelle il n’existe pas d’autocorrélation des résidus. On conclut donc à l’absence d’autocorrélation.",
      "tags": [
        "test de Breusch–Godfrey",
        "autocorrélation",
        "résidus",
        "régression linéaire"
      ],
      "id_gen": [
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "qcm4-q14",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Dans un test de modèles emboîtés comparant des modèles à 10 et 7 paramètres, on obtient $F = 2{,}6$ avec une p-valeur de $0{,}22$. Que recommandez-vous ?",
      "choices": [
        "Il faut effectuer des tests de Student pour plus de puissance statistique",
        "Le modèle le plus simple à 7 paramètres est adéquat",
        "Il faut plus de données pour conclure",
        "Le modèle complet à 10 paramètres est nécessaire"
      ],
      "answer": [
        1
      ],
      "explanation": "Une p-valeur élevée (0,22) indique qu’on ne rejette pas l’hypothèse nulle selon laquelle le modèle restreint à 7 paramètres est suffisant. On retient donc le modèle le plus simple, jugé adéquat.",
      "tags": [
        "test de Fisher",
        "modèles emboîtés",
        "sélection de modèle",
        "régression multiple"
      ],
      "id_gen": [
        "qcm003_2025-10-24"
      ]
    },
    {
      "id": "qcm4-q15",
      "qcm": "QCM4",
      "theme": "Régression linéaire",
      "question": "Après une recherche exhaustive basée sur le BIC, que peut-on affirmer sur le modèle retenu ?",
      "choices": [
        "Il a le meilleur score $C_p$ de Mallows",
        "Il ne présente aucun problème de multicolinéarité",
        "Il ne présente aucun problème d’hétéroscédasticité",
        "Il a le plus faible BIC",
        "Il a le plus grand BIC",
        "Il peut souffrir de problèmes d’autocorrélation"
      ],
      "answer": [
        3,
        5
      ],
      "explanation": "Le critère BIC (Bayesian Information Criterion) est minimisé pour sélectionner le meilleur modèle. Ainsi, le modèle retenu est celui avec le plus faible BIC, mais cela ne garantit pas l’absence d’autocorrélation des résidus.",
      "tags": [
        "BIC",
        "sélection de modèle",
        "autocorrélation",
        "critères d’information"
      ]
    },
    {
      "id": "qcm6-q01",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Dans un modèle de régression linéaire simple, l’estimation de la pente vaut $\\tfrac{1}{4}$, d’écart-type estimé $0{,}1$. En vous aidant des quantiles donnés dans la table ci-dessous, à quel niveau $\\alpha$ pouvez-vous affirmer que cette pente est significativement différente de 0 ?",
      "image": "/Images/regression_lineaire/qcm6-2024-q1.png",
      "choices": [
        "$\\alpha = 0{,}1$",
        "$\\alpha = 0{,}05$",
        "$\\alpha = 0{,}001$",
        "$\\alpha = 0{,}01$",
        "$\\alpha = 0{,}15$"
      ],
      "answer": [
        0,
        1,
        4
      ],
      "explanation": "La statistique de test vaut $t = \\tfrac{0{,}25}{0{,}1} = 2{,}5$. En comparant aux quantiles de Student (≈2{,}07 pour $\\alpha=0{,}1$, ≈1{,}71 pour $\\alpha=0{,}15$, ≈2{,}49 pour $\\alpha=0{,}05$), on rejette $H_0$ pour $\\alpha = 0{,}15$, $0{,}1$ et légèrement pour $0{,}05$, mais pas pour des niveaux plus faibles.",
      "tags": [
        "test de Student",
        "pente de régression",
        "niveau de signification",
        "régression simple"
      ]
    },
    {
      "id": "qcm6-q02",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "L’ANOVA permet de tester des différences significatives :",
      "choices": [
        "Entre deux ou plusieurs groupes",
        "Entre deux groupes seulement",
        "Entre plus de deux groupes uniquement",
        "Entre des variables qualitatives"
      ],
      "answer": [
        0
      ],
      "explanation": "L’ANOVA (analyse de la variance) compare les moyennes de plusieurs groupes pour déterminer s’il existe au moins une différence significative. Elle s’applique dès deux groupes, mais est surtout utile pour plus de deux.",
      "tags": [
        "ANOVA",
        "analyse de la variance",
        "comparaison de moyennes",
        "tests statistiques"
      ]
    },
    {
      "id": "qcm6-q03",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Si la p-valeur obtenue dans une ANOVA vaut 0.013, cela signifie que :",
      "choices": [
        "Il y a une différence significative de variance entre au moins deux groupes",
        "Les moyennes des groupes sont toutes significativement différentes",
        "Toutes les moyennes des groupes sont identiques",
        "Les variances des groupes sont toutes significativement différentes",
        "Il y a une différence significative de moyenne entre au moins deux groupes"
      ],
      "answer": [
        4
      ],
      "explanation": "Une p-valeur de 0.013 est inférieure au seuil usuel de 5 %, donc on rejette l’hypothèse nulle d’égalité des moyennes. L’ANOVA conclut qu’il existe une différence significative de moyenne entre au moins deux groupes.",
      "tags": [
        "ANOVA",
        "p-valeur",
        "test de Fisher",
        "comparaison de moyennes"
      ],
      "id_gen": [
        "qcm001_2025-10-24"
      ]
    },
    {
      "id": "qcm6-q04",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "On considère un modèle de régression contenant deux variables explicatives, qui sont deux facteurs à respectivement 3 et 4 modalités. Si on n’introduit aucune interaction, combien de coefficients seront estimés ?",
      "choices": [
        "5",
        "6",
        "9",
        "8",
        "12",
        "7",
        "10"
      ],
      "answer": [
        1
      ],
      "explanation": "Un facteur à 3 modalités nécessite 2 coefficients (car une modalité sert de référence) et un facteur à 4 modalités en nécessite 3. Avec l’intercept, on obtient donc $1 + 2 + 3 = 6$ coefficients estimés.",
      "tags": [
        "variables qualitatives",
        "facteurs",
        "paramétrisation",
        "modèle linéaire"
      ]
    },
    {
      "id": "qcm6-q05",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Le test $F$ utilisé dans l’ANOVA est défini comme :",
      "choices": [
        "La moyenne (normalisée) des différences de variances entre chaque groupe",
        "Le rapport entre la variance intergroupes et la variance totale",
        "Le rapport entre la variance intergroupes et la variance intragroupes",
        "La moyenne (normalisée) des différences de moyennes entre chaque groupe"
      ],
      "answer": [
        2
      ],
      "explanation": "Le test $F$ de l’ANOVA repose sur le rapport entre la variance intergroupes (due au modèle) et la variance intragroupes (due à l’erreur). Une valeur élevée de ce rapport indique une différence significative entre les groupes.",
      "tags": [
        "ANOVA",
        "test F",
        "variance intergroupes",
        "variance intragroupes"
      ]
    },
    {
      "id": "qcm6-q06",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Dans un modèle de régression linéaire simple, l’estimation de l’ordonnée à l’origine vaut $-2$, d’écart-type estimé $1$. En vous aidant des quantiles donnés dans la table ci-dessous, à quel niveau $\\alpha$ pouvez-vous affirmer que cette ordonnée à l’origine est significativement différente de 0 ?",
      "image": "/Images/regression_lineaire/qcm6-2024-q6.png",
      "choices": [
        "$\\alpha = 0{,}15$",
        "$\\alpha = 0{,}05$",
        "$\\alpha = 0{,}001$",
        "$\\alpha = 0{,}01$",
        "$\\alpha = 0{,}1$"
      ],
      "answer": [
        0,
        4
      ],
      "explanation": "La statistique de test vaut $t = \\tfrac{-2}{1} = -2$. En valeur absolue, $|t| = 2$, ce qui dépasse les seuils critiques pour $\\alpha = 0{,}15$ et $\\alpha = 0{,}1$, mais pas pour $\\alpha$ plus petits. On rejette donc $H_0$ à ces niveaux uniquement.",
      "tags": [
        "test de Student",
        "ordonnée à l’origine",
        "niveau de signification",
        "régression simple"
      ]
    },
    {
      "id": "qcm6-q07",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Quel est l’objectif principal de l’ANOVA ?",
      "choices": [
        "Comparer les variances d’une variable quantitative dans plusieurs groupes",
        "Tester la significativité de l’égalité de plusieurs variances",
        "Déterminer si la corrélation entre deux variables qualitatives est significative",
        "Comparer les moyennes d’une variable quantitative dans plusieurs groupes"
      ],
      "answer": [
        3
      ],
      "explanation": "L’ANOVA (analyse de la variance) vise à comparer les moyennes d’une variable quantitative entre plusieurs groupes afin de déterminer si elles diffèrent significativement.",
      "tags": [
        "ANOVA",
        "comparaison de moyennes",
        "analyse statistique",
        "variance intergroupes"
      ]
    },
    {
      "id": "qcm6-q08",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Parmi ces options, quelle est la principale condition à vérifier avant d’appliquer une ANOVA ?",
      "choices": [
        "Vérifier que chaque groupe a même espérance",
        "Vérifier la normalité des résidus",
        "Vérifier que chaque groupe a même variance",
        "Détecter la présence de valeurs aberrantes dans les données",
        "Vérifier que chaque groupe a même taille"
      ],
      "answer": [
        2
      ],
      "explanation": "L’ANOVA suppose l’homogénéité des variances (homoscédasticité) entre les groupes. Cette condition garantit la validité du test de Fisher utilisé pour comparer les moyennes.",
      "tags": [
        "ANOVA",
        "conditions d’application",
        "homoscédasticité",
        "variance"
      ]
    },
    {
      "id": "qcm6-q09",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "On considère un modèle de régression contenant deux variables explicatives, qui sont un facteur à trois modalités et une variable quantitative. Si on introduit une interaction entre les deux variables, combien de coefficients seront estimés ?",
      "choices": [
        "3",
        "7",
        "4",
        "6",
        "9",
        "8",
        "5"
      ],
      "answer": [
        3
      ],
      "explanation": "Un facteur à trois modalités nécessite 2 coefficients, une variable quantitative en ajoute 1, et l’interaction entre les deux introduit 2 coefficients supplémentaires (car elle dépend des modalités du facteur). Avec l’intercept, on obtient $1 + 2 + 1 + 2 = 6$ coefficients estimés.",
      "tags": [
        "interaction",
        "modèle linéaire",
        "facteurs",
        "variables explicatives"
      ]
    },
    {
      "id": "qcm6-q10",
      "qcm": "QCM6",
      "theme": "Régression linéaire",
      "question": "Lorsqu’une ANOVA indique une différence significative, que devrait-on faire pour identifier précisément quels groupes diffèrent ?",
      "choices": [
        "Effectuer un test de Fisher",
        "Effectuer un test de Tukey",
        "Effectuer des tests de Student 2 à 2",
        "Effectuer un test de Durbin–Watson"
      ],
      "answer": [
        1
      ],
      "explanation": "Après une ANOVA significative, le test de Tukey permet de comparer toutes les paires de moyennes tout en contrôlant le risque global d’erreur de première espèce.",
      "tags": [
        "ANOVA",
        "test de Tukey",
        "comparaison multiple",
        "analyse post-hoc"
      ]
    }
  ]
}