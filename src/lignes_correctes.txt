question;choices;answer;explanation;tags
"On se place dans le modèle $Y=X\beta+\varepsilon$ (rang(X)=p)). Quelles hypothèses fait-on sur $\varepsilon$ ? (plusieurs réponses possibles)";"Sa variance vaut $\sigma^2(XX')^{-1}$||Sa moyenne empirique est nulle||Sa variance vaut $\sigma^2(X'X)^{-1}$||Son espérance est nulle||Il est non-corrélé à $Y$||Sa variance vaut $\sigma^2 I_n$";[3,5];"Hypothèses usuelles : $\mathbb E[\varepsilon]=0$ et $\operatorname{Var}(\varepsilon)=\sigma^2 I_n$. En revanche $\varepsilon$ n’est pas non-corrélé à $Y$ puisque $Y=X\beta+\varepsilon$.";"QCM1, régression linéaire"
"Que vaut l’estimateur MCO (OLS) de $\beta$ ?";"$\mathbb E\big((XX')^{-1}XY\big)$||$(X'X)^{-1}X'Y$||$(XX')^{-1}XY$||$\mathbb E\big((X'X)^{-1}XY\big)$";1;"Formule classique des MCO : $\hat\beta=(X'X)^{-1}X'Y$.";"QCM1, régression linéaire"
"Que signifie $\operatorname{rg}(X)=p$ ? (plusieurs réponses possibles)";"$p\le n$||La matrice $X'X$ est inversible||La corrélation entre les variables explicatives est nulle||Les colonnes de $X$ sont linéairement indépendantes||La matrice $X$ est inversible";[0,1,3];"Rang colonne plein : colonnes LI, donc $X'X$ inversible et nécessairement $p\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.";"QCM1, régression linéaire"
"Si la p-valeur du test de Student pour le coefficient de $X^{(j)}$ vaut 0,023, que concluez-vous ?";"Le modèle de régression est inadéquat.||Ce coefficient de régression est nul.||Le modèle est surajusté.||Il y a une relation statistiquement significative entre $X^{(j)}$ et $Y$.";3;"Avec un seuil usuel $\alpha=5\%$, $p=0{,}023<0{,}05$ : on rejette $H_0$ « coefficient nul ».";"QCM1, régression linéaire"
"Comment sont définis les résidus $\hat\varepsilon$ ? (plusieurs réponses possibles)";"$\hat\varepsilon$=$P[X]Y$||$\hat\varepsilon$=$\hat Y - X\beta$||$\hat\varepsilon$=$Y - X\hat\beta$||$\hat\varepsilon$=$P[X]\varepsilon$||$\hat\varepsilon$=$P[X]^\perp \varepsilon$||$\hat\varepsilon$=$P[X]^\perp Y$||$\hat\varepsilon$=$Y - X\beta$";[2,4,5];"Par définition $\hat\varepsilon=Y-X\hat\beta=(I-P[X])Y=P[X]^\perp Y$. Comme $Y=X\beta+\varepsilon$, on a aussi $P[X]^\perp Y=P[X]^\perp\varepsilon$.";"QCM1, régression linéaire"
"Supposons que pour un test statistique donné, la région critique associée au risque de première espèce α = 5% est {|T| > 3.1}, où T désigne la statistique de test. On observe T = −4, que peut-on conclure ? (plusieurs réponses possibles)";"Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 5%, on ne peut pas conclure au test.||Pour un risque de première espèce de 10%, on ne peut pas conclure au test.||Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 1%, on ne peut pas conclure au test.||Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.";[4,3,5];"Si |T|=4 > 3.1, on est dans la région critique pour α=5% et α=10% donc on rejette. Mais pas assez extrême pour α=1%.";"QCM1, régression linéaire"
"Pour le même test qu’à la question précédente, on note F la fonction de répartition de T sous H0. On observe toujours T = −4. Que vaut alors la p-value ?";"1−F(−4)||F(−4)||2F(−4)||2(1−F(−4))";2;"La p-value bilatérale est 2×F(−4).";"QCM1, régression linéaire"
"Lors d’une étude statistique effectuée auprès d’étudiants, on relève la mention obtenue au BAC. Cette variable peut être considérée comme :";"qualitative ordinale||quantitative continue||qualitative nominale||quantitative discrète";0;"Les mentions (« passable », « bien », « très bien ») ont un ordre naturel → qualitative ordinale.";"QCM1, régression linéaire"
"Lors d’une étude statistique, on souhaite étudier le lien entre le pays d’origine des individus observés et leur souscription (ou non) à une assurance vie. Quel outil vous semble adapté ?";"La mise en place d’un test du chi-deux||La représentation d’un nuage de points||Une représentation graphique à l’aide de boxplots||Le calcul de la corrélation de Pearson";0;"Deux variables qualitatives → test du chi² d’indépendance.";"QCM1, régression linéaire"
"Dans l’écriture classique d’un modèle de régression linéaire concernant n individus, Y = Xβ + ε, (plusieurs réponses possibles)";"Y et X sont des vecteurs de taille n||Y est toujours considéré comme aléatoire||ε est toujours considéré comme aléatoire||Y et β sont des vecteurs de taille n||X est une matrice";[1,2,4];"Y est un vecteur aléatoire (n×1), ε aussi. X est une matrice n×p. β est p×1. Donc pas un vecteur de taille n.";"QCM1, régression linéaire"
"Supposons que lors de l’application d’un test statistique, on observe une p-value de 0.037. Que peut-on conclure ? (plusieurs réponses possibles)";"Pour un risque de première espèce de 1%, on ne peut pas conclure au test.||Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 5%, on ne peut pas conclure au test.||Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 10%, on ne peut pas conclure au test.";[2,4];"p=0,037 < 0,05 donc rejet pour α=5% et α=10%. Mais p > 0,01 donc pas de rejet au seuil 1%.";"QCM1, régression linéaire"
"Pour un test donné, la région critique associée à un risque de première espèce α est {|T| > a}, où T suit une loi continue. On a observé T = −2 et une p-value de 0.037. Que peut-on conclure ? (plusieurs réponses possibles)";"Pour α=5%, a < 2||Pour α=3.7%, a > 2||Pour α=3.7%, a < 2||Pour α=3.7%, a = 2||Pour α=5%, a > 2||Pour α=5%, a = 2";[0,3];"La p-value=0,037 → seuil critique α≈3,7%. Donc pour α=5%, rejet, donc a<2. Pour α=3,7%, T est sur la frontière, donc a>2.";"QCM1, régression linéaire"
"Lors d’une étude statistique effectuée auprès d’étudiants, on relève le code postal du lycée dont ils proviennent. Cette variable peut être considérée comme :";"qualitative nominale||quantitative continue||qualitative ordinale||quantitative discrète";0;"Code postal est une étiquette, sans ordre ni métrique → qualitative nominale.";"QCM1, régression linéaire"
"Lors d’une étude statistique, on souhaite étudier le lien entre le pays d’origine des individus observés et leur revenu annuel. Quel outil vous semble adapté ?";"La mise en place d’un test du chi-deux||Le calcul de la corrélation de Pearson||La représentation d’un nuage de points||Une représentation graphique à l’aide de boxplots";3;"Pays d’origine = variable qualitative, revenu = quantitative → comparaison de distributions via boxplots.";"QCM1, régression linéaire"
"En pratique, lorsqu’on met en place un modèle de régression linéaire Y = Xβ + ε, quelles quantités sont connues (observées) ?";"Y et Xβ||Xβ||X, Y et ε||X et Y";3;"On observe X et Y. β et ε sont inconnus.";"QCM1, régression linéaire"
"Si $\mathbb E(\varepsilon)=0$ et $\operatorname{Var}(\varepsilon)=\sigma^2 I_n$, que vaut la variance du vecteur $Y$ ?";"$\sigma^2(X'X)^{-1}$||$\sigma^2X\beta$||$\sigma^2 I_n$||$\sigma^2(XX')^{-1}$";2;"En effet, $\operatorname{Var}(Y)=\operatorname{Var}(X\beta+\varepsilon)=\sigma^2 I_n$.";"QCM2, régression linéaire"
"Comment sont définis les résidus $\hat\varepsilon$ ? (plusieurs réponses possibles)";"$\hat\varepsilon$=$P[X]Y$||$\hat\varepsilon$=$\hat Y - X\beta$||$\hat\varepsilon$=$Y - X\hat\beta$||$\hat\varepsilon$=$P[X]\varepsilon$||$\hat\varepsilon$=$P[X]^\perp \varepsilon$||$\hat\varepsilon$=$P[X]^\perp Y$||$\hat\varepsilon$=$Y - X\beta$";[2,4,5];"Par définition $\hat\varepsilon=Y-X\hat\beta=(I-P[X])Y=P[X]^\perp Y$. Comme $Y=X\beta+\varepsilon$, on a aussi $P[X]^\perp Y=P[X]^\perp\varepsilon$.";"QCM2, régression linéaire"
"Quelle statistique constitue un estimateur sans biais de $\sigma^2$ ?";"$\frac{1}{n-p}\sum_{i=1}^n \hat\varepsilon_i^2$||$\frac{1}{n}\sum_{i=1}^n \varepsilon_i^2$||$\frac{1}{n}\sum_{i=1}^n \hat\varepsilon_i^2$||$\frac{1}{n-p}\sum_{i=1}^n \varepsilon_i^2$";0;"L’estimateur sans biais de $\sigma^2$ est $\hat\sigma^2=\frac{1}{n-p}\sum \hat\varepsilon_i^2$.";"QCM2, régression linéaire"
"La statistique du test de Student associée au coefficient de régression de la variable $X^{(j)}$ vaut -23, que concluez-vous ?";"Le modèle de régression est inadéquat.||Ce coefficient de régression est nul.||Le modèle est surajusté.||Relation statistiquement significative entre $X^{(j)}$ et $Y$.";3;"Avec une statistique $t=-23$ (valeur absolue énorme), on rejette $H_0$. Il y a donc une relation significative entre $X^{(j)}$ et $Y$.";"QCM2, régression linéaire"
"Que peut-on dire de l’estimateur MCO de $\beta$ ? (plusieurs réponses possibles)";"Sa variance vaut $\sigma^2$||C’est le meilleur estimateur parmi tous les estimateurs sans biais||C’est un estimateur sans biais||Son espérance est nulle||Sa variance vaut $\hat\sigma^2/(n-p)$||Sa variance vaut $\sigma^2(X'X)^{-1}$||C’est le meilleur estimateur parmi tous les estimateurs linéaires et sans biais||C’est le meilleur estimateur parmi tous les estimateurs||C’est le meilleur estimateur parmi tous les estimateurs consistants";[2,5,6];"Théorème de Gauss–Markov : l’estimateur MCO est sans biais, de variance $\sigma^2(X'X)^{-1}$ et c’est le meilleur estimateur linéaire sans biais (BLUE).";"QCM2, régression linéaire"
"Supposons que lors de l’application d’un test statistique, on observe une p-value de 0.037. Que peut-on conclure ? (plusieurs réponses possibles)";"Pour un risque de première espèce de 1%, on ne peut pas conclure au test.||Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 5%, on ne peut pas conclure au test.||Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 10%, on ne peut pas conclure au test.";[2,4];"p=0,037 < 0,05 donc rejet pour α=5% et α=10%. Mais p > 0,01 donc pas de rejet au seuil 1%.";"QCM2, régression linéaire"
"On se place dans le modèle $Y=X\beta+\varepsilon$ (rang(X)=p)). Quelles hypothèses fait-on sur $\varepsilon$ ? (plusieurs réponses possibles)";"Sa variance vaut $\sigma^2(XX')^{-1}$||Sa moyenne empirique est nulle||Sa variance vaut $\sigma^2(X'X)^{-1}$||Son espérance est nulle||Il est non-corrélé à $Y$||Sa variance vaut $\sigma^2 I_n$";[3,5];"Hypothèses usuelles : $\mathbb E[\varepsilon]=0$ et $\operatorname{Var}(\varepsilon)=\sigma^2 I_n$. En revanche $\varepsilon$ n’est pas non-corrélé à $Y$ puisque $Y=X\beta+\varepsilon$.";"QCM2, régression linéaire"
"Qu'implique l'hypothèse $\operatorname{rg}(X)=p$ ? (plusieurs réponses possibles)";"$p\le n$||La matrice $X'X$ est inversible||La corrélation entre les variables explicatives est nulle||Les colonnes de $X$ sont linéairement indépendantes||La matrice $X$ est inversible";[0,1,3];"Rang colonne plein : colonnes LI, donc $X'X$ inversible et nécessairement $p\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.";"QCM2, régression linéaire"
"Que vaut l’estimateur MCO de $\beta$ ?";"$\mathbb E\big((XX')^{-1}XY\big)$||$(X'X)^{-1}X'Y$||$(XX')^{-1}XY$||$\mathbb E\big((X'X)^{-1}XY\big)$";1;"Formule classique des MCO : $\hat\beta=(X'X)^{-1}X'Y$.";"QCM2, régression linéaire"
"On note $\hat\varepsilon$ le vecteur des résidus de la régression. Quelles relations sont vraies ? (plusieurs réponses possibles)";"$\hat\varepsilon$=$P[X]Y$||$\hat\varepsilon$=$\hat Y - X\beta$||$\hat\varepsilon$=$Y - X\hat\beta$||$\hat\varepsilon$=$P[X]\varepsilon$||$\hat\varepsilon$=$P[X]^\perp \varepsilon$||$\hat\varepsilon$=$P[X]^\perp Y$||$\hat\varepsilon$=$Y - X\beta$";[2,4,5];"Par définition $\hat\varepsilon=Y-X\hat\beta=(I-P[X])Y=P[X]^\perp Y$. Comme $Y=X\beta+\varepsilon$, on a aussi $P[X]^\perp Y=P[X]^\perp\varepsilon$.";"QCM2, régression linéaire"
"Supposons que pour un test statistique donné, la région critique associée au risque de première espèce α = 5% est {|T| > 3.1}, où T désigne la statistique de test. On observe T = −4, que peut-on conclure ? (plusieurs réponses possibles)";"Pour un risque de première espèce de 1%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 5%, on ne peut pas conclure au test.||Pour un risque de première espèce de 10%, on ne peut pas conclure au test.||Pour un risque de première espèce de 5%, on rejette l’hypothèse nulle du test.||Pour un risque de première espèce de 1%, on ne peut pas conclure au test.||Pour un risque de première espèce de 10%, on rejette l’hypothèse nulle du test.";[4,3,5];"Si |T|=4 > 3.1, on est dans la région critique pour α=5% et α=10% donc on rejette. Mais pas assez extrême pour α=1%.";"QCM2, régression linéaire"
"Pour la suite, on se place dans un modèle de régression linéaire standard de la forme $Y = X\beta + \varepsilon$, où $\beta \in \mathbb{R}^p$, $X$ est une matrice déterministe de taille $n \times p$ de rang $p$ et $\varepsilon$ est un vecteur aléatoire de taille $n$. Quelles hypothèses fait-on sur le vecteur $\varepsilon=(\varepsilon_1,...,\varepsilon_n)$ ? (plusieurs réponses possibles)";"$E[\varepsilon_i \varepsilon_j]=0$ pour $i\neq j$||$E[\varepsilon_i]=0$||$E[\varepsilon_i Y_i]=0$||$Var(\varepsilon_i)=\sigma^2(XX')^{-1}_{ii}$||$Var(\varepsilon_i)=0$||$\frac1n\sum_i \varepsilon_i = 0$";[0,1];"On suppose indépendance faible ($i\neq j$), espérance nulle, variance homogène $\sigma^2$ (pas les autres).";"QCM2, régression linéaire"
"Que peut-on dire de l’estimateur MCO de $\beta$ ? (plusieurs réponses possibles)";"Sa variance vaut $\sigma^2$||C’est le meilleur estimateur parmi tous les estimateurs sans biais||Il s'agit d'un estimateur sans biais||Son espérance est nulle||Sa variance vaut $\hat\sigma^2/(n-p)$||Sa variance vaut $\sigma^2(X'X)^{-1}$||C’est le meilleur estimateur parmi tous les estimateurs linéaires et sans biais||C’est le meilleur estimateur parmi tous les estimateurs||C’est le meilleur estimateur parmi tous les estimateurs consistants";[2,5,6];"Théorème de Gauss–Markov : l’estimateur MCO est sans biais, de variance $\sigma^2(X'X)^{-1}$ et c’est le meilleur estimateur linéaire sans biais (BLUE).";"QCM2, régression linéaire"
"On note $\hat\varepsilon$ le vecteur des résidus de la régression. Quelles relations sont vraies ? (plusieurs réponses possibles)";"$\hat\varepsilon$=$P[X]Y$||$\hat\varepsilon$=$\hat Y - X\beta$||$\hat\varepsilon$=$Y - X\hat\beta$||$\hat\varepsilon$=$P[X]\varepsilon$||$\hat\varepsilon$=$P[X]^\perp \varepsilon$||$\hat\varepsilon$=$P[X]^\perp Y$||$\hat\varepsilon$=$Y - X\beta$";[2,4,5];"Par définition $\hat\varepsilon=Y-X\hat\beta=(I-P[X])Y=P[X]^\perp Y$. Comme $Y=X\beta+\varepsilon$, on a aussi $P[X]^\perp Y=P[X]^\perp\varepsilon$.";"QCM2, régression linéaire"
"On note $\hat\varepsilon$ le vecteur des résidus de la régression. Quelle statistique donne un estimateur sans biais de $\sigma^2$ ?";"$\frac1{n-p}\sum \hat\varepsilon_i^2$||$\frac1n\sum \varepsilon_i^2$||$\frac1{n-p}\sum \varepsilon_i^2$||$\frac1n\sum \hat\varepsilon_i^2$";0;"L’estimateur sans biais de la variance est $s^2=\frac{1}{n-p}\sum \hat\varepsilon_i^2$.";"QCM2, régression linéaire"
"Que vaut la variance du vecteur $Y$ ?";"$\sigma^2(X'X)^{-1}$||$\sigma^2 X\beta$||$\sigma^2 I_n$||$\sigma^2(XX')^{-1}$";2;"Puisque $Y=X\beta+\varepsilon$ avec $\operatorname{Var}(\varepsilon)=\sigma^2 I_n$, on a $\operatorname{Var}(Y)=\sigma^2 I_n$.";"QCM3, régression linéaire"
"Qu’est-ce que la multicolinéarité dans un modèle de régression multiple ?";"L’erreur dans l’estimation des coefficients de régression||L’interaction entre $Y$ et les variables explicatives||L’influence excessive d’une seule observation sur l'estimation du modèle||La forte corrélation entre deux ou plusieurs variables explicatives";3;"La multicolinéarité correspond à une forte corrélation linéaire entre deux ou plusieurs variables explicatives, ce qui rend $X'X$ proche de la singularité.";"QCM3, régression linéaire"
"On note SCR la somme des carrés des résidus du modèle initial, et $SCR_c$ celle d’un sous-modèle contenant p' variables ($p'<p$). Avec quelle statistique peut-on comparer les deux modèles ?";"$\dfrac{n-p}{p'}\dfrac{SCR_c-SCR}{SCR}$||$\dfrac{n-p}{p'}\dfrac{SCR-SCR_c}{SCR}$||$\dfrac{n-p}{p-p'}\dfrac{SCR_c-SCR}{SCR}$||$\dfrac{n-p}{p-p'}\dfrac{SCR-SCR_c}{SCR}$";2;"La statistique de Fisher utilisée est $F=\dfrac{(SCR_c-SCR)/(p-p')}{SCR/(n-p)} = \dfrac{n-p}{p-p'}\dfrac{SCR_c-SCR}{SCR}$.";"QCM3, régression linéaire"
"On effectue une nouvelle régression en ajoutant une variable au modèle initial. Que se passe-t-il forcément ? (plusieurs réponses possibles)";"Le $R^2$ augmente||Le $R^2_a$ diminue||La SCR diminue||Le $R^2$ diminue||La SCR augmente||Le $R^2_a$ augmente";[0,2];"L’ajout d’une variable explique au moins autant la variance, donc $R^2$ ne peut qu’augmenter et la SCR diminuer. En revanche, $R^2_a$ peut monter ou baisser selon la pertinence de la variable.";"QCM3, régression linéaire"
"Dans le contexte d’une régression multiple, qu’est-ce que l’hétéroscédasticité ?";"Le fait que la variance des $\varepsilon_i$ n’est pas constante||Le fait que les $\varepsilon_i$ ne sont pas corrélés entre eux||Le fait que la relation entre $Y$ et les explicatives n’est pas linéaire||Le fait que les coefficients du modèle sont biaisés";0;"L’hétéroscédasticité correspond à une variance non constante des erreurs : $\operatorname{Var}(\varepsilon_i)=\sigma_i^2$ dépend de $i$.";"QCM3, régression linéaire"
"Supposons qu’une constante est incluse dans le modèle. Quelle(s) propriété(s) vérifie(nt) le vecteur des résidus $\hat\varepsilon$ ? (plusieurs réponses possibles)";"Sa variance vaut $\sigma^2 I_n$||Il est non corrélé à $Y$||Sa variance vaut $\sigma^2(X'X)^{-1}$||Sa moyenne est nulle||Il est de taille $p$||Il est non corrélé à $\hat Y$";[3,5];"Les résidus vérifient $\mathbb E[\hat\varepsilon]=0$ (si constante incluse) et sont orthogonaux aux valeurs ajustées ($\hat Y$).";"QCM4, régression linéaire"
"Qu'implique l'hypothèse $\operatorname{rg}(X)=p$ ? (plusieurs réponses possibles)";"$p\le n$||La matrice $X'X$ est inversible||La corrélation entre les variables explicatives est nulle||Les colonnes de $X$ sont linéairement indépendantes||La matrice $X$ est inversible";[0,1,3];"Rang colonne plein : colonnes LI, donc $X'X$ inversible et nécessairement $p\le n$. $X$ n’est pas forcément carrée, et l’absence de colinéarité n’implique pas corrélation nulle.";"QCM4, régression linéaire"
"Quelle est la formule du $R^2$ ?";"$\dfrac{\sum_{i=1}^n (Y_i-\bar Y)^2}{\sum_{i=1}^n (\hat Y_i-\bar Y)^2}$||$\dfrac{\sum_{i=1}^n (Y_i-\hat Y_i)^2}{\sum_{i=1}^n (Y_i-\bar Y)^2}$||$\dfrac{\sum_{i=1}^n (\hat Y_i-\bar Y)^2}{\sum_{i=1}^n (Y_i-\bar Y)^2}$||$\dfrac{\sum_{i=1}^n (Y_i-\bar Y)^2}{\sum_{i=1}^n (Y_i-\hat Y_i)^2}$";2;"Définition : $R^2=\dfrac{\text{Somme des carrés expliquée}}{\text{Somme totale des carrés}}=\dfrac{\sum (\hat Y_i-\bar Y)^2}{\sum (Y_i-\bar Y)^2}$.";"QCM4, régression linéaire"
"La statistique du test de Fisher de significativité globale du modèle vaut 41, que concluez-vous ?";"Toutes les variables explicatives sont significatives.||Aucune variable explicative, autre que la constante, n’est significative dans le modèle.||Toutes les variables explicatives, autre que la constante, sont significatives.||Au moins une variable autre que la constante est significative.";3;"Un test de Fisher global très significatif ($F=41$) permet de rejeter $H_0$ et conclure qu’au moins une variable explicative influence $Y$.";"QCM4, régression linéaire"
"Dans le contexte d’une régression multiple, qu’est-ce que l’homoscedasticité ?";"Le fait que les $\varepsilon_i$ ne sont pas corrélés entre eux||Le fait que la relation entre $Y$ et les variables explicatives est linéaire||Le fait que l’estimateur MCO $\hat\beta$ est sans biais||Le fait que la variance des $\varepsilon_i$ est constante";3;"Homoscedasticité : $\operatorname{Var}(\varepsilon_i)=\sigma^2$ pour tout $i$. Variance constante mais pas nécessairement indépendance.";"QCM4, régression linéaire"
